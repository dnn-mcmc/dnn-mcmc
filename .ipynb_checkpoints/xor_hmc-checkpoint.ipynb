{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0, 0],\n",
    "           [0, 1],\n",
    "           [1, 0],\n",
    "           [1, 1]])\n",
    "y_train = np.array([[0],\n",
    "           [1],\n",
    "           [1],\n",
    "           [0]])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = tf.math.sigmoid(x)\n",
    "    #t = tf.cast(tf.math.greater(samples[0], 0.5), tf.int32)\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerange(x, r = 6.0):\n",
    "    \n",
    "    out_of_range = tf.cast(tf.math.greater(tf.math.abs(x), r), tf.float32)\n",
    "    sign = tf.math.sign(x)\n",
    "    \n",
    "    return x * (1 - out_of_range) + sign * r * out_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        #x = Flatten()(x)\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_current = [h_current[0]]\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "\n",
    "    def target_log_prob2(self, x, h, y):\n",
    "\n",
    "        h_current = convert2_zero_one(tf.split(h, self.hidden_layer_sizes, axis = 1))\n",
    "        h_current = [h_current[0]]\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        nlog_prob = 0.\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = h_current[0]\n",
    "\n",
    "        # initialize the HMC transition kernel\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = pow(1000, -1/4)),\n",
    "            num_adaptation_steps=int(100*0.8))\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 100\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = adaptive_hmc,\n",
    "            trace_fn = None)\n",
    "\n",
    "        h_new = tf.split(samples[0], self.hidden_layer_sizes, axis = 1)\n",
    "\n",
    "        return(h_new)\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tf.math.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tf.math.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layer_sizes = [2], n_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weight\n",
    "w_0 = np.array([[1, -1], [1, -1]], dtype = \"float32\")\n",
    "b_0 = np.array([-0.5, 1], dtype = \"float32\")\n",
    "l_0 = [w_0, b_0]\n",
    "\n",
    "w_1 = np.array([[1], [1]], dtype = \"float32\")\n",
    "b_1 = np.array([-1], dtype = \"float32\")\n",
    "l_1 = [w_1, b_1]\n",
    "\n",
    "model.fc_layers[0].set_weights(l_0)\n",
    "model.output_layer.set_weights(l_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "  array([[0, 1],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0]], dtype=int32)>]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [model.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "burnin = 1000\n",
    "step_sizes = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    network_new = []\n",
    "    kernels_new = []\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network, kernels):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        net_current = net_current[0]\n",
    "        \n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2][4].numpy()\n",
    "        step_sizes.append(new_step_size)\n",
    "        \n",
    "        new_state = rerange(samples[0][0])\n",
    "        net_new = tf.split(new_state, [2], axis = 1)   \n",
    "        network_new.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new.append(ker_new)\n",
    "            \n",
    "    network = network_new\n",
    "    kernels = kernels_new\n",
    "    \n",
    "    #print(network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXyU1b3H8c/JHpJAEggJECBhSxAFlE3BFURoXWpdqm21ar3Vqu3V1i54u1p7W2211t5a64LaVqu1rhXqQhEXlMWwKBCSEBK2QPaQfc+5f8xkkpBEsgx5MpPv+/XKi5lnJjO/hwe+c+Y85znHWGsRERHfE+B0ASIi0jcKcBERH6UAFxHxUQpwEREfpQAXEfFRQQP5ZqNGjbJJSUkD+ZYiIj5vy5YtxdbauGO3D2iAJyUlkZaWNpBvKSLi84wx+7vari4UEREfpQAXEfFRCnARER+lABcR8VEKcBERH6UAFxHxUQpwEREfpQAXEZ/zbmYhucXVTpfhOAW4iPiUlhbLrc9u5Q9r9zhdiuMU4CLiU45U1FHT0MzeoiqnS3GcAlxEfMrewirPn0N9RTEFuIj4lNaWd3VDMwUV9Q5X4ywFuIj4lJyitpOXQ70bRQEuIj7BWsu9b2Twt437GTsiDFCAK8BFxCccLq/jz+/tBWBMdDiRoUGe/vChSgEuIj6hfVgHBRiSR0Xwlw37OVha42BVzlKAi4hPaN9dcs+lJ9PU4hqB8p1/bHeqJMcpwEVk0CutbuDu19MByP3155kWH0VUmGtBsbyjtV55jz++s4e1uwu88loDRQEuIoPeyvU5ntvGGADuu3wmABGh/V8Zsqm5hfvfzuLGv/jWko8KcBEZ9GoamjttSx4VwU1nT+JAaQ3N7u6U0uoGvvzYxl73i9/RrhvGl1rhCnARGfTyylzdJKu+fWaH7VPiImloauFQmSuwN+eWsCGnhPf3FPXq9Vd9esRz25da4ccNcGPMk8aYQmPMznbbYo0xa4wxe9x/xpzYMkXEn134hw9IWrGaP72bTdKK1Xz1iY1sP3iUpBWrSVqxmrfTCzh/ejwnjxvR4fcmj44E4JzfvgvAnoKqDn/2RElV56s5V316uI97MrB60gJ/Glh+zLYVwFpr7VRgrfu+iJwgLS2WqvomT1eBP6lrbGbX4QoAfvNmJgAfZpfw1Ie5HZ4XGtw5rqa4AxzgYGkNO/LKAdhTWNnj989yh/1lp43zbPv5v9J7/PtOOm6AW2vfB0qP2fwF4C/u238BLvVyXSLSzo9e3cnJP3uLFS996nQpXvXWrnxSf/Jml49V1DZ2uH/SmOGdnjMiPNhz+6zfrOPtdFf/dW9a4NnusP/+shRuPnsSAI3NLT3+fSf1tQ883lrb2mmUD8R390RjzE3GmDRjTFpRUe/6pUTE5bnNBwD455ZDDlfiXQ+8ndntYxtzOrYbv3nO5C6f99T18zptK6ysp7Cy7riLPhwsrWHbwaNEhQaRMDyM7y1LYeLIYQQH9j4aG5pa2FtURWZ+5YDNktjvk5jWVWm31VprH7PWzrXWzo2Li+vv24mIn1i7u8DTfQEw3d3Cnpno6ueubWxm+YwEAL67dBqBAabL1zlnWsdcWTh5JABX/nkD593/LpV1jV39Gi0tlrN+s46Xt+YxJT4SYwzBgQFcs2AixVX1lFU39Gp/7n59F0seeI9lv3+fl7fm9ep3+6qvAV5gjBkD4P6z0HsliUh7VfVNHe5/tLeYxuYWXtuex5s78ymv6Tqg+qKusZnXPzk8IC3InXkVHe6/cutC/vPdc3jh5jMIc/d3L5o6is3/s4RvnTel29cJCDBs/clS3rrjbNbeeY5nfPj+EtfIlJv+uoXD7ot9NuaUcKCkhrLqBv66YZ/nNaa260ufGu+6/ZcN+3p1kdBbu9qGH249UAa4xpc/uCaLj/cd2wvtHX0N8H8B17lvXwe85p1yRORYewpcfbRXzxsPwFce38RTH+Zy+/Pb+eYzW/jOC967lPw3b2by7ee2sSGnxGuv2Z2CyjrP7TkTYwgLDmTK6EjCggOZHOcK0amjIxk9PIyAblrfrWIjQkhJiGJyXCTjosM7PLYhp4RL/rgeay3f+GsaD6zJ5NZnt/Lz19tOVE6Lj/Lcnuq+/fv/7OHi/1vfiz1q+9Br/dD9++YDPLR2D1f+eUMvXqfnejKM8DlgA5BijDlkjLkRuBdYaozZA5zvvi/i10qrG1h8/7u8kHZwQN+39YTchTPHeLbtLWzr2911uLzHr7VlfxmL7n2HTw8d7fTY4+/n8KR75MedL3xCywke8ZKVX8n8pFj23XshL92ysMNjrYHaPlh7qquwL65q4L2sIirrmngvq6jTB9TIyBDP7dapasF1zHuiucVSUdf2Ten9rCLuezPD8y0A8Oo3pVY9GYXyZWvtGGttsLU20Vq70lpbYq1dYq2daq0931p7Yr4fiAwiv30rg5zian7w4sCOBMksqCQsOIAzJo30bEvb3/Zfrje9HZc/8hF5R2u55I8fdtje0mL533/v9tw/Ul53QufattaSWVDJtITILh+/ZPZYrpiTSGxESJePH8/dl8wAOnaNXP/UxwAcPSZIz02JY8n0tnEYrZfqt+rJB9m+kmoamloYOyKMqLAgymoaeeTdvbydnu95TnG191cP0pWYIj1UVt32H7+yrpH0wxV86c8buj1J1l8vbjlE0orVrFyfy9jocIICA3jj9rMA2NtuVZrCynqSVqzmuic3d3qNN3ce8VwMk7RidYfHvvX3rSStWM3B0pou+3r3eGmu7eYWy+xfvE3SitXUNbouiS+oqKeyromUblrY56WM5v4rZ/X5Pa9bmMS+ey9kzXfP8XQ9defpG+YzPCy4w7bw4EDP7W8/t81ze8PeEq5duYmGpo7DDLPyXd1cj147l59edJJn+8HSWpakjmbfvRd6uoW8SQEu0kPtv05nFVTxk9d2snlfKR/tLaGlxXp17LC1lu/98xPP/Ypa19fzSXERnm2JMeEsm9HWcnwvq8hTQ21DM7UNzXzzma3dvkfr5eMPr8smI7/twpc7zp8KQGZ+zy+G+SwHSms8rd7W7p5Md79+X7pIemvF51K73H7X51L55zfP6PKxV25byBVzEgFYveMI9U2uD56b/prGB3uK2ZFX3qFlnllQiTGuE6ApCR33aVrCidtHBbhID1hryciv4KypowB4+qN9bNnvGmlwqKyW+97K4It/+vCzXqJX7n0jo8P9s93vGxrU1jK8fmESd5w/rcPzcoqq+Si7mOk/fZPpP+36AplFU0Z2uP/8xwf51yeuS8d3/PwC7jh/GsmjIrwW4Jn5baNNst2t+tYW60AEePSwEC4/LbHDttMmRHPzOZOZlxTb5e+kJgznZxe3taRTfvwmt/19K5Xuk5OXP/IR33ux7QM2M7+SpJERnhOx7XX3LcMbFOAiPZBfUUdFXRPnT49nWEggr3/SNlfGMxv382F2MTvzKjoN+fssm3NLO30VB9dQvkffd02fagz8+rJT+N8vnuJ5fO2d5/Cby2dyw6LkDi1ygIz8Ct7ald9h25gRYR3C6OKZYzu95+ufHGZcdDhR7q6EafGRnlZyX23ZX0ZtQ3OH1n1GfiWNzS08s2k/o6NCieljH3dv/fLSk/n1Zafw4YrFvHbbIp66fv5xfycqLJg7l7Z9QK5uN+EVwMtb8zzdZ5kFlUxzDz8cFhLEq7ctYnHqaODEfkgpwEV6oDWEUhOiOv2HzC2u9oxp7mmrNaeoii89uoGXt3a+svLHr3rmjePJ6+bx5fkTCA9pa3lPjovkS/PGExhgCA0KJCQwgPlJsQQFGDLzK6mq7zj16orPpXLDomRuOdd1JeMy98UxAPOS2uahCwpsO3mXkjCcfSXVnj7r3sovr+PyRz7iR6/sIDO/kuRREcwaH01mfiUPvJ3F/pKaAb1cPTwkkC/Pn8C46HBmjY9mxLDg4/8ScNX8z+4/v+3v26hrbGZfcTUpCW2X+s8eH8350+OJjQjp9CHrTQpwkc9QWFnH797OJN092VJqwnBS3X2a85JiiDkmCC5/5CPeySiguKqeB97OpKmbkGqdvGlnF0MA380scr9XFOe5W3GfZefdy/j7NxYwOS6SjPxKsgoqmZcUwzB36Lf2yX7/ghQy7llOTEQIu3+xnN2/WM7zN7X1AQe1G36XmhCFtfDL1X2b1Gm3u9vk5W15fJhdTEp8FKnxUWTkV7LFPYKmL5erD7S4yNBO2zJ/uZzzp7uOy/tZRXywp5gW27mr5Op54/loxWLC2p0Q9bbB/zco4qDXth3mD+9k89KWQyQMD2PEsGBPC/yUcdE8eu3cTr/z9afT+OWqdP7vnWzWZxd3+boZ+d232FuXCnvgSz0bhRESFEBQYAApCVGkH64gq6CSmYnRPH3DfM6fPtoz+iEgwHjCJDwkkPCQQAIDDD9cnup+v9me12zdx2c2Huj2Q+izZBxp26+KuiZSEqJISYiitLqB/ArXBTwPf/W0Xr/uQDPG8O3FU5g9PhqAx66dQ2hQID++sK1L6ht/dc0fnnLMkMj2f98nSv/XIhLxY60tyZzias+cG60t8NSEKOYnxxIzLJiyY8YWv7rd1Ud+/VMfExsR4hnB8pOLTuKeVW2t2o/3lZG0YjWb/mcJ8cPDeHhdNrnF1dx8ziRmjO049/XxpCREeU5GttY2P7nrk3Tt3XLuZE/3SqukkcM8t/eVVDNldO/6cXcf6XiZfGpClGfmwIOltXxlwYRuTyAONndekMKdF6R02JY0KoLzp4/mP7vbZhFJGnniukq6oxa4SDcOH63t0JJsDe75ybHc9blUPu++MvLFWxZy/5WzeOHmroektR9+2D6823t+80F25pXz27dcs/NN6cOY4dR2w9VSEzpPvdobQYEB3HaeK9TTj/T+ZGZGfoVncirA0wJvdezl7r7oXvecK62CHOgSUoCLdGHrgTIW3vsO6e1akq0BFBQYwM3nTCbSvZju5LhIrpiTyPzk2E5Lfn2W9sP5HvxPFhe1m3ejLwHccT6P/l80cvuSaQQHmk6t6eOpa2xmb1E1S1JHe4ZdThwZwch2/cnHrqzji0ZFhnLpbNeInsQYZz6Q1IUicgxrbYeFE6KHBXO0prHTBRpdaT8GOCo0yDNuePqY4Z2C8Op5E/j+slSueWJTh+GHy2bEc0pi7wNufOww3rj9LMKCA73S9xoSFMDkuMheB3h2YRXNLZbUMVHcet5kSqsbPFPBbrhrMfnldZw6wT9WYbz38pl8ZcFEUsec+PHsXVELXOQYO/MqOsxT/c1zJjMrcUSnCzS6EhYcyNnT4rhz6TR+d9VsAowryB+6eranNfrzi09idFQoCyePZPb46E7DzFpPKvbF9DHDSR7lvb7Yk7r44DmetiGXwxkWEkRiTFt/+pgR4X4T3uA63vOTYztdij9Q1AKXIef257ex9KR4LjrmgpafvbaTTbmlHS48AbjxzORuV4Ppyl+/3naRSM6vL/Tc/tuNCzy3r1+U7Lk9OioMKOelW85gzsTBdWJv+pjhvLwtj9Lqhm4nlsovr+OWZ7dQ19jC/3w+ld1HKggNCvDqB4l0TQEuQ0ppdQOvbT9MTUOzJ8ALKuqICgviLxv2d3ju95elEBcZesLHK//qspOZ/fEITh0/+FqmrSciM45UsHDKqE6Pl9c08uNXd7DtgGt62mtXbmZSXAQpCVHdrqAj3qMAlyGltTug9cKchqYWFvxqbYcRHK1u+4xVYLxpdFQY31o8dUDeq7emu/t207sJ8Fm/eLvTtpyiar546rhO28X71AcuPqWwoo4v/ulDrn5sA+W1vZvGtam5hVufdc3Ol3e0lq0HyphzzxqADt0m85Ji2PLj871XtA8bGRnKyIgQfrl6Nxc8+J7nAiRwTRPbHbW+B4YCXHzKD1/6lG0HjrIxp5T73+p+RfOurMss6hD6l/3pI88okVbTxwznwatmdxjyNtSVuMexZxVUsfz3H3i2t1/xPWnksA4nY289t+fnDKTv1IUiPuVAadsSVX/buJ/rFyV1OVH+xpwSrn5so+f+zWdP8szw152ZiSP417d6Po57qDh1QrSnj7vVg2uyeGjtHgDeuP2sDhftyMBRC1x8SmNzx6/tP+xmebOH12V3uN8+vN+64+wOj92wKAlwDZmTzh69Zo5ncQNwnfRtDW/ghKw0Iz2jABefUVHXyIHSGr53wTSumuua5jNtf5lnYYX2mpq77p+NiwolJSGK+OGuLpLfXDGT7y6dxvCwIM6YPLLL3xnqRg8P4/4rZ/GIe/Kps+5b1+HxkCDFiFPUhSI+4yuPu7pEZowdwa3nTuEf7tXh9xRUMmdixyF42UVVXDxrLG/sOEJTu5NtG+9aAsBHK5bQ2NziuWIx7cdLCQ7UibfP0jq5VoN7dsJrTp/Azy+e4WRJQ54+OsUnHCyt8SyaMGPccAICDCuvc03luurTI+zMK6e2oXXB3DqKKus5dXw0L9+6kOvOmOhZlaZ1dETgMVN9hgQFdFqNXDoaH9txvo/vLk1xZAInaaMWuPiEs37T9rXddeUiLJkez6jIENZnF3PR/61nzsQYXrplITsOuRZJOCVxBDMTo5mZGM3dXzjZkbr9SfsPuDuXTuv2ykwZOApwGdQeXpfNrMRoz/1jZ/trv8hva1/4zsPlGKOTkifC9p8uJae4mtntjok4R99/ZNAqq27gt29l8kP3zIBjRoR1mob02FVd6pua2ZlXzuS4SCJC1T7xtuhhIZw2IYYAXagzKCjAZdDakefqCsk7WgvA613MtT17fDT77r3QM0IiM7+SHXnlnDxWrW/xfwpwGbRaAxxcre9Rn3F1ZGvLfO3uQgoq6v1iwQCR41GAy6D16aG2q/+OF8iJMeHEDAvmBffQwlMU4DIE9CvAjTHfMcbsMsbsNMY8Z4wJ81ZhIjsOlbNw8kgCDJ5VwbtjjOHkcSM4Uu5a8XyGAlyGgD4HuDFmHPDfwFxr7clAIHC1twqToevw0Vp++tpODpfXsTh1NP/85hlcvzDpuL83070M2aRREZ71KkX8WX//lQcB4caYRmAYcLj/JclQ94MXP2V9djHg6grp6So1rd0m6v+WoaLPLXBrbR5wP3AAOAKUW2s7ze5ujLnJGJNmjEkrKirqe6UyZBx2jzqB3nWFzBofjelBd4uIv+hPF0oM8AUgGRgLRBhjrjn2edbax6y1c621c+Pi4vpeqQwJ1lpKaxo893vTFTJmRDiv3rqIr54+4USUJjLo9Ock5vlArrW2yFrbCLwMLPROWTJUHSyt5WhNI4kx4fzy0t5f/j5rfHSHqzNF/Fl/+sAPAKcbY4YBtcASIM0rVcmQtd09dPDP18xRX7bIcfSnD3wT8CKwFdjhfq3HvFSXDEElVfX893PbAEjpYpFhEemoX6NQrLU/A37mpVpkiHtu8wHP7WBNUypyXPpfIoPGYfdFOK2r7YjIZ1OAy6Cx/cBRTp8Uy68vO8XpUkR8ggJcBoWahiYy8iuYnxSrqUpFekgBLoPCmvQCWiycOiHm+E8WEUABLoNAaXUDtz+/HdBVlCK9oQAXx207UOa5HaN1FkV6TAEujqpvauZ7//wEgIeunu1wNSK+RQEujlq5PpeymkYAvjB7nMPViPgWBbg4am9hNQCnT+rZlLEi0kYBLo55P6uIl7YeImF4GI9/ba7T5Yj4HAW4OOZrT24GYF5yLFFhwQ5XI+J7FODiCGut5/bNZ09ysBIR36UAF0fsK6kB4FdfPEXTxor0kQJcHLE5twSA+ck6eSnSVwpwccSm3FJGRoQwOS7C6VJEfJYCXByxObeU+cmxGKOJq0T6SgEuAy7vaC2HymrVfSLSTwpwGXAf55YC6v8W6S8FuAy4TbmlRIUFkZow3OlSRHyaAlwG1KacEp7bfIB5SbEEauEGkX5RgMuAuuqxjQBMHDnM4UpEfJ8CXAZMc0vb1ZdXz5vgYCUi/kEBLgMm/XAFAL+/ajYpCVEOVyPi+xTgMmDWZxcDsHDKSIcrEfEPCnAZMB9mF5MSH8XoqDCnSxHxCwpwGRB1jc1s3lfKmVNHOV2KiN9QgMuASNtXRkNTC2dOUYCLeIsCXAbE+uxiggONrr4U8aJ+BbgxJtoY86IxJsMYs9sYc4a3ChP/sj67iFMnxBARGuR0KSJ+o78t8IeAN621qcAsYHf/SxJ/Ut/UzM68cnYdrlD3iYiX9bk5ZIwZAZwNXA9grW0AGrxTlvgDay23PbuN/+wuANAJTBEv608LPBkoAp4yxmwzxjxhjOk0O78x5iZjTJoxJq2oqKgfbye+ZuX6XE94A8zU0mkiXtWfAA8CTgMesdaeClQDK459krX2MWvtXGvt3Li4uH68nfiaP7+X47kdFRpEUKDOmYt4U3/OKB0CDllrN7nvv0gXAS5DU11jM9X1TcQPD+W+y2cyOS7S6ZJE/E6fA9xam2+MOWiMSbHWZgJLgHTvlSa+bGNOCbWNzfzpmtM4N2W00+WI+KX+jun6NvCsMSYEyAFu6H9J4g/WZRQSFhzAGZM074nIidKvALfWbgfmeqkW8RPWWt7JLGTR5FGEBQc6XY6I39JZJfG6vUXVHCyt5bxUdZ2InEgKcPGKD/YUMfVH/2bbgTLWZRQCKMBFTjBd1yxece3KzQB88U8fccakkaTERzEuOtzhqkT8m1rg0m+FlXUd7m/IKVHrW2QAKMCl39budnWZ/GB5imfbeSm6aEvkRFOAS7+tSS9gfGw4X1+UTGRoEKOjQpkzMcbpskT8nvrApV+q65tYn13MNQsmEhYcyM67lzldksiQoRa49Mv7WUU0NLVwwYx4p0sRGXIU4NIva9ILiB4WzFx1mYgMOAW49FljcwtrMwpZnDpaMw2KOED/66TPPt5XSnltIxeclOB0KSJDkgJc+mz1p0cIDw7k7GlaaUfECQpw6ZOm5hbe3JnPkumjGRaiwUwiTlCAS59syCmhpLqBi2aOdboUkSFLAS59suqTI0SGBnGurrgUcYwCXHqtoamFN3fls/SkeM33LeIgBbj02ofZxZTXNnLhKWOcLkVkSFOAS6+9/ulhosKCOEujT0QcpQCXXqlrbGbNrgKWzUggNEjdJyJOUoBLr6zdXUhlfROXzNLoExGnKcClV17aeoiE4WEsmqLuExGnKcClx4oq63kvq4hLTx1HYIBxuhyRIU8BLj322vY8mlssV8wZ53QpIoICXHrhxS2HmJU4gimjo5wuRURQgEsP7TpcTkZ+JZfPSXS6FBFxU4BLj7y0JY/gQMPFmvtEZNBQgMtx1Tc18+r2PJakxhMTEeJ0OSLipgCX43pzZz6l1Q18ZcEEp0sRkXb6HeDGmEBjzDZjzCpvFCSDz7ObDjAhdhhnauy3yKDijRb47cBuL7yODELZhZVszi3lKwsmEKCx3yKDSr8C3BiTCFwIPOGdcmSweXbTAYIDDVdq9InIoNPfFvjvgR8ALd09wRhzkzEmzRiTVlRU1M+3k4FU29DMS1sO8bmTxzAyMtTpckTkGH0OcGPMRUChtXbLZz3PWvuYtXautXZuXJxWb/Elqz49TEVdk05eigxS/WmBLwIuMcbsA54HFhtjnvFKVeI4ay1Pf7SPqaMjWZAc63Q5ItKFPge4tfYua22itTYJuBp4x1p7jdcqE0dtzCll1+EKbjwzGWN08lJkMNI4cOnSyvW5xEaEcOmpmrhKZLDySoBba9+11l7kjdcS5+UUVbE2o4BrTp+oRYtFBjG1wKWTpz7cR3BAANeePtHpUkTkMyjApYOjNQ28uOUQX5g9lrgoDR0UGcwU4NLBXz7aT21jMzeelex0KSJyHApw8aiqb+LJD3M5f3o8qQnDnS5HRI5DAS4ez2zcT3ltI99aPMXpUkSkBxTgArgum3/igxzOmjqK2eOjnS5HRHpAAS4APLf5AMVVDXx78VSnSxGRHlKAC3WNzTz6/l7mJ8cyX5fNi/gMBbjwzMb9FFTUc8cStb5FfIkCfIirrGvk4XXZnDV1FAu14o6IT1GAD3GPf5BLWU0j31+W4nQpItJLCvAhrLiqnic+yOHCU8YwM1EjT0R8jQJ8CPvjO9nUN7Xw3QumOV2KiPSBAnyIyi6s4pmN+/nS3PFMjot0uhwR6QMF+BBkreWeVemEhwRyp1rfIj5LAT4EvZNRyHtZRdy+ZCqjtFixiM9SgA8xDU0t3LMqnclxEVy3MMnpckSkHxTgQ8zK9bnsK6nhJxedRHCgDr+IL9P/4CFkf0k1D63NYulJ8ZybMtrpckSknxTgQ4S1lh+9spOggADu+cLJTpcjIl6gAB8iXtmWx/rsYn64PIWEEWFOlyMiXqAAHwJKquq5Z1U6p02I5qsLtFCxiL9QgPs5ay13vbyD6vpm7r18JgEBxumSRMRLFOB+7sUth3g7vYDvLZvGtPgop8sRES9SgPuxg6U13P16OvOTY7nxzElOlyMiXqYA91PNLZY7//kJAA9cOYtAdZ2I+J0gpwuQE+PBNVlszi3lgStnMT52mNPliMgJoBa4H1qXWcgf12XzpbmJXD4n0elyROQE6XOAG2PGG2PWGWPSjTG7jDG3e7Mw6Zu8o7V85x/bSU2I4he6YEfEr/WnC6UJuNNau9UYEwVsMcassdame6k26aW6xmZufXYrTc2WR66ZQ1hwoNMlicgJ1OcWuLX2iLV2q/t2JbAbGOetwqR3Wsd7f3LwKL+9YibJoyKcLklETjCv9IEbY5KAU4FNXTx2kzEmzRiTVlRU5I23ky48vC6bV7blcefSaXzulDFOlyMiA6DfAW6MiQReAu6w1lYc+7i19jFr7Vxr7dy4uLj+vp10YfWnR7j/7SwunT2Wby2e4nQ5IjJA+hXgxphgXOH9rLX2Ze+UJL3x8b5SvvvCduZMjOHey2dijMZ7iwwV/RmFYoCVwG5r7e+8V5L01K7D5Xz96Y8ZFx3OY9fqpKXIUNOfFvgi4FpgsTFmu/vn816qS44jp6iKr63cTFRoEH/7rwWM1NqWIkNOn4cRWmvXA/q+7oCDpTVcu3IzAH/7rwWMiw53uCIRcYIupfcx+4qr+crjG6luaObZ/1rA5LhIp0sSEYcowH1IdmEVX31iI43Nlr9/Y+vlUhAAAAjgSURBVAEzxo5wuiQRcZAC3EfszCvn+qc2A4bnbzpdc3uLiCaz8gXvZRVx1aMbCAkM4B83K7xFxEUt8EHuhY8PctcrO0iJj+KpG+YRP1wLEouIiwJ8kGpqbuG+NzN4/INczpo6ikeumUNkqA6XiLRRIgxCJVX1fPu5bXy0t4RrT5/ITy8+ieBA9XaJSEcK8EHmk4NHufXZrRRV1XP/lbO4QgsyiEg3FOCDRHOL5c/v7eXBNVnEDw/jpW8u5JREDRMUke4pwAeB1lV0NueWcuHMMfzq0lMYMSzY6bJEZJBTgDuopcXy980HuO+NDFqs5YErZ3HZaeM0o6CI9IgC3CF7Ciq56+UdpO0vY9GUkfz6izOZMFKrx4tIzynAB1hlXSOPvLuXxz/IITI0SK1uEekzBfgAaWpu4YW0Q/xuTSbFVQ1cdto4fvT56ZoGVkT6TAF+grW0WN5Oz+fBNXvILKhkflIsT14/nZmJ0U6XJiI+TgF+grS0WN7alc9Da/eQkV9J8qgIHvnqaSw/OUHdJSLiFQpwL6trbOb1Tw7zxAe5ZBZUMmlUBA9eNYuLZ44lSFdTiogXKcC9pLCijmc27ufZTQcoqW5gWnwkv79qNhfPGktggFrcIuJ9CvB+aGpu4YPsYl5MO8Tb6fk0tVgWp4zmhkXJLJoyUl0lInJCKcD7IKugkpe35vHKtkMUVNQTMyyYa06fyNfOSCJ5VITT5YnIEKEA7wFrLelHKnhjRz5v7DzC3qJqAgMM506L4+5LElmcGk9IkPq3RWRgKcC7UdvQzKbcEt7PKuY/uws4UFpDgIEFySO5bmESy09OYHSUFlcQEecowN2aWywZ+RV8mF3MB3uK2ZRbSkNTCyFBAZw+aSS3nDuZC06K14U3IjJoDNkAr2loYvuBo3y8r4y0/aVsO3CUqvomAKaOjuTa0ydy9rQ45ifFEh4S6HC1IiKdDYkAL69tJP1wBbsOl7v/rCC7qIrmFosxkBIfxaWnjmXuxFgWTIplzIhwp0sWETkuvwrwsuoG9hZVuX+qySmqIqugigOlNZ7njI4KZcbY4VwwI57TJsZw2oQYRoRr7m0R8T0+FeD1Tc0cPlrHobIaDpXVkldWy6GyGg6W1ZJbXE1pdYPnuSFBAUwaFcEp40Zw1bzxzBg7nBljRxAXpT5sEfEPPhHgP3plB2vSCyisrO+wPTDAkDA8jHEx4SybEc/kuEjPz7iYcF0BKSJ+rV8BboxZDjwEBAJPWGvv9UpVxxgbHc450+JIjBlGYkw442LCSYwJJ2F4mOYXEZEhq88BbowJBB4GlgKHgI+NMf+y1qZ7q7hWt503xdsvKSLi8/rTfJ0PZFtrc6y1DcDzwBe8U5aIiBxPfwJ8HHCw3f1D7m0dGGNuMsakGWPSioqK+vF2IiLS3gnvQLbWPmatnWutnRsXF3ei305EZMjoT4DnAePb3U90bxMRkQHQnwD/GJhqjEk2xoQAVwP/8k5ZIiJyPH0ehWKtbTLGfAt4C9cwwiettbu8VpmIiHymfo0Dt9b+G/i3l2oREZFe0FUwIiI+ylhrB+7NjCkC9vfx10cBxV4sxxdon4cG7fPQ0J99nmit7TSMb0ADvD+MMWnW2rlO1zGQtM9Dg/Z5aDgR+6wuFBERH6UAFxHxUb4U4I85XYADtM9Dg/Z5aPD6PvtMH7iIiHTkSy1wERFpRwEuIuKjfCLAjTHLjTGZxphsY8wKp+vxBmPMeGPMOmNMujFmlzHmdvf2WGPMGmPMHvefMe7txhjzB/ffwafGmNOc3YO+M8YEGmO2GWNWue8nG2M2ufftH+65dTDGhLrvZ7sfT3Ky7r4yxkQbY140xmQYY3YbY87w9+NsjPmO+9/1TmPMc8aYMH87zsaYJ40xhcaYne229fq4GmOucz9/jzHmut7UMOgDvN3KP58DTgK+bIw5ydmqvKIJuNNaexJwOnCbe79WAGuttVOBte774Nr/qe6fm4BHBr5kr7kd2N3u/n3Ag9baKUAZcKN7+41AmXv7g+7n+aKHgDettanALFz77rfH2RgzDvhvYK619mRccyVdjf8d56eB5cds69VxNcbEAj8DFuBaJOdnraHfI9baQf0DnAG81e7+XcBdTtd1AvbzNVzL02UCY9zbxgCZ7tuPAl9u93zP83zpB9e0w2uBxcAqwOC6Oi3o2OONa6K0M9y3g9zPM07vQy/3dwSQe2zd/nycaVvsJdZ93FYBy/zxOANJwM6+Hlfgy8Cj7bZ3eN7xfgZ9C5wervzjy9xfGU8FNgHx1toj7ofygXj3bX/5e/g98AOgxX1/JHDUWtvkvt9+vzz77H683P18X5IMFAFPubuNnjDGRODHx9lamwfcDxwAjuA6blvw7+PcqrfHtV/H2xcC3K8ZYyKBl4A7rLUV7R+zro9kvxnnaYy5CCi01m5xupYBFAScBjxirT0VqKbtazXgl8c5Btf6uMnAWCCCzl0Nfm8gjqsvBLjfrvxjjAnGFd7PWmtfdm8uMMaMcT8+Bih0b/eHv4dFwCXGmH24FsFejKt/ONoY0zq1cfv98uyz+/ERQMlAFuwFh4BD1tpN7vsv4gp0fz7O5wO51toia20j8DKuY+/Px7lVb49rv463LwS4X678Y4wxwEpgt7X2d+0e+hfQeib6Olx9463bv+Y+m306UN7uq5pPsNbeZa1NtNYm4TqO71hrvwqsA65wP+3YfW79u7jC/Xyfaqlaa/OBg8aYFPemJUA6fnyccXWdnG6MGeb+d966z357nNvp7XF9C7jAGBPj/uZygXtbzzh9EqCHJwo+D2QBe4EfOV2Pl/bpTFxfrz4Ftrt/Po+r728tsAf4DxDrfr7BNRpnL7AD1xl+x/ejH/t/LrDKfXsSsBnIBv4JhLq3h7nvZ7sfn+R03X3c19lAmvtYvwrE+PtxBu4GMoCdwN+AUH87zsBzuPr4G3F907qxL8cV+Lp737OBG3pTgy6lFxHxUb7QhSIiIl1QgIuI+CgFuIiIj1KAi4j4KAW4iIiPUoCLiPgoBbiIiI/6f+7AYcrkq7YbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes)), step_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
      "array([[ 6.       ,  6.       ],\n",
      "       [ 6.       ,  1.3267002],\n",
      "       [ 6.       ,  4.863472 ],\n",
      "       [ 6.       , -6.       ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: - 0.0639s/step - loss: 0.8022 - accuracy: 0.7500\n",
      "Epoch 2/500: - 0.0623s/step - loss: 0.8015 - accuracy: 0.7500\n",
      "Epoch 3/500: - 0.0594s/step - loss: 0.8157 - accuracy: 0.7500\n",
      "Epoch 4/500: - 0.0563s/step - loss: 0.8133 - accuracy: 0.7500\n",
      "Epoch 5/500: - 0.0550s/step - loss: 0.7924 - accuracy: 0.7500\n",
      "Epoch 6/500: - 0.0565s/step - loss: 0.7909 - accuracy: 0.7500\n",
      "Epoch 7/500: - 0.0539s/step - loss: 0.7887 - accuracy: 0.7500\n",
      "Epoch 8/500: - 0.0516s/step - loss: 0.7860 - accuracy: 0.7500\n",
      "Epoch 9/500: - 0.0501s/step - loss: 0.7843 - accuracy: 0.7500\n",
      "Epoch 10/500: - 0.0492s/step - loss: 0.7823 - accuracy: 0.7500\n",
      "Epoch 11/500: - 0.0499s/step - loss: 0.7815 - accuracy: 0.7500\n",
      "Epoch 12/500: - 0.0490s/step - loss: 0.7797 - accuracy: 0.7500\n",
      "Epoch 13/500: - 0.0481s/step - loss: 1.2880 - accuracy: 0.7500\n",
      "Epoch 14/500: - 0.0477s/step - loss: 1.2696 - accuracy: 0.7500\n",
      "Epoch 15/500: - 0.0475s/step - loss: 1.2518 - accuracy: 0.7500\n",
      "Epoch 16/500: - 0.0477s/step - loss: 1.2332 - accuracy: 0.7500\n",
      "Epoch 17/500: - 0.0471s/step - loss: 0.7679 - accuracy: 0.7500\n",
      "Epoch 18/500: - 0.0465s/step - loss: 0.7667 - accuracy: 0.7500\n",
      "Epoch 19/500: - 0.0460s/step - loss: 0.7674 - accuracy: 0.7500\n",
      "Epoch 20/500: - 0.0459s/step - loss: 0.7645 - accuracy: 0.7500\n",
      "Epoch 21/500: - 0.0463s/step - loss: 0.7630 - accuracy: 0.7500\n",
      "Epoch 22/500: - 0.0462s/step - loss: 0.7618 - accuracy: 0.7500\n",
      "Epoch 23/500: - 0.0461s/step - loss: 0.7658 - accuracy: 0.7500\n",
      "Epoch 24/500: - 0.0461s/step - loss: 0.7647 - accuracy: 0.7500\n",
      "Epoch 25/500: - 0.0462s/step - loss: 0.7609 - accuracy: 0.7500\n",
      "Epoch 26/500: - 0.0462s/step - loss: 0.7572 - accuracy: 0.7500\n",
      "Epoch 27/500: - 0.0459s/step - loss: 1.2069 - accuracy: 0.7500\n",
      "Epoch 28/500: - 0.0457s/step - loss: 1.1919 - accuracy: 0.7500\n",
      "Epoch 29/500: - 0.0457s/step - loss: 0.7571 - accuracy: 0.7500\n",
      "Epoch 30/500: - 0.0456s/step - loss: 0.7540 - accuracy: 0.7500\n",
      "Epoch 31/500: - 0.0457s/step - loss: 0.7547 - accuracy: 0.7500\n",
      "Epoch 32/500: - 0.0455s/step - loss: 0.7536 - accuracy: 0.7500\n",
      "Epoch 33/500: - 0.0452s/step - loss: 0.7517 - accuracy: 0.7500\n",
      "Epoch 34/500: - 0.0451s/step - loss: 0.7507 - accuracy: 0.7500\n",
      "Epoch 35/500: - 0.0450s/step - loss: 0.7486 - accuracy: 0.7500\n",
      "Epoch 36/500: - 0.0453s/step - loss: 0.7496 - accuracy: 0.7500\n",
      "Epoch 37/500: - 0.0452s/step - loss: 0.7490 - accuracy: 0.7500\n",
      "Epoch 38/500: - 0.0451s/step - loss: 0.7463 - accuracy: 0.7500\n",
      "Epoch 39/500: - 0.0449s/step - loss: 0.7463 - accuracy: 0.7500\n",
      "Epoch 40/500: - 0.0449s/step - loss: 0.7437 - accuracy: 0.7500\n",
      "Epoch 41/500: - 0.0451s/step - loss: 0.7452 - accuracy: 0.7500\n",
      "Epoch 42/500: - 0.0453s/step - loss: 0.7442 - accuracy: 0.7500\n",
      "Epoch 43/500: - 0.0457s/step - loss: 0.7406 - accuracy: 0.7500\n",
      "Epoch 44/500: - 0.0463s/step - loss: 0.7395 - accuracy: 0.7500\n",
      "Epoch 45/500: - 0.0465s/step - loss: 0.7396 - accuracy: 0.7500\n",
      "Epoch 46/500: - 0.0464s/step - loss: 0.7385 - accuracy: 0.7500\n",
      "Epoch 47/500: - 0.0463s/step - loss: 0.7360 - accuracy: 0.7500\n",
      "Epoch 48/500: - 0.0461s/step - loss: 0.7350 - accuracy: 0.7500\n",
      "Epoch 49/500: - 0.0461s/step - loss: 0.7370 - accuracy: 0.7500\n",
      "Epoch 50/500: - 0.0462s/step - loss: 0.7360 - accuracy: 0.7500\n",
      "Epoch 51/500: - 0.0460s/step - loss: 0.7319 - accuracy: 0.7500\n",
      "Epoch 52/500: - 0.0459s/step - loss: 0.7309 - accuracy: 0.7500\n",
      "Epoch 53/500: - 0.0457s/step - loss: 0.7299 - accuracy: 0.7500\n",
      "Epoch 54/500: - 0.0456s/step - loss: 0.7289 - accuracy: 0.7500\n",
      "Epoch 55/500: - 0.0461s/step - loss: 0.7279 - accuracy: 0.7500\n",
      "Epoch 56/500: - 0.0466s/step - loss: 0.7277 - accuracy: 0.7500\n",
      "Epoch 57/500: - 0.0466s/step - loss: 0.7259 - accuracy: 0.7500\n",
      "Epoch 58/500: - 0.0470s/step - loss: 0.7249 - accuracy: 0.7500\n",
      "Epoch 59/500: - 0.0468s/step - loss: 0.7239 - accuracy: 0.7500\n",
      "Epoch 60/500: - 0.0467s/step - loss: 0.7229 - accuracy: 0.7500\n",
      "Epoch 61/500: - 0.0466s/step - loss: 0.7234 - accuracy: 0.7500\n",
      "Epoch 62/500: - 0.0466s/step - loss: 0.7224 - accuracy: 0.7500\n",
      "Epoch 63/500: - 0.0465s/step - loss: 0.7214 - accuracy: 0.7500\n",
      "Epoch 64/500: - 0.0467s/step - loss: 0.7197 - accuracy: 0.7500\n",
      "Epoch 65/500: - 0.0467s/step - loss: 0.7292 - accuracy: 0.7500\n",
      "Epoch 66/500: - 0.0466s/step - loss: 0.7275 - accuracy: 0.7500\n",
      "Epoch 67/500: - 0.0467s/step - loss: 0.7265 - accuracy: 0.7500\n",
      "Epoch 68/500: - 0.0470s/step - loss: 0.7152 - accuracy: 0.7500\n",
      "Epoch 69/500: - 0.0473s/step - loss: 0.7142 - accuracy: 0.7500\n",
      "Epoch 70/500: - 0.0473s/step - loss: 0.7132 - accuracy: 0.7500\n",
      "Epoch 71/500: - 0.0473s/step - loss: 0.7123 - accuracy: 0.7500\n",
      "Epoch 72/500: - 0.0475s/step - loss: 0.7220 - accuracy: 0.7500\n",
      "Epoch 73/500: - 0.0475s/step - loss: 0.7210 - accuracy: 0.7500\n",
      "Epoch 74/500: - 0.0475s/step - loss: 0.7201 - accuracy: 0.7500\n",
      "Epoch 75/500: - 0.0474s/step - loss: 0.7192 - accuracy: 0.7500\n",
      "Epoch 76/500: - 0.0473s/step - loss: 0.7183 - accuracy: 0.7500\n",
      "Epoch 77/500: - 0.0473s/step - loss: 0.7174 - accuracy: 0.7500\n",
      "Epoch 78/500: - 0.0473s/step - loss: 0.7184 - accuracy: 0.7500\n",
      "Epoch 79/500: - 0.0472s/step - loss: 0.7049 - accuracy: 0.7500\n",
      "Epoch 80/500: - 0.0471s/step - loss: 0.7061 - accuracy: 0.7500\n",
      "Epoch 81/500: - 0.0470s/step - loss: 0.7052 - accuracy: 0.7500\n",
      "Epoch 82/500: - 0.0470s/step - loss: 1.1546 - accuracy: 0.7500\n",
      "Epoch 83/500: - 0.0471s/step - loss: 1.1299 - accuracy: 0.7500\n",
      "Epoch 84/500: - 0.0472s/step - loss: 1.1077 - accuracy: 0.7500\n",
      "Epoch 85/500: - 0.0473s/step - loss: 1.0869 - accuracy: 0.7500\n",
      "Epoch 86/500: - 0.0478s/step - loss: 1.0674 - accuracy: 0.7500\n",
      "Epoch 87/500: - 0.0480s/step - loss: 1.0499 - accuracy: 0.7500\n",
      "Epoch 88/500: - 0.0479s/step - loss: 0.7173 - accuracy: 0.7500\n",
      "Epoch 89/500: - 0.0478s/step - loss: 0.7151 - accuracy: 0.7500\n",
      "Epoch 90/500: - 0.0477s/step - loss: 0.7146 - accuracy: 0.7500\n",
      "Epoch 91/500: - 0.0477s/step - loss: 0.7132 - accuracy: 0.7500\n",
      "Epoch 92/500: - 0.0477s/step - loss: 0.7118 - accuracy: 0.7500\n",
      "Epoch 93/500: - 0.0476s/step - loss: 1.8481 - accuracy: 0.7500\n",
      "Epoch 94/500: - 0.0475s/step - loss: 2.7406 - accuracy: 0.7500\n",
      "Epoch 95/500: - 0.0474s/step - loss: 1.5076 - accuracy: 0.7500\n",
      "Epoch 96/500: - 0.0473s/step - loss: 1.4902 - accuracy: 0.7500\n",
      "Epoch 97/500: - 0.0474s/step - loss: 1.4780 - accuracy: 0.7500\n",
      "Epoch 98/500: - 0.0473s/step - loss: 0.7072 - accuracy: 0.7500\n",
      "Epoch 99/500: - 0.0472s/step - loss: 0.7093 - accuracy: 0.7500\n",
      "Epoch 100/500: - 0.0474s/step - loss: 0.7071 - accuracy: 0.7500\n",
      "Epoch 101/500: - 0.0476s/step - loss: 0.7036 - accuracy: 0.7500\n",
      "Epoch 102/500: - 0.0478s/step - loss: 0.7015 - accuracy: 0.7500\n",
      "Epoch 103/500: - 0.0478s/step - loss: 0.7002 - accuracy: 0.7500\n",
      "Epoch 104/500: - 0.0477s/step - loss: 0.7017 - accuracy: 0.7500\n",
      "Epoch 105/500: - 0.0476s/step - loss: 0.7005 - accuracy: 0.7500\n",
      "Epoch 106/500: - 0.0476s/step - loss: 0.6966 - accuracy: 0.7500\n",
      "Epoch 107/500: - 0.0476s/step - loss: 0.6954 - accuracy: 0.7500\n",
      "Epoch 108/500: - 0.0475s/step - loss: 0.6942 - accuracy: 0.7500\n",
      "Epoch 109/500: - 0.0474s/step - loss: 0.6931 - accuracy: 0.7500\n",
      "Epoch 110/500: - 0.0473s/step - loss: 0.6919 - accuracy: 0.7500\n",
      "Epoch 111/500: - 0.0473s/step - loss: 0.6934 - accuracy: 0.7500\n",
      "Epoch 112/500: - 0.0474s/step - loss: 0.6897 - accuracy: 0.7500\n",
      "Epoch 113/500: - 0.0476s/step - loss: 0.6887 - accuracy: 0.7500\n",
      "Epoch 114/500: - 0.0477s/step - loss: 0.6876 - accuracy: 0.7500\n",
      "Epoch 115/500: - 0.0478s/step - loss: 0.6871 - accuracy: 0.7500\n",
      "Epoch 116/500: - 0.0479s/step - loss: 0.6861 - accuracy: 0.7500\n",
      "Epoch 117/500: - 0.0479s/step - loss: 0.6851 - accuracy: 0.7500\n",
      "Epoch 118/500: - 0.0478s/step - loss: 0.6835 - accuracy: 0.7500\n",
      "Epoch 119/500: - 0.0477s/step - loss: 1.2512 - accuracy: 0.7500\n",
      "Epoch 120/500: - 0.0477s/step - loss: 1.2373 - accuracy: 1.0000\n",
      "Epoch 121/500: - 0.0477s/step - loss: 1.2250 - accuracy: 1.0000\n",
      "Epoch 122/500: - 0.0477s/step - loss: 0.6830 - accuracy: 1.0000\n",
      "Epoch 123/500: - 0.0476s/step - loss: 0.6819 - accuracy: 1.0000\n",
      "Epoch 124/500: - 0.0475s/step - loss: 0.6809 - accuracy: 1.0000\n",
      "Epoch 125/500: - 0.0474s/step - loss: 0.6941 - accuracy: 1.0000\n",
      "Epoch 126/500: - 0.0474s/step - loss: 0.6930 - accuracy: 1.0000\n",
      "Epoch 127/500: - 0.0474s/step - loss: 0.6919 - accuracy: 1.0000\n",
      "Epoch 128/500: - 0.0473s/step - loss: 1.2348 - accuracy: 1.0000\n",
      "Epoch 129/500: - 0.0476s/step - loss: 1.2036 - accuracy: 1.0000\n",
      "Epoch 130/500: - 0.0482s/step - loss: 1.1883 - accuracy: 1.0000\n",
      "Epoch 131/500: - 0.0485s/step - loss: 0.6755 - accuracy: 1.0000\n",
      "Epoch 132/500: - 0.0486s/step - loss: 0.6746 - accuracy: 1.0000\n",
      "Epoch 133/500: - 0.0485s/step - loss: 0.6732 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500: - 0.0486s/step - loss: 1.2287 - accuracy: 1.0000\n",
      "Epoch 135/500: - 0.0485s/step - loss: 1.2162 - accuracy: 1.0000\n",
      "Epoch 136/500: - 0.0485s/step - loss: 0.6731 - accuracy: 1.0000\n",
      "Epoch 137/500: - 0.0484s/step - loss: 0.6751 - accuracy: 1.0000\n",
      "Epoch 138/500: - 0.0483s/step - loss: 0.6770 - accuracy: 1.0000\n",
      "Epoch 139/500: - 0.0483s/step - loss: 0.6733 - accuracy: 1.0000\n",
      "Epoch 140/500: - 0.0483s/step - loss: 0.6697 - accuracy: 1.0000\n",
      "Epoch 141/500: - 0.0483s/step - loss: 0.6718 - accuracy: 1.0000\n",
      "Epoch 142/500: - 0.0483s/step - loss: 0.6709 - accuracy: 1.0000\n",
      "Epoch 143/500: - 0.0487s/step - loss: 0.6709 - accuracy: 1.0000\n",
      "Epoch 144/500: - 0.0489s/step - loss: 1.4834 - accuracy: 1.0000\n",
      "Epoch 145/500: - 0.0491s/step - loss: 1.4688 - accuracy: 1.0000\n",
      "Epoch 146/500: - 0.0490s/step - loss: 1.4497 - accuracy: 1.0000\n",
      "Epoch 147/500: - 0.0489s/step - loss: 1.4316 - accuracy: 1.0000\n",
      "Epoch 148/500: - 0.0489s/step - loss: 0.6649 - accuracy: 1.0000\n",
      "Epoch 149/500: - 0.0488s/step - loss: 0.6640 - accuracy: 1.0000\n",
      "Epoch 150/500: - 0.0488s/step - loss: 0.6634 - accuracy: 1.0000\n",
      "Epoch 151/500: - 0.0487s/step - loss: 0.7514 - accuracy: 1.0000\n",
      "Epoch 152/500: - 0.0487s/step - loss: 0.7495 - accuracy: 1.0000\n",
      "Epoch 153/500: - 0.0486s/step - loss: 1.1454 - accuracy: 1.0000\n",
      "Epoch 154/500: - 0.0486s/step - loss: 1.1218 - accuracy: 1.0000\n",
      "Epoch 155/500: - 0.0487s/step - loss: 1.0998 - accuracy: 1.0000\n",
      "Epoch 156/500: - 0.0488s/step - loss: 1.0791 - accuracy: 1.0000\n",
      "Epoch 157/500: - 0.0489s/step - loss: 1.0598 - accuracy: 1.0000\n",
      "Epoch 158/500: - 0.0490s/step - loss: 0.6725 - accuracy: 1.0000\n",
      "Epoch 159/500: - 0.0490s/step - loss: 0.6687 - accuracy: 1.0000\n",
      "Epoch 160/500: - 0.0490s/step - loss: 1.4630 - accuracy: 1.0000\n",
      "Epoch 161/500: - 0.0489s/step - loss: 1.2708 - accuracy: 1.0000\n",
      "Epoch 162/500: - 0.0488s/step - loss: 1.2601 - accuracy: 1.0000\n",
      "Epoch 163/500: - 0.0488s/step - loss: 1.2499 - accuracy: 1.0000\n",
      "Epoch 164/500: - 0.0488s/step - loss: 1.1941 - accuracy: 1.0000\n",
      "Epoch 165/500: - 0.0487s/step - loss: 1.1880 - accuracy: 1.0000\n",
      "Epoch 166/500: - 0.0487s/step - loss: 1.1785 - accuracy: 1.0000\n",
      "Epoch 167/500: - 0.0486s/step - loss: 0.6721 - accuracy: 1.0000\n",
      "Epoch 168/500: - 0.0486s/step - loss: 0.6707 - accuracy: 1.0000\n",
      "Epoch 169/500: - 0.0486s/step - loss: 0.6674 - accuracy: 1.0000\n",
      "Epoch 170/500: - 0.0486s/step - loss: 0.6656 - accuracy: 1.0000\n",
      "Epoch 171/500: - 0.0486s/step - loss: 1.0677 - accuracy: 1.0000\n",
      "Epoch 172/500: - 0.0486s/step - loss: 1.3322 - accuracy: 1.0000\n",
      "Epoch 173/500: - 0.0487s/step - loss: 1.3132 - accuracy: 1.0000\n",
      "Epoch 174/500: - 0.0487s/step - loss: 0.6616 - accuracy: 1.0000\n",
      "Epoch 175/500: - 0.0488s/step - loss: 0.6603 - accuracy: 1.0000\n",
      "Epoch 176/500: - 0.0488s/step - loss: 0.6591 - accuracy: 1.0000\n",
      "Epoch 177/500: - 0.0489s/step - loss: 0.6574 - accuracy: 1.0000\n",
      "Epoch 178/500: - 0.0490s/step - loss: 0.6565 - accuracy: 1.0000\n",
      "Epoch 179/500: - 0.0491s/step - loss: 0.6554 - accuracy: 1.0000\n",
      "Epoch 180/500: - 0.0491s/step - loss: 0.6543 - accuracy: 1.0000\n",
      "Epoch 181/500: - 0.0492s/step - loss: 0.6530 - accuracy: 1.0000\n",
      "Epoch 182/500: - 0.0492s/step - loss: 0.6520 - accuracy: 1.0000\n",
      "Epoch 183/500: - 0.0493s/step - loss: 0.6509 - accuracy: 1.0000\n",
      "Epoch 184/500: - 0.0494s/step - loss: 0.6499 - accuracy: 1.0000\n",
      "Epoch 185/500: - 0.0494s/step - loss: 0.6489 - accuracy: 1.0000\n",
      "Epoch 186/500: - 0.0495s/step - loss: 0.6497 - accuracy: 1.0000\n",
      "Epoch 187/500: - 0.0495s/step - loss: 0.6470 - accuracy: 1.0000\n",
      "Epoch 188/500: - 0.0494s/step - loss: 0.6481 - accuracy: 1.0000\n",
      "Epoch 189/500: - 0.0494s/step - loss: 0.6461 - accuracy: 1.0000\n",
      "Epoch 190/500: - 0.0494s/step - loss: 0.6443 - accuracy: 1.0000\n",
      "Epoch 191/500: - 0.0494s/step - loss: 0.6438 - accuracy: 1.0000\n",
      "Epoch 192/500: - 0.0495s/step - loss: 0.6425 - accuracy: 1.0000\n",
      "Epoch 193/500: - 0.0495s/step - loss: 0.6416 - accuracy: 1.0000\n",
      "Epoch 194/500: - 0.0494s/step - loss: 0.6408 - accuracy: 1.0000\n",
      "Epoch 195/500: - 0.0495s/step - loss: 1.1172 - accuracy: 1.0000\n",
      "Epoch 196/500: - 0.0495s/step - loss: 0.6415 - accuracy: 1.0000\n",
      "Epoch 197/500: - 0.0495s/step - loss: 1.2754 - accuracy: 1.0000\n",
      "Epoch 198/500: - 0.0496s/step - loss: 1.2580 - accuracy: 1.0000\n",
      "Epoch 199/500: - 0.0496s/step - loss: 1.1306 - accuracy: 1.0000\n",
      "Epoch 200/500: - 0.0496s/step - loss: 1.1066 - accuracy: 1.0000\n",
      "Epoch 201/500: - 0.0495s/step - loss: 1.0852 - accuracy: 1.0000\n",
      "Epoch 202/500: - 0.0495s/step - loss: 1.0652 - accuracy: 1.0000\n",
      "Epoch 203/500: - 0.0495s/step - loss: 1.0466 - accuracy: 1.0000\n",
      "Epoch 204/500: - 0.0495s/step - loss: 0.6503 - accuracy: 1.0000\n",
      "Epoch 205/500: - 0.0495s/step - loss: 0.6483 - accuracy: 1.0000\n",
      "Epoch 206/500: - 0.0495s/step - loss: 0.6468 - accuracy: 1.0000\n",
      "Epoch 207/500: - 0.0495s/step - loss: 0.6454 - accuracy: 1.0000\n",
      "Epoch 208/500: - 0.0495s/step - loss: 0.6467 - accuracy: 1.0000\n",
      "Epoch 209/500: - 0.0496s/step - loss: 0.6427 - accuracy: 1.0000\n",
      "Epoch 210/500: - 0.0495s/step - loss: 0.6433 - accuracy: 1.0000\n",
      "Epoch 211/500: - 0.0495s/step - loss: 0.6421 - accuracy: 1.0000\n",
      "Epoch 212/500: - 0.0494s/step - loss: 0.6437 - accuracy: 1.0000\n",
      "Epoch 213/500: - 0.0494s/step - loss: 0.6410 - accuracy: 1.0000\n",
      "Epoch 214/500: - 0.0495s/step - loss: 0.6413 - accuracy: 1.0000\n",
      "Epoch 215/500: - 0.0495s/step - loss: 0.6384 - accuracy: 1.0000\n",
      "Epoch 216/500: - 0.0495s/step - loss: 0.6374 - accuracy: 1.0000\n",
      "Epoch 217/500: - 0.0495s/step - loss: 0.6382 - accuracy: 1.0000\n",
      "Epoch 218/500: - 0.0495s/step - loss: 0.6369 - accuracy: 1.0000\n",
      "Epoch 219/500: - 0.0496s/step - loss: 0.6360 - accuracy: 1.0000\n",
      "Epoch 220/500: - 0.0495s/step - loss: 0.6335 - accuracy: 1.0000\n",
      "Epoch 221/500: - 0.0495s/step - loss: 0.6294 - accuracy: 1.0000\n",
      "Epoch 222/500: - 0.0495s/step - loss: 0.6284 - accuracy: 1.0000\n",
      "Epoch 223/500: - 0.0495s/step - loss: 0.6275 - accuracy: 1.0000\n",
      "Epoch 224/500: - 0.0495s/step - loss: 0.6299 - accuracy: 1.0000\n",
      "Epoch 225/500: - 0.0496s/step - loss: 0.6290 - accuracy: 1.0000\n",
      "Epoch 226/500: - 0.0496s/step - loss: 0.6274 - accuracy: 1.0000\n",
      "Epoch 227/500: - 0.0497s/step - loss: 0.6238 - accuracy: 1.0000\n",
      "Epoch 228/500: - 0.0497s/step - loss: 0.6229 - accuracy: 1.0000\n",
      "Epoch 229/500: - 0.0497s/step - loss: 0.6220 - accuracy: 1.0000\n",
      "Epoch 230/500: - 0.0498s/step - loss: 0.6235 - accuracy: 1.0000\n",
      "Epoch 231/500: - 0.0498s/step - loss: 0.6236 - accuracy: 1.0000\n",
      "Epoch 232/500: - 0.0498s/step - loss: 0.6221 - accuracy: 1.0000\n",
      "Epoch 233/500: - 0.0498s/step - loss: 0.6204 - accuracy: 1.0000\n",
      "Epoch 234/500: - 0.0497s/step - loss: 0.6185 - accuracy: 1.0000\n",
      "Epoch 235/500: - 0.0497s/step - loss: 0.6185 - accuracy: 1.0000\n",
      "Epoch 236/500: - 0.0498s/step - loss: 1.3988 - accuracy: 1.0000\n",
      "Epoch 237/500: - 0.0498s/step - loss: 0.6185 - accuracy: 1.0000\n",
      "Epoch 238/500: - 0.0498s/step - loss: 0.6177 - accuracy: 1.0000\n",
      "Epoch 239/500: - 0.0499s/step - loss: 0.6169 - accuracy: 1.0000\n",
      "Epoch 240/500: - 0.0499s/step - loss: 0.6134 - accuracy: 1.0000\n",
      "Epoch 241/500: - 0.0499s/step - loss: 0.6127 - accuracy: 1.0000\n",
      "Epoch 242/500: - 0.0499s/step - loss: 0.6119 - accuracy: 1.0000\n",
      "Epoch 243/500: - 0.0499s/step - loss: 0.6112 - accuracy: 1.0000\n",
      "Epoch 244/500: - 0.0499s/step - loss: 0.6104 - accuracy: 1.0000\n",
      "Epoch 245/500: - 0.0499s/step - loss: 0.6097 - accuracy: 1.0000\n",
      "Epoch 246/500: - 0.0499s/step - loss: 0.6090 - accuracy: 1.0000\n",
      "Epoch 247/500: - 0.0499s/step - loss: 0.6083 - accuracy: 1.0000\n",
      "Epoch 248/500: - 0.0499s/step - loss: 0.6075 - accuracy: 1.0000\n",
      "Epoch 249/500: - 0.0499s/step - loss: 0.6068 - accuracy: 1.0000\n",
      "Epoch 250/500: - 0.0499s/step - loss: 0.6061 - accuracy: 1.0000\n",
      "Epoch 251/500: - 0.0499s/step - loss: 0.6054 - accuracy: 1.0000\n",
      "Epoch 252/500: - 0.0498s/step - loss: 0.6047 - accuracy: 1.0000\n",
      "Epoch 253/500: - 0.0498s/step - loss: 0.6040 - accuracy: 1.0000\n",
      "Epoch 254/500: - 0.0499s/step - loss: 1.1458 - accuracy: 1.0000\n",
      "Epoch 255/500: - 0.0499s/step - loss: 1.1327 - accuracy: 1.0000\n",
      "Epoch 256/500: - 0.0499s/step - loss: 0.6025 - accuracy: 1.0000\n",
      "Epoch 257/500: - 0.0500s/step - loss: 0.6018 - accuracy: 1.0000\n",
      "Epoch 258/500: - 0.0500s/step - loss: 0.6012 - accuracy: 1.0000\n",
      "Epoch 259/500: - 0.0501s/step - loss: 0.6019 - accuracy: 1.0000\n",
      "Epoch 260/500: - 0.0501s/step - loss: 0.6013 - accuracy: 1.0000\n",
      "Epoch 261/500: - 0.0501s/step - loss: 0.6006 - accuracy: 1.0000\n",
      "Epoch 262/500: - 0.0501s/step - loss: 0.6022 - accuracy: 1.0000\n",
      "Epoch 263/500: - 0.0501s/step - loss: 0.6002 - accuracy: 1.0000\n",
      "Epoch 264/500: - 0.0501s/step - loss: 0.5973 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/500: - 0.0501s/step - loss: 0.5966 - accuracy: 1.0000\n",
      "Epoch 266/500: - 0.0502s/step - loss: 0.5960 - accuracy: 1.0000\n",
      "Epoch 267/500: - 0.0502s/step - loss: 0.5967 - accuracy: 1.0000\n",
      "Epoch 268/500: - 0.0501s/step - loss: 0.5978 - accuracy: 1.0000\n",
      "Epoch 269/500: - 0.0501s/step - loss: 0.5957 - accuracy: 1.0000\n",
      "Epoch 270/500: - 0.0501s/step - loss: 0.5951 - accuracy: 1.0000\n",
      "Epoch 271/500: - 0.0500s/step - loss: 0.5944 - accuracy: 1.0000\n",
      "Epoch 272/500: - 0.0500s/step - loss: 0.5938 - accuracy: 1.0000\n",
      "Epoch 273/500: - 0.0500s/step - loss: 0.5932 - accuracy: 1.0000\n",
      "Epoch 274/500: - 0.0499s/step - loss: 0.5925 - accuracy: 1.0000\n",
      "Epoch 275/500: - 0.0499s/step - loss: 0.5902 - accuracy: 1.0000\n",
      "Epoch 276/500: - 0.0499s/step - loss: 0.5895 - accuracy: 1.0000\n",
      "Epoch 277/500: - 0.0499s/step - loss: 0.5889 - accuracy: 1.0000\n",
      "Epoch 278/500: - 0.0498s/step - loss: 0.5905 - accuracy: 1.0000\n",
      "Epoch 279/500: - 0.0498s/step - loss: 0.5898 - accuracy: 1.0000\n",
      "Epoch 280/500: - 0.0497s/step - loss: 0.5870 - accuracy: 1.0000\n",
      "Epoch 281/500: - 0.0498s/step - loss: 0.5864 - accuracy: 1.0000\n",
      "Epoch 282/500: - 0.0498s/step - loss: 0.5882 - accuracy: 1.0000\n",
      "Epoch 283/500: - 0.0499s/step - loss: 0.5876 - accuracy: 1.0000\n",
      "Epoch 284/500: - 0.0499s/step - loss: 0.5845 - accuracy: 1.0000\n",
      "Epoch 285/500: - 0.0498s/step - loss: 1.2309 - accuracy: 1.0000\n",
      "Epoch 286/500: - 0.0498s/step - loss: 1.2171 - accuracy: 1.0000\n",
      "Epoch 287/500: - 0.0499s/step - loss: 1.2041 - accuracy: 1.0000\n",
      "Epoch 288/500: - 0.0499s/step - loss: 1.1871 - accuracy: 1.0000\n",
      "Epoch 289/500: - 0.0499s/step - loss: 1.1752 - accuracy: 1.0000\n",
      "Epoch 290/500: - 0.0500s/step - loss: 1.1640 - accuracy: 1.0000\n",
      "Epoch 291/500: - 0.0500s/step - loss: 1.1533 - accuracy: 1.0000\n",
      "Epoch 292/500: - 0.0500s/step - loss: 1.1444 - accuracy: 1.0000\n",
      "Epoch 293/500: - 0.0501s/step - loss: 1.1349 - accuracy: 1.0000\n",
      "Epoch 294/500: - 0.0501s/step - loss: 0.5953 - accuracy: 1.0000\n",
      "Epoch 295/500: - 0.0501s/step - loss: 0.5929 - accuracy: 1.0000\n",
      "Epoch 296/500: - 0.0501s/step - loss: 0.5936 - accuracy: 1.0000\n",
      "Epoch 297/500: - 0.0502s/step - loss: 0.5924 - accuracy: 1.0000\n",
      "Epoch 298/500: - 0.0502s/step - loss: 0.5913 - accuracy: 1.0000\n",
      "Epoch 299/500: - 0.0501s/step - loss: 0.5902 - accuracy: 1.0000\n",
      "Epoch 300/500: - 0.0501s/step - loss: 1.1377 - accuracy: 1.0000\n",
      "Epoch 301/500: - 0.0501s/step - loss: 1.1283 - accuracy: 1.0000\n",
      "Epoch 302/500: - 0.0501s/step - loss: 1.1224 - accuracy: 1.0000\n",
      "Epoch 303/500: - 0.0501s/step - loss: 1.1119 - accuracy: 1.0000\n",
      "Epoch 304/500: - 0.0500s/step - loss: 1.1037 - accuracy: 1.0000\n",
      "Epoch 305/500: - 0.0500s/step - loss: 1.6056 - accuracy: 1.0000\n",
      "Epoch 306/500: - 0.0500s/step - loss: 1.5656 - accuracy: 1.0000\n",
      "Epoch 307/500: - 0.0499s/step - loss: 1.5286 - accuracy: 1.0000\n",
      "Epoch 308/500: - 0.0499s/step - loss: 1.4944 - accuracy: 1.0000\n",
      "Epoch 309/500: - 0.0499s/step - loss: 1.0287 - accuracy: 1.0000\n",
      "Epoch 310/500: - 0.0499s/step - loss: 1.9308 - accuracy: 1.0000\n",
      "Epoch 311/500: - 0.0499s/step - loss: 1.9226 - accuracy: 1.0000\n",
      "Epoch 312/500: - 0.0499s/step - loss: 1.9151 - accuracy: 1.0000\n",
      "Epoch 313/500: - 0.0498s/step - loss: 1.9073 - accuracy: 1.0000\n",
      "Epoch 314/500: - 0.0499s/step - loss: 1.8992 - accuracy: 1.0000\n",
      "Epoch 315/500: - 0.0498s/step - loss: 1.8919 - accuracy: 1.0000\n",
      "Epoch 316/500: - 0.0498s/step - loss: 1.8847 - accuracy: 1.0000\n",
      "Epoch 317/500: - 0.0498s/step - loss: 1.5620 - accuracy: 1.0000\n",
      "Epoch 318/500: - 0.0498s/step - loss: 1.5482 - accuracy: 1.0000\n",
      "Epoch 319/500: - 0.0498s/step - loss: 0.6351 - accuracy: 1.0000\n",
      "Epoch 320/500: - 0.0498s/step - loss: 0.6314 - accuracy: 1.0000\n",
      "Epoch 321/500: - 0.0499s/step - loss: 0.9899 - accuracy: 1.0000\n",
      "Epoch 322/500: - 0.0499s/step - loss: 0.9763 - accuracy: 1.0000\n",
      "Epoch 323/500: - 0.0500s/step - loss: 0.6363 - accuracy: 1.0000\n",
      "Epoch 324/500: - 0.0500s/step - loss: 0.6324 - accuracy: 1.0000\n",
      "Epoch 325/500: - 0.0500s/step - loss: 0.6288 - accuracy: 1.0000\n",
      "Epoch 326/500: - 0.0501s/step - loss: 0.6257 - accuracy: 1.0000\n",
      "Epoch 327/500: - 0.0501s/step - loss: 0.6225 - accuracy: 1.0000\n",
      "Epoch 328/500: - 0.0501s/step - loss: 0.6192 - accuracy: 1.0000\n",
      "Epoch 329/500: - 0.0501s/step - loss: 0.6164 - accuracy: 1.0000\n",
      "Epoch 330/500: - 0.0502s/step - loss: 0.6137 - accuracy: 1.0000\n",
      "Epoch 331/500: - 0.0502s/step - loss: 0.6112 - accuracy: 1.0000\n",
      "Epoch 332/500: - 0.0502s/step - loss: 0.6088 - accuracy: 1.0000\n",
      "Epoch 333/500: - 0.0502s/step - loss: 0.6065 - accuracy: 1.0000\n",
      "Epoch 334/500: - 0.0502s/step - loss: 0.6105 - accuracy: 1.0000\n",
      "Epoch 335/500: - 0.0502s/step - loss: 0.6023 - accuracy: 1.0000\n",
      "Epoch 336/500: - 0.0502s/step - loss: 0.6004 - accuracy: 1.0000\n",
      "Epoch 337/500: - 0.0501s/step - loss: 0.5985 - accuracy: 1.0000\n",
      "Epoch 338/500: - 0.0501s/step - loss: 0.5978 - accuracy: 1.0000\n",
      "Epoch 339/500: - 0.0501s/step - loss: 0.5961 - accuracy: 1.0000\n",
      "Epoch 340/500: - 0.0501s/step - loss: 0.5945 - accuracy: 1.0000\n",
      "Epoch 341/500: - 0.0500s/step - loss: 0.5929 - accuracy: 1.0000\n",
      "Epoch 342/500: - 0.0500s/step - loss: 1.3158 - accuracy: 1.0000\n",
      "Epoch 343/500: - 0.0500s/step - loss: 1.2941 - accuracy: 1.0000\n",
      "Epoch 344/500: - 0.0500s/step - loss: 1.2713 - accuracy: 1.0000\n",
      "Epoch 345/500: - 0.0501s/step - loss: 0.5844 - accuracy: 1.0000\n",
      "Epoch 346/500: - 0.0501s/step - loss: 0.5834 - accuracy: 1.0000\n",
      "Epoch 347/500: - 0.0501s/step - loss: 0.5807 - accuracy: 1.0000\n",
      "Epoch 348/500: - 0.0501s/step - loss: 0.5797 - accuracy: 1.0000\n",
      "Epoch 349/500: - 0.0501s/step - loss: 0.5788 - accuracy: 1.0000\n",
      "Epoch 350/500: - 0.0501s/step - loss: 0.5780 - accuracy: 1.0000\n",
      "Epoch 351/500: - 0.0501s/step - loss: 0.5771 - accuracy: 1.0000\n",
      "Epoch 352/500: - 0.0500s/step - loss: 0.5763 - accuracy: 1.0000\n",
      "Epoch 353/500: - 0.0500s/step - loss: 0.5754 - accuracy: 1.0000\n",
      "Epoch 354/500: - 0.0500s/step - loss: 0.5746 - accuracy: 1.0000\n",
      "Epoch 355/500: - 0.0500s/step - loss: 0.5757 - accuracy: 1.0000\n",
      "Epoch 356/500: - 0.0500s/step - loss: 0.5782 - accuracy: 1.0000\n",
      "Epoch 357/500: - 0.0499s/step - loss: 0.5755 - accuracy: 1.0000\n",
      "Epoch 358/500: - 0.0500s/step - loss: 0.5717 - accuracy: 1.0000\n",
      "Epoch 359/500: - 0.0501s/step - loss: 0.5707 - accuracy: 1.0000\n",
      "Epoch 360/500: - 0.0501s/step - loss: 0.5700 - accuracy: 1.0000\n",
      "Epoch 361/500: - 0.0502s/step - loss: 0.5712 - accuracy: 1.0000\n",
      "Epoch 362/500: - 0.0502s/step - loss: 0.5685 - accuracy: 1.0000\n",
      "Epoch 363/500: - 0.0502s/step - loss: 0.5678 - accuracy: 1.0000\n",
      "Epoch 364/500: - 0.0502s/step - loss: 0.5671 - accuracy: 1.0000\n",
      "Epoch 365/500: - 0.0501s/step - loss: 0.5670 - accuracy: 1.0000\n",
      "Epoch 366/500: - 0.0501s/step - loss: 0.5684 - accuracy: 1.0000\n",
      "Epoch 367/500: - 0.0502s/step - loss: 1.4445 - accuracy: 1.0000\n",
      "Epoch 368/500: - 0.0502s/step - loss: 1.4403 - accuracy: 1.0000\n",
      "Epoch 369/500: - 0.0502s/step - loss: 2.0903 - accuracy: 1.0000\n",
      "Epoch 370/500: - 0.0502s/step - loss: 2.0552 - accuracy: 1.0000\n",
      "Epoch 371/500: - 0.0502s/step - loss: 2.0193 - accuracy: 1.0000\n",
      "Epoch 372/500: - 0.0503s/step - loss: 1.9889 - accuracy: 1.0000\n",
      "Epoch 373/500: - 0.0503s/step - loss: 1.1482 - accuracy: 1.0000\n",
      "Epoch 374/500: - 0.0503s/step - loss: 1.2174 - accuracy: 1.0000\n",
      "Epoch 375/500: - 0.0504s/step - loss: 1.2039 - accuracy: 1.0000\n",
      "Epoch 376/500: - 0.0504s/step - loss: 1.1923 - accuracy: 1.0000\n",
      "Epoch 377/500: - 0.0504s/step - loss: 1.1730 - accuracy: 1.0000\n",
      "Epoch 378/500: - 0.0504s/step - loss: 0.5746 - accuracy: 1.0000\n",
      "Epoch 379/500: - 0.0503s/step - loss: 0.5737 - accuracy: 1.0000\n",
      "Epoch 380/500: - 0.0503s/step - loss: 0.5731 - accuracy: 1.0000\n",
      "Epoch 381/500: - 0.0504s/step - loss: 0.5754 - accuracy: 1.0000\n",
      "Epoch 382/500: - 0.0504s/step - loss: 0.5719 - accuracy: 1.0000\n",
      "Epoch 383/500: - 0.0503s/step - loss: 0.5701 - accuracy: 1.0000\n",
      "Epoch 384/500: - 0.0503s/step - loss: 1.1586 - accuracy: 1.0000\n",
      "Epoch 385/500: - 0.0503s/step - loss: 0.5702 - accuracy: 1.0000\n",
      "Epoch 386/500: - 0.0503s/step - loss: 0.5693 - accuracy: 1.0000\n",
      "Epoch 387/500: - 0.0503s/step - loss: 1.1465 - accuracy: 1.0000\n",
      "Epoch 388/500: - 0.0503s/step - loss: 1.1328 - accuracy: 1.0000\n",
      "Epoch 389/500: - 0.0502s/step - loss: 1.1197 - accuracy: 1.0000\n",
      "Epoch 390/500: - 0.0502s/step - loss: 1.1094 - accuracy: 1.0000\n",
      "Epoch 391/500: - 0.0502s/step - loss: 1.0974 - accuracy: 1.0000\n",
      "Epoch 392/500: - 0.0502s/step - loss: 0.5785 - accuracy: 1.0000\n",
      "Epoch 393/500: - 0.0502s/step - loss: 0.5802 - accuracy: 1.0000\n",
      "Epoch 394/500: - 0.0502s/step - loss: 0.5739 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/500: - 0.0503s/step - loss: 0.5727 - accuracy: 1.0000\n",
      "Epoch 396/500: - 0.0502s/step - loss: 0.5715 - accuracy: 1.0000\n",
      "Epoch 397/500: - 0.0502s/step - loss: 0.5704 - accuracy: 1.0000\n",
      "Epoch 398/500: - 0.0502s/step - loss: 0.5693 - accuracy: 1.0000\n",
      "Epoch 399/500: - 0.0502s/step - loss: 0.5682 - accuracy: 1.0000\n",
      "Epoch 400/500: - 0.0502s/step - loss: 0.5673 - accuracy: 1.0000\n",
      "Epoch 401/500: - 0.0502s/step - loss: 0.5662 - accuracy: 1.0000\n",
      "Epoch 402/500: - 0.0502s/step - loss: 0.5880 - accuracy: 1.0000\n",
      "Epoch 403/500: - 0.0502s/step - loss: 0.5880 - accuracy: 1.0000\n",
      "Epoch 404/500: - 0.0502s/step - loss: 0.5870 - accuracy: 1.0000\n",
      "Epoch 405/500: - 0.0503s/step - loss: 0.5860 - accuracy: 1.0000\n",
      "Epoch 406/500: - 0.0503s/step - loss: 0.5851 - accuracy: 1.0000\n",
      "Epoch 407/500: - 0.0502s/step - loss: 0.5619 - accuracy: 1.0000\n",
      "Epoch 408/500: - 0.0502s/step - loss: 0.5610 - accuracy: 1.0000\n",
      "Epoch 409/500: - 0.0502s/step - loss: 0.5602 - accuracy: 1.0000\n",
      "Epoch 410/500: - 0.0502s/step - loss: 0.5579 - accuracy: 1.0000\n",
      "Epoch 411/500: - 0.0502s/step - loss: 0.5571 - accuracy: 1.0000\n",
      "Epoch 412/500: - 0.0501s/step - loss: 0.5568 - accuracy: 1.0000\n",
      "Epoch 413/500: - 0.0501s/step - loss: 0.5564 - accuracy: 1.0000\n",
      "Epoch 414/500: - 0.0501s/step - loss: 0.5547 - accuracy: 1.0000\n",
      "Epoch 415/500: - 0.0501s/step - loss: 0.5539 - accuracy: 1.0000\n",
      "Epoch 416/500: - 0.0501s/step - loss: 0.5531 - accuracy: 1.0000\n",
      "Epoch 417/500: - 0.0500s/step - loss: 0.5524 - accuracy: 1.0000\n",
      "Epoch 418/500: - 0.0500s/step - loss: 0.5519 - accuracy: 1.0000\n",
      "Epoch 419/500: - 0.0500s/step - loss: 0.5509 - accuracy: 1.0000\n",
      "Epoch 420/500: - 0.0500s/step - loss: 0.5502 - accuracy: 1.0000\n",
      "Epoch 421/500: - 0.0500s/step - loss: 0.5605 - accuracy: 1.0000\n",
      "Epoch 422/500: - 0.0499s/step - loss: 1.1300 - accuracy: 1.0000\n",
      "Epoch 423/500: - 0.0499s/step - loss: 2.3077 - accuracy: 1.0000\n",
      "Epoch 424/500: - 0.0499s/step - loss: 2.2763 - accuracy: 1.0000\n",
      "Epoch 425/500: - 0.0499s/step - loss: 1.2186 - accuracy: 1.0000\n",
      "Epoch 426/500: - 0.0499s/step - loss: 1.2032 - accuracy: 1.0000\n",
      "Epoch 427/500: - 0.0498s/step - loss: 0.5578 - accuracy: 1.0000\n",
      "Epoch 428/500: - 0.0498s/step - loss: 0.5751 - accuracy: 1.0000\n",
      "Epoch 429/500: - 0.0499s/step - loss: 0.5742 - accuracy: 1.0000\n",
      "Epoch 430/500: - 0.0499s/step - loss: 0.5753 - accuracy: 1.0000\n",
      "Epoch 431/500: - 0.0499s/step - loss: 1.5613 - accuracy: 1.0000\n",
      "Epoch 432/500: - 0.0498s/step - loss: 1.5490 - accuracy: 1.0000\n",
      "Epoch 433/500: - 0.0498s/step - loss: 0.5521 - accuracy: 1.0000\n",
      "Epoch 434/500: - 0.0499s/step - loss: 0.5512 - accuracy: 1.0000\n",
      "Epoch 435/500: - 0.0498s/step - loss: 0.5505 - accuracy: 1.0000\n",
      "Epoch 436/500: - 0.0498s/step - loss: 0.5496 - accuracy: 1.0000\n",
      "Epoch 437/500: - 0.0498s/step - loss: 0.5489 - accuracy: 1.0000\n",
      "Epoch 438/500: - 0.0498s/step - loss: 0.5481 - accuracy: 1.0000\n",
      "Epoch 439/500: - 0.0499s/step - loss: 0.5473 - accuracy: 1.0000\n",
      "Epoch 440/500: - 0.0499s/step - loss: 0.5465 - accuracy: 1.0000\n",
      "Epoch 441/500: - 0.0499s/step - loss: 0.5456 - accuracy: 1.0000\n",
      "Epoch 442/500: - 0.0498s/step - loss: 0.5470 - accuracy: 1.0000\n",
      "Epoch 443/500: - 0.0498s/step - loss: 0.5464 - accuracy: 1.0000\n",
      "Epoch 444/500: - 0.0499s/step - loss: 0.5456 - accuracy: 1.0000\n",
      "Epoch 445/500: - 0.0499s/step - loss: 0.5448 - accuracy: 1.0000\n",
      "Epoch 446/500: - 0.0498s/step - loss: 0.5441 - accuracy: 1.0000\n",
      "Epoch 447/500: - 0.0498s/step - loss: 0.5457 - accuracy: 1.0000\n",
      "Epoch 448/500: - 0.0498s/step - loss: 0.5633 - accuracy: 1.0000\n",
      "Epoch 449/500: - 0.0498s/step - loss: 0.5422 - accuracy: 1.0000\n",
      "Epoch 450/500: - 0.0498s/step - loss: 0.5415 - accuracy: 1.0000\n",
      "Epoch 451/500: - 0.0498s/step - loss: 0.5408 - accuracy: 1.0000\n",
      "Epoch 452/500: - 0.0498s/step - loss: 0.5402 - accuracy: 1.0000\n",
      "Epoch 453/500: - 0.0499s/step - loss: 0.5372 - accuracy: 1.0000\n",
      "Epoch 454/500: - 0.0499s/step - loss: 0.5377 - accuracy: 1.0000\n",
      "Epoch 455/500: - 0.0498s/step - loss: 0.5370 - accuracy: 1.0000\n",
      "Epoch 456/500: - 0.0498s/step - loss: 0.5364 - accuracy: 1.0000\n",
      "Epoch 457/500: - 0.0498s/step - loss: 0.5358 - accuracy: 1.0000\n",
      "Epoch 458/500: - 0.0498s/step - loss: 0.5340 - accuracy: 1.0000\n",
      "Epoch 459/500: - 0.0498s/step - loss: 0.5334 - accuracy: 1.0000\n",
      "Epoch 460/500: - 0.0498s/step - loss: 0.5328 - accuracy: 1.0000\n",
      "Epoch 461/500: - 0.0498s/step - loss: 0.5322 - accuracy: 1.0000\n",
      "Epoch 462/500: - 0.0498s/step - loss: 0.5316 - accuracy: 1.0000\n",
      "Epoch 463/500: - 0.0498s/step - loss: 0.5315 - accuracy: 1.0000\n",
      "Epoch 464/500: - 0.0498s/step - loss: 0.5309 - accuracy: 1.0000\n",
      "Epoch 465/500: - 0.0498s/step - loss: 0.5318 - accuracy: 1.0000\n",
      "Epoch 466/500: - 0.0498s/step - loss: 0.5314 - accuracy: 1.0000\n",
      "Epoch 467/500: - 0.0498s/step - loss: 0.5293 - accuracy: 1.0000\n",
      "Epoch 468/500: - 0.0498s/step - loss: 0.5287 - accuracy: 1.0000\n",
      "Epoch 469/500: - 0.0498s/step - loss: 0.5280 - accuracy: 1.0000\n",
      "Epoch 470/500: - 0.0498s/step - loss: 0.5274 - accuracy: 1.0000\n",
      "Epoch 471/500: - 0.0498s/step - loss: 0.5264 - accuracy: 1.0000\n",
      "Epoch 472/500: - 0.0497s/step - loss: 0.5277 - accuracy: 1.0000\n",
      "Epoch 473/500: - 0.0497s/step - loss: 1.4860 - accuracy: 1.0000\n",
      "Epoch 474/500: - 0.0497s/step - loss: 1.4675 - accuracy: 1.0000\n",
      "Epoch 475/500: - 0.0497s/step - loss: 0.5478 - accuracy: 1.0000\n",
      "Epoch 476/500: - 0.0497s/step - loss: 0.5259 - accuracy: 1.0000\n",
      "Epoch 477/500: - 0.0497s/step - loss: 0.5254 - accuracy: 1.0000\n",
      "Epoch 478/500: - 0.0497s/step - loss: 0.5232 - accuracy: 1.0000\n",
      "Epoch 479/500: - 0.0498s/step - loss: 0.5245 - accuracy: 1.0000\n",
      "Epoch 480/500: - 0.0498s/step - loss: 0.5221 - accuracy: 1.0000\n",
      "Epoch 481/500: - 0.0498s/step - loss: 0.5215 - accuracy: 1.0000\n",
      "Epoch 482/500: - 0.0498s/step - loss: 0.5210 - accuracy: 1.0000\n",
      "Epoch 483/500: - 0.0498s/step - loss: 1.2348 - accuracy: 1.0000\n",
      "Epoch 484/500: - 0.0498s/step - loss: 1.2196 - accuracy: 1.0000\n",
      "Epoch 485/500: - 0.0497s/step - loss: 1.2052 - accuracy: 1.0000\n",
      "Epoch 486/500: - 0.0497s/step - loss: 1.1926 - accuracy: 1.0000\n",
      "Epoch 487/500: - 0.0497s/step - loss: 1.1797 - accuracy: 1.0000\n",
      "Epoch 488/500: - 0.0497s/step - loss: 1.7760 - accuracy: 1.0000\n",
      "Epoch 489/500: - 0.0497s/step - loss: 1.7658 - accuracy: 1.0000\n",
      "Epoch 490/500: - 0.0497s/step - loss: 1.7557 - accuracy: 1.0000\n",
      "Epoch 491/500: - 0.0497s/step - loss: 1.1560 - accuracy: 1.0000\n",
      "Epoch 492/500: - 0.0497s/step - loss: 1.1443 - accuracy: 1.0000\n",
      "Epoch 493/500: - 0.0496s/step - loss: 1.1355 - accuracy: 1.0000\n",
      "Epoch 494/500: - 0.0496s/step - loss: 1.1251 - accuracy: 1.0000\n",
      "Epoch 495/500: - 0.0496s/step - loss: 1.1386 - accuracy: 1.0000\n",
      "Epoch 496/500: - 0.0496s/step - loss: 1.1321 - accuracy: 1.0000\n",
      "Epoch 497/500: - 0.0496s/step - loss: 1.1215 - accuracy: 1.0000\n",
      "Epoch 498/500: - 0.0496s/step - loss: 0.5428 - accuracy: 1.0000\n",
      "Epoch 499/500: - 0.0496s/step - loss: 0.5393 - accuracy: 1.0000\n",
      "Epoch 500/500: - 0.0496s/step - loss: 0.5385 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(images, network[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #               zip(train_ds, network)]\n",
    "        \n",
    "        network_new = []\n",
    "        kernels_new = []\n",
    "        for (images, labels), net, hmc_kernel in zip(train_ds, network, kernels):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            net_current = net_current[0]\n",
    "        \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.split(new_state, [2], axis = 1)   \n",
    "            network_new.append(net_new)\n",
    "            \n",
    "            ker_new = model.generate_hmc_kernel(images, labels, new_step_size)\n",
    "            kernels_new.append(ker_new)\n",
    "            \n",
    "        network = network_new\n",
    "        kernels = kernels_new\n",
    "        \n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(images, network[bs], labels))\n",
    "    \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    #print(preds)\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[ 3.73042  , -2.611004 ],\n",
      "       [ 3.599267 , -2.4101195]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(2,) dtype=float32, numpy=array([-0.8907312,  3.8896167], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[2.8464344],\n",
      "       [2.8018098]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-4.36044], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = StochasticMLP(hidden_layer_sizes = [30], n_outputs=1)\n",
    "network2 = [model2.call(images) for images, labels in train_ds]\n",
    "kernels2 = [model2.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor: shape=(4, 30), dtype=int32, numpy=\n",
      "array([[1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0],\n",
      "       [1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0],\n",
      "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0]], dtype=int32)>]]\n"
     ]
    }
   ],
   "source": [
    "print(network2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1000\n",
    "step_sizes2 = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    #print(i)\n",
    "    network_new2 = []\n",
    "    kernels_new2 = []\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network2, kernels2):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        net_current = net_current[0]\n",
    "        \n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2][4].numpy()\n",
    "        step_sizes2.append(new_step_size)\n",
    "        \n",
    "        new_state = rerange(samples[0][0])\n",
    "        #print(new_state)\n",
    "        net_new = tf.split(new_state, [30], axis = 1)   \n",
    "        network_new2.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model2.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new2.append(ker_new)\n",
    "            \n",
    "    network2 = network_new2\n",
    "    kernels2 = kernels_new2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXyU5bnw8d+VFUhCWBJCCEtYwi4QCIuCSMEFxWO1asV9e8Vq69L2vD1oF+t2XttarZ6eaqlrW2u1ihtYFRUVEYGAyBZIAgRIgGRC9n2Z+/1jlkz2STJbJtf388mHmWeembmePOSae67nXsQYg1JKqcAV4u8AlFJKdUwTtVJKBThN1EopFeA0USulVIDTRK2UUgEuzBsvGhcXZ5KTk73x0kopFZR27NhRaIyJb+sxryTq5ORk0tPTvfHSSikVlETkaHuPaelDKaUCnCZqpZQKcG4lahH5sYjsE5G9IvKqiPTzdmBKKaVsOk3UIpIE3A2kGWOmA6HASm8HppRSysbd0kcY0F9EwoABwAnvhaSUUspVp4naGJMHPA4cA04CpcaYj1ruJyKrRCRdRNItFovnI1VKqT7KndLHYOC7wFhgBBAlIte13M8Ys8YYk2aMSYuPb7MroFJKqW5wp/RxLnDEGGMxxtQDa4GzvBuWUsHrVGkNH+w96e8wVC/iTqI+BiwQkQEiIsAyIMO7YSkVvK5/fis/+PtOymvq/R2K6iXcqVFvBd4AdgJ77M9Z4+W4lApaWQUVALz8VY5/A1G9hlu9PowxDxhjJhtjphtjrjfG1Ho7MKWC3eMfZWK16gpLqnM6MlEpH9qeU9Ts/smyGj9Fojyhqq6BV7cd4/XtxynzYinLK5MyKaXaduWzW5rdP1RQQdKg/n6KRvXUbz84yEv2EtbmQ4U8tTLVK++jLWqlfMR1Iek1188B4IF396ELTPdeucVVztvv7DrBo+v3e+V8aqJWykcKK+qctxelxAFwpLCSbUeK2nuKCnC1DdZm9/+y6Qi2znGepaUPFbQqaxu47a/p7DtRhgjUN1j5xcVTuXreaJ/FUFhRy2vbj3PHOeM5ZLH39rhlHgMimv70TmmdOiA0Wg3PfJbNynmjiYuO7HDfTzLyCQ8N4VhRVYf7eYq2qFXQ+vPnh/jq0GlKq+spqaqnsq6R+9bu8Wmp4Zdv7+V3Hx5kx7Fisu3d8iYMiwbgZ8snAbY6tfK/fSdKefyjTN7Z1flURre+nM4NL2zjeFEVIQIp9nP6m8vP8Eps2qJWQauoqq7N7flltQyP9c1MvQXltp6srhcREwfa3vvOJRN45etjPP1pNtfMH+OzmHriqY+zWLf7BG//cCFRkb5JH19kWjhaVMX1C8a0emzNF4eYPXowaclDevw+jg/S7E4+OOtcyh1WA09fncolM0f0+P07oi1qFbQKy9tO1FkF5T6LoaHR2mpbSEhTDfNEaTUAD6/f77OYustqNTz5cSZZBRX8K/24z973L5sO89t/H2j1TaimvpHH/n2Av25pdwWrLmlK1B3//zh6urLZ/Qnx0R55/45oolZBK9tSwflTE8h5bAU5j61g28+X2bb7qNSwYX8+3+aWdrjPqMEDACitCtzh5Ot3nyR59Xre/bapJFBW0+Cz9z9UUEF5bYPz24nDYUslVtM00rOn3GlRG2O45rmtzbaNjYvyyPt3RBO1Ckp1DVZyCitJSWhq7cRHRxLbP9xnifq2v9oWeE4eOsC57bVVC5rts+YGWze92oZGn8TUHT/8x04A7n1tl3NbTotWpbdU1DZwotR2sTUrv/l5c3wzOmSpoNEDIzyz7Rd7i6vqOV3R9uDrwoo6LPYPjJGD+3Pt/NH0jwjt8Xt3RhO1CkpHT1fSYDXOC3cAIsKEYdG8svUY1z231WfDt+9cMsHZqp8/bmizxyYPH8jKuaPYnlPMxf+ziaLKtss13vb8l0dIXr2ejQcKOt33jKRY1u7MI3n1ev70WbbXYtp4sIDpD3zovH/d81t5ckOm877jA7euwdqqHNGeJzdk8oTLazg889khDlsqmTkyttlrt+RaNvvyv5by6GXeuXjYkiZqFZQcf2gpw2KabXfUE7/MLiSvpNpr719Tb2shhwhcmprU4b6OD5O9eWV8kpHvtZg68vA6W43c8S3AobS6eUkmPiaSOWMGO+//9oODXovp9r/uaLXtqU+ynLez8isItdf73S1/rP0ml7U7c1tt/80HBwA4f9rwDl/P0ap/+LvT3Ho/T9FErYKS4w9tXHzz+qFrKSQzv/VFo5c221qW963d7db7fP/PW3j280Otth+22Fp4T1+dSkRYx39mrq1+f7WoHRpafMtwXFibP9bWq2JCfHSzeAGSV6/n2GnP9yeua+NCrOP9sgsqyCoo50z7N5SsNs5lS1V1DRwvqia3uJrK2qYau+tFSseH0C/e3ktOYetWelZBOTH9wriujR4o3qSJWgUdYwzpR4sZObh/s4ElAONdkkxbraZfv2drWb667Tj17SQKh/KaerYdKWJTloXqukZnKxqaJl9q2aJvS0pC0z7bc4p8PqT8uMugjdj+4c3e39GCvOiMRMD2QZcyrHUvhzd25jY7/p4yxjB4QDgAz9+Yxr/vOZtIlw+819OPk3O6ilmjBpE0qL9bLepDBU2J17W0cdJeA58/dojzAwlgU5aF4hYfnFn5FaQMi/bK6MOOaKJWQedPnx3ii0wLgwdEtHpsfFzHLWpX//df33b4uOOPPTO/gnmPfszSxz8DIK+kmgfe3QdActyA9p7uNMKl//THGQU8t+lIp8/xpLN/uxGApZOHUVpdj8XlQlpWQQX9wkM4d2oCIjA1caCzRT1qSNNkUk9/kkXqQxs8FlNhRR3FVfX86uKpLJuSwJTEgTx3Y5rz8TVfHKbRfg0iJSGazPzOE7Vrfdn13Dtu/+S8iYgIifbzsTn7NKkPb2hWKskuqGBiQucfvp6miVoFlcz8ctZ8cRgAQ+uW6eihA1h/9yIWjBvSqheB1WroH950Bf/tXSdsrXOXVu6Oo0XUNVjZcbTY+XxLeS3l9t4JlvJa1u5o+sOODOu8R4CI8OlPz2FYjG3Y8qPvZ2C1GvJKqr1aRweormtqBTtazY7j2p1bwp68UiYMiyZpUH/W3bWIy+eMZGh0JB/cezb/XHVm89eqbyQ9p4ia+kb25JaSnlPEruMlbM4u7HJcjlKGa1I8OyWev906r9l+E4bZWvid9fwwxvDqtmOEhwoRYSHNWuCO43W817/vORsR+GDfKQA+3HeKmvpG/rYlh9OVda1KP76gIxNV0LBaDec/+YXz/hWzR7a537QRsUxNjOXVbcewWo1zAEpeSTXV9Y2sOCOR9XtsaxruPFbMFc9u4cWb5jI8th+XP7OFSQkxHMwvJ83loprD3Ec/dt7uyvSl4+KjWbV4HI+st61y98LmI3y0Lx8EXr/9zE6e3X2O+UcAFtsnisrMLydlWDSX/HEzAJfZL4ZOGxHr3Hfy8IFtlmiueHYLURGhVNY1L4Ns/M8lXepv7EikExOaJ8WzU+IZM3QAR+018fHx0aQkxFDXYOVYUVW77/F6+nG25xQDMCUxplWLOi46ksFRtm9ggwZEMGX4QPafLANsc4D88u29/Mv+AZwSiC1qEZkkIrtcfspE5F5fBKdUVxx3mXLy4hmJ3HhWcrv7piREU13fSG5xU4v19x/ZejDcvDCZH5wzHoA/bbRdKNx3opR9J2x/uAftf+TpR4sZEtW6vOKw8T+XdCn+WxeNdd7enVvKvhOl7D9R5tWa9cFTtmN570eLiI+x9TPPzK9oVkpISWi7BSkiZDy0nIyHlvPY95q6qbVM0gDPf3mYA6fK3I4rM7+cgf3CiI9pPTnShh+f47zdPyLU2RJ2JN+Cshpe336cn7+1hw/treKMk02JeWJCdLNvU5kFFa0+EFxL0B9nFDiTNNBmjd7b3Fkz8aAxZpYxZhYwB6gC3vJ6ZEp1kWtySR09uMMLPo4/TEfdsqSqjrftk/GkJMSQOnoQAJ/Y+xUfzK9os6Y9akjbNejZowd12tujJRHh8Stn2uOqoLKusdmAD2/IzC8nIjSEKYkxiAgTE6LJLih3fhhBxxdE+0eE0j8itFX/8Jb+/vUxlv9hk9txZeXbasFtncOIsBAunTWCq9JGAU29ZhzXDO58ZSc/e3M3r2w9xu1/24ExhmL7vC8PXzqdiQkx5JVUU1HbgDGG7PzyVnXn1RdObje2RD/MydLVGvUy4JAxxjOD65XykJr6xmZ9gOOi22/pAkywJ59bX06npr7ROfoObD0fWraaMk+VO1ufrmIiw5yDWe49N8W5/Y0fnNWt47hizkgunD6cjJNNrc/OLnr2RGZ+OePiowgLtaWClIQYtucUs+aLpi6H7tRkR7fzgdUdnx0sYFtOUYclhj+sTOU3V8wAIDoyjKRB/Z2/p5Y9QD7cd4p3dp3gnInxXL9gjPPcLvndRmY9tIHKusZW3xrOTokn57EV3L10gnPbsJhIch5b4fMeH9D1RL0SeLWtB0RklYiki0i6xWLpeWRKdcHnmU3/5y5LTeK8qQkd7h/bP9x5e+OBAjZnnwZg1eJxACQPjWKAy9Dgw4UVzpolNPXUePjS6c5tN56ZTGRYCLedPbbZxEtd1bJ1l9nGB4SnZOZXMGl40/uNs9d488tqiYkM49r5oxnjRhIODRF+et5E5wXRSQkxXDprBD85b6JzUArY+jJ35qYXtzeLxR0ThjWVM1qWin7wd9uHsOOcO36/hRV1zgE97fXkuGnhWMJDbfG79jrxNbcTtYhEAJcA/2rrcWPMGmNMmjEmLT4+3lPxKdWp6rpGbv9b0yi2J6+a1ar/dEdck/x99q+8ISHCy7fYehiECNQ3GizltcwcZSuJPPjd6eQ8tqLZxavBUREcfORCfr5iao+Ox5E0htunQ/1//z5A8ur1vOlSJy2urGPOwxv4+vDpDl/ry6xC0h75mKc+zmLF002lh/0nyphw//vklVQ3S1LDBjZ9rb9sdhKPXnaG2x86dy1L4S57C/R3V87gDytTuXtZSrPXd5QnrFZD8ur1JK9ez4T73+ebY8WUVtWT9kjTxdhhAzuevN/VxIRo9p8sI3n1+nYnjHKULNoqV01sp7wzJCqCrEcvIuexFcwYOcjteDytKy3qC4Gdxhj/jHFVqh2u9dT1dy9y+3nr7rLt+++9tgtO9180udnX2rQxg3nk0un847amiZTuOGc8D14yjSWTvNcYmTTc9jV84vDmyeOnLv26P9p/itOVdW3OW+HqsQ8yKKyo5cmPM9l3oozM/HLyy2r43YcHnKMQXRPpCnsXPYAR3Vh099LUJB65dDpnJDX1EHnhpjTuWWYrC320L5/Sqnq+dOmy12A1PLRuPx9n5FNo78MdHxPJxTPcn+O5ZR39qrRRrLtrkbOFD3DvuRMBmrXwHWIHhLfaFki60j3vatopeyjlT641XNcuZJ2ZnhRL2pjBpB8tJiI0hFsWjm32uIhw3YIx1NQ3EiK2SeKnJw1k+fThHou9LWOGRhEdGcYZSQOxWk2zpAZQVlPPf725x3anix1CXLsvOkxySdShIcLvrpjB/31jN5OHd70bWky/8FbDqxNj+3P3shSe+iSLP27M5o8bW0/k9M2xEr45VuK8/+il09tMqO1pWWO+ZdFYJg2PYdLwGArKa3nou9OazXI3Li6Kw/Yh4l15H39xK1GLSBRwHnC7d8NRquscNdyt9y/r8nMnDo8h/Wgx44dFOy+otdQvPJTkuCjyS2u61De6u8JDQ1h31yJn17QdR4t5YkMm+0+W0dBobVazLihvv0fI8aIq9uZ13iVu5ODmx3TFnJG2vuYjBnbzCFrrajKcPLxr7+36rWDtnWc56+4TE2LYlFXY7MMI4K0fLqS8pp7w0JBekajdKn0YYyqNMUONMR3Pgq6UHxw4Vc4ZSbEkDOx6tylHq7Gz1uPilHgWT4z32RX/5LgooiLDiIoMY/HEeK5bMMY2x/bpKjJcEvXRoqp2L9A5hoZ3ZEriwFY1aBHxaJJ2uHhGYqttbQ0aEmn94dGZqMgwEmP7ccOZY5g9uuk1F02II2FgJFNaHE9s/3BGDh5AwsB+nS5kGwh0ZKLq1YwxZJwsY9mUYd16vqOlNamTRP3rS3w7rWVLjg+SG57fyncmDyOmXxi/vXwGd7yyk6m/+pC37jyLVHuCqmuw8sC7e53PffSy6Vw7fwzJq9c3e82cx1b47gCAP14zm++nWbjhhW2cnRLH326d73zMEdvh/76o2z1mttzX+hvVdyYPY+v953Yv4ACiiVr1apbyWk5X1jElsXstwJmjBvG91CQu9HLduaccfZlPlNbw2UELU4YPZLLLMa9c8zUHH7kQgG1Hinh1W9Oaho6FV/907WyOFFYSERpCXEzH/cy9Zd7YIXwvNYm7lqU02+6IrSfdGoOZJmrlU19kWvivN3fzwb2Lycov545XdvL+3We3OVTYHY6+zd1N1P3CQ3niqlndeq4v9XOZLCqvpJqlk4c1G2RS22Bt1WIG+OM1qcT0s/VouOiM1qUHX2vv9x0IsQUynT1P+dTnmRZOltawN6+ULzItWMpr2XW8pPMntsMxh8OULl586o3+7lIqmJwYQ2iI8NLNczt8TlcvyqnApIla+dR++8RGGSfLnBfFXIdLd1XGyTKSBvUP+H6wnrAoJc55kc2RgJdMGsavLm5/gI3rwrqq99JErXzGGOMsVWScLHcm6Cc2ZHKqmxMPZZwsY0qi76ed9BdHgna9+HnzwuRm+1w7fzT7HryAPb8+v90uh6p30bOofOZkaY1zboVtOaebTTH6q3f2tve0dtXUN3K4sLLb9ene6PLZSVwzfzTRkU2Xl0SEf9w2n7nJg1k4YSj3nJtCVGSYszatej+9mKh8xlH2mJs82DmJu4Pr8k/uysq3rerRlxL1hWckcmEbF97OGh/HWePj/BCR8gVtUSuf2X+yDBG4YFpTV7j37z4bgAr7RDo7jhZ3uEhqSVUde/NK2XakyFk66UuJWvVN2qJWPrP/RBnJQ6NYPDEe7EtOTUmM4e6lE/jjxmyyC8q5/JmvuCptlHOu4ZaufW6rc6WVaSMGMiAi1K1pOJXqzbRFrXwm45Ttwt/EhBj2/Pp8Djy8HBFhSuJArAbOfcI2YdAWl6k7dxwtInn1ej47aFtpxZGkHbcnDY/RQRIq6GmiVj5RXlPP0dNVTLWXKWL6hTsHcbScV6LRamhotFJWU8/lz2wBbJPJF1fWtXpdLXuovkBLH8onDtj7TLc12c+owc1LFydLq7nsT1+xJ6/5HGCpD29o9VxN1Kov0EStvM4Ywy/esnW/ayuxhoQI6+9exFfZp2mwGn7zwYFWSdrV41fOZFOWhXd2nWBqH+pDrfouLX0or9t6pMi5CsvwdqYinTYiltsWj2tzKsxfrJjS7P4lM0dw5ZxRTB4ew9RE9xcKUKq30ha18qr/fj+DNV8cdt7vbD5n13mIH7xkGjeelQzA/zl7XLP9FqXE8cG9iz0XqFIBzK0WtYgMEpE3ROSAiGSIyJneDkz1fpby2mZJ+t0fLez0OSLCs9fN4ftpI7l63mhvhqdUr+Fui/op4ANjzBX21ci146rq1H/8z5fO29GRYW6v4rx8+nCvr0uoVG/SaaIWkVhgMXATgDGmDmjdT0qpFk6V2SZaWn3hZG5rUbpQSrnPndLHWMACvCgi34jIc/bFbpsRkVUiki4i6RaLxeOBqt7lsKXCefuGM8f0igVElQpU7iTqMGA28IwxJhWoBFa33MkYs8YYk2aMSYuPj/dwmKq3Wfr7zwH48/VzGBCh16yV6gl3EnUukGuM2Wq//wa2xK1Um1xXxT4jSbvPKdVTnSZqY8wp4LiITLJvWgbs92pUqldzzMcxJXEgibFt95tWSrnP3e+kdwGv2Ht8HAZu9l5IqrfbnWsbVfjyzXM77TetlOqcW4naGLMLSPNyLCoIHDtdxcPr9jMgIpRh7YxCVEp1jQ4hVx71+w0HAaiqa3/yf6VU12iiVh5VWWu7kLhqsfabVspTNFGrHtt3opQrn/2K8pp6Mk6W851J8axePtnfYSkVNDRRqx578N39bM8p5t5/7iKvpJqFE+J01RWlPEgTteqxukYrAJ8csC2XNXOUe3N6KKXco4la9Uij1ZBdUNFs27Q2VnFRSnWfJmrVI4csFVTUNnDt/KYpSXXIuFKepX9Rqke+OVYMwC2LxhIiQnJcq/m6lFI9pIla9ciu4yXE9g9n7NAoHr50ur/DUSooaelDddtX2YW8uu04M0bGai8PpbxIE7Xqtmues02oOGhAhJ8jUSq4aaJW3dJoNc7bV88b5cdIlAp+mqhVl316IJ/x978PwM8vmsJZ4+P8HJFSwU0TteqSkqo6bnkp3XlfB7co5X2aqFWXvLL1mPN2yrBoUkdrolbK27R7nnJbQ6OVY6erALh+wRjtjqeUj2iiVm4pra5n5oMfAbBkUjwPfXeanyNSqu9wK1GLSA5QDjQCDcYYXe2lj0nPKXLenjd2iC6xpZQPdaVF/R1jTKHXIlEBbZtLok4bM8SPkSjV9+jFROWWbUdsiTq2fzgzRsb6ORql+hZ3E7UBPhKRHSKyqq0dRGSViKSLSLrFYvFchMrvquoa2JNbyp1LxvPtA+fTLzzU3yEp1ae4m6gXGWNmAxcCPxSRxS13MMasMcakGWPS4uPjPRqk8q+dR0tosBrmjdWSh1L+4FaiNsbk2f8tAN4C5nkzKBVYth05TYjAnDGD/R2KUn1Sp4laRKJEJMZxGzgf2OvtwFTg2HqkiGkjYonpF+7vUJTqk9xpUScAX4rIt8A2YL0x5gPvhqUCRW1DI98cL9Gyh1J+1Gn3PGPMYWCmD2JRAWh3bil1DVZN1Er5kXbPUx3aevg0APOSNVEr5S+aqFWHth4pYlJCDIOjdHEApfxFE7VqV21DI+k5xcwfp61ppfxJE7Vq1zfHSqiub2TRBF0YQCl/0kSt2rU5u5AQgQXjh/o7FKX6NE3Uql1fZhcyc9QgBmr/aaX8ShO1alNZTT3fHi/RsodSAUATtWrT14dOYzWwUBO1Un6niVq1aXN2If3DQ3VNRKUCgCZq1aYvswuZN3YIkWE6palS/qaJWrXy8lc5HLJUan1aqQChiVo1U9dg5YF39wFan1YqUGiiVs2kH21aG3Hy8Bg/RqKUctBErZr57KCFiNAQ9j14ASEhutK4UoFAE7Vq5tMDBcwfN4SoyK4sUK+U8iZN1MrpeFEV2QUVLJk0zN+hKKVcaKJWTp8dLABg6WRN1EoFErcTtYiEisg3IrLOmwEp//n0QAHJQwcwNi7K36EopVx0pUV9D5DhrUCUf9XUN/LVodNa9lAqALmVqEVkJLACeM674Sh/2XL4NLUNVi17KBWA3G1R/wH4GWBtbwcRWSUi6SKSbrFYPBKc8p2NBwroHx6qi9gqFYA6TdQicjFQYIzZ0dF+xpg1xpg0Y0xafHy8xwJU3meM4dMDBSycMJR+4Tq3h1KBxp0W9ULgEhHJAf4JLBWRv3s1KuVT+0+WkVtczblTEvwdilKqDZ0mamPMfcaYkcaYZGAl8Kkx5jqvR6Z85sN9+YQInDtVE7VSgUj7USs+2neKtOQhxEVH+jsUpVQbupSojTGfGWMu9lYwyvdyCis5cKqcC6YN93coSql2aIu6j/tw3ykAzteyh1IBSxN1H/fhvlNMGzGQUUMG+DsUpVQ7NFH3YVsOnWbnsRIteygV4DRR91EVtQ1c/ZevAVg+XRO1UoFME3Uf9UlGvvN2yrBoP0ailOqMJuo+6r1vTzIsJpKDjyxHRFdyUSqQaaLug0qr6/ki08J/zBxBZJgOGVcq0Gmi7oM27M+nrtHKihmJ/g5FKeUGTdR90LrdJ0ga1J/UUYP8HYpSyg2aqPsYS3ktm7IK+Y+ZI7Q2rVQvoYm6j3n32xM0Wg3fm53k71CUUm7SRN3HvPVNLtOTBjIxIcbfoSil3KSJug/JzC9nb14Z30sd6e9QlFJdoIm6D1m7M4/QEOGSWSP8HYpSqgs0UfcRjVbDO7vyOGdivM47rVQvo4m6j/j68GlOltZwWapeRFSqt3Fncdt+IrJNRL4VkX0i8qAvAlOe9a/048REhnGezjutVK/jTou6FlhqjJkJzAKWi8gC74alPKm4so73957istlJusq4Ur1QWGc7GGMMUGG/G27/Md4MSnnW2m/yqGuwsnLuaH+HopTqBrdq1CISKiK7gAJggzFmq3fDUp5ijOHVbceYNWoQU0cM9Hc4SqlucCtRG2MajTGzgJHAPBGZ3nIfEVklIukikm6xWDwdp+qm9KPFZBdUcM18bU0r1Vt1dRXyEmAjsLyNx9YYY9KMMWnx8fGeik/10KtbjxETGcbFOlOeUr2WO70+4kVkkP12f+A84IC3A1M9V1pVz/o9J7k0NYkBEZ1ejlBKBSh3/noTgZdFJBRbYn/dGLPOu2EpT3gt/Ri1DVaunqdlD6V6M3d6fewGUn0Qi/KghkYrL391lAXjhuhFRKV6OR2ZGKQ+2p9PXkk1tywc6+9QlFI9pIk6SL24+Qijhwxg2RQdiahUb6eJOgjtzi1he04xN52VTGiIruKiVG+niToIvbg5h+jIMK5M03mnlQoGmqiDzImSatbtPsGVaSOJ6Rfu73CUUh6giTrIrPniMMbArYv0IqJSwUITdRAprKjln9uPcWlqEiMHD/B3OEopD9FEHURe3HyE2gYrdywZ7+9QlFIepIk6SJTV1PPXr45y0fRExsdH+zscpZQHaaIOEn/bcpTy2gZtTSsVhDRRB4Gymnr+sukwSybFMz0p1t/hKKU8TBN1EHh+0xFKqur56XmT/B2KUsoLNFH3ckWVdTy36TAXnTGcM0Zqa1qpYKSJupd75rNsqusb+cl5E/0dilLKSzRR92KnSmt4ectRLksdyYRhMf4ORynlJZqoe7EnN2RijOHec1P8HYpSyos0UfdSe/NKeX3HcW46K5lRQ3QUolLBzJ01E0eJyEYR2S8i+0TkHl8EptpnjOHhdfsZPCCCHy3V1rRSwc6dFnUD8FNjzFRgAfBDEZnq3bBURz7Ye4qtR4r4yWr9ZVwAAAtqSURBVHkTie2vM+QpFew6TdTGmJPGmJ322+VABpDk7cBU22rqG/nvf2cwKSGGlXNH+TscpZQPdKlGLSLJ2Ba63drGY6tEJF1E0i0Wi2eiU608+/khjhdV88uLpxIWqpcYlOoL3P5LF5Fo4E3gXmNMWcvHjTFrjDFpxpi0+Ph4T8ao7A5ZKvjTxkNcMnMEi1Li/B2OUspH3ErUIhKOLUm/YoxZ692QVFuMMfz8rT30Cw/hFxdP8Xc4SikfcqfXhwDPAxnGmCe8H5Jqy5s78/j6cBGrL5zCsJh+/g5HKeVD7rSoFwLXA0tFZJf95yIvx6VcFJTX8Oj6/cwZM1gvICrVB4V1toMx5ktAfBCLaoMxhvvX7qGyrpHfXH4GISF6KpTqa7TbQIB7Y0cuH2cU8LMLJul8Hkr1UZqoA1heSTUPvbef+WOHcMtCXVVcqb5KE3WAarQafvLaLqzG8PiVM7XkoVQf1mmNWvnHU59ksfVIEb+/cqZOuqRUH6ct6gC0ObuQ//k0iyvmjOTyOSP9HY5Sys80UQeYgvIa7vnnLibER/PQd6f5OxylVADQ0kcAqW1o5I6/76Sitp5/3DafARF6epRSmqgDhjGGX769lx1Hi/nfa2YzMUG74imlbLT0ESBe3JzD6+m53L0shRUzEv0djlIqgGiiDgCfZ1p4ZP1+LpiWwL3LdMUWpVRzmqj97NvjJdzx9x1MGj6QJ74/S/tLK6Va0UTtR4ctFdz80naGRkfw8s1ziYrUSwZKqdY0UftJflkN1z+/DQH+est8hg3UqUuVUm3TRO0HBWU1XPOXrympquPFm+cyNi7K3yEppQKYftf2sYKyGq7+y9ecLK3hpZvnMWPkIH+HpJQKcNqi9qGWSXre2CH+Dkkp1Qtoi9pHcgorueGFbRRW1PLyLfOYm6xJWinlHnfWTHxBRApEZK8vAgpGe3JLueLZr6iobeAfty3QJK2U6hJ3Sh8vAcu9HEfQ+iLTwso1W4gMC+WNH5zJrFFak1ZKdU2nidoY8wVQ5INYgooxhuc2HeamF7cxasgA1t55FuPio/0dllKqF/JYjVpEVgGrAEaPHu2pl+2Vauobuf+tPazdmccF0xJ44vuzdDCLUqrbPJY9jDFrgDUAaWlpxlOv29scL6riR//Yybe5pdx7bgp3L03RYeFKqR7RZp4HvfftCe5fuweAZ6+bw/Lpw/0ckVIqGGii9oDK2gYeem8/r6UfJ3X0IJ5emarrHCqlPKbTRC0irwJLgDgRyQUeMMY87+3AeotNWRbuW7uHvJJq7lwynh+fN5HwUB1HpJTynE4TtTHmal8E0tuUVtfz6Pr9vJ6ey7i4KF6//UztH62U8gotfXSR1Wp4c2cuv/3wIEWVddyxZDz3LEuhX3iov0NTSgUpTdRdsONoEQ++t5/duaWkjh7EizfNZXpSrL/DUkoFOU3UbsguKOfJj7NYv/skCQMj+cNVs7hk5gjtdqeU8glN1B04UljJ059k8c6uPPqFh3L30gncfs54HbyilPIpzTht2JtXyvNfHuHdb08QERrCbYvHcfvi8QyJivB3aEqpPkgTtZ3Vath4sIDnNh1hy+HTREWEcvNZydx+znjiYyL9HZ5Sqg/r84k6v6yGN3bk8tr24xwrqiIxth/3XzSZq+aOJrZ/uL/DU0qpvpmoaxsa+fyghdfTc9l4sIBGq+HMcUP56fkTueiMRB2wopQKKH0mUdc1WNmcXch7u0+wYV8+5bUNxEVHsmrxOL6fNkoXmFVKBaygTtTFlXV8kWVh44ECNh60UFpdz8B+YSyfPpyLZ47grPFDtfWslAp4QZWo6xqs7MkrYcuh02w8aOGbY8VYDQyNiuDcKQmsmDGcRRPiiQjT5KyU6j16daKuqW/k2+MlbD1SxNYjp9l5tITq+kYAZoyM5a6lKXxn8jBmJMXq4BSlVK/VaxJ1faOVg6fK2ZNXyu7cEnbnlnLwVDkNVoMITEqI4aq5o5g/dgjzxg5haLR2qVNKBYeAS9RWqyG3uJrM/HIyC8rJyq8gM7+crIIK6hqsAMT2D2fGyFhWLR7HrFGDmDd2CIMG6GAUpVRwCphEXd9o5Xt/+oqsgnJq6q3O7Ymx/UhJiOGs8UOZMXIQM0bGMnrIAES0lKGU6hsCJlGHh4YwPj6KuclDmJgQTUpCDCkJ0Qzsp4NOlFJ9m1uJWkSWA08BocBzxpjHvBHMH1ameuNllVKqV+u0n5qIhAL/C1wITAWuFpGp3g5MKaWUjTsdiucB2caYw8aYOuCfwHe9G5ZSSikHdxJ1EnDc5X6ufZtSSikf8NgQPRFZJSLpIpJusVg89bJKKdXnuZOo84BRLvdH2rc1Y4xZY4xJM8akxcfHeyo+pZTq89xJ1NuBFBEZKyIRwErgXe+GpZRSyqHT7nnGmAYR+RHwIbbueS8YY/Z5PTKllFKAm/2ojTHvA+97ORallFJtEGOM519UxAIc7ebT44BCD4bTG+gx9w16zMGvJ8c7xhjT5gU+ryTqnhCRdGNMmr/j8CU95r5Bjzn4eet4dQZ9pZQKcJqolVIqwAViol7j7wD8QI+5b9BjDn5eOd6Aq1ErpZRqLhBb1EoppVxoolZKqQAXMIlaRJaLyEERyRaR1f6Ox1NEZJSIbBSR/SKyT0TusW8fIiIbRCTL/u9g+3YRkaftv4fdIjLbv0fQfSISKiLfiMg6+/2xIrLVfmyv2ackQEQi7fez7Y8n+zPu7hKRQSLyhogcEJEMETkz2M+ziPzY/v96r4i8KiL9gu08i8gLIlIgIntdtnX5vIrIjfb9s0Tkxq7EEBCJOsgXJ2gAfmqMmQosAH5oP7bVwCfGmBTgE/t9sP0OUuw/q4BnfB+yx9wDZLjc/w3wpDFmAlAM3GrffitQbN/+pH2/3ugp4ANjzGRgJrZjD9rzLCJJwN1AmjFmOrYpJlYSfOf5JWB5i21dOq8iMgR4AJiPbY7/BxzJ3S3GGL//AGcCH7rcvw+4z99xeelY3wHOAw4CifZticBB++0/A1e77O/crzf9YJtl8RNgKbAOEGwjtsJannNs88icab8dZt9P/H0MXTzeWOBIy7iD+TzTNFf9EPt5WwdcEIznGUgG9nb3vAJXA3922d5sv85+AqJFTR9ZnMD+VS8V2AokGGNO2h86BSTYbwfL7+IPwM8Ax5LyQ4ESY0yD/b7rcTmP2f54qX3/3mQsYAFetJd7nhORKIL4PBtj8oDHgWPASWznbQfBfZ4dunpee3S+AyVRBz0RiQbeBO41xpS5PmZsH7FB009SRC4GCowxO/wdiw+FAbOBZ4wxqUAlTV+HgaA8z4OxLcs3FhgBRNG6RBD0fHFeAyVRu7U4QW8lIuHYkvQrxpi19s35IpJofzwRKLBvD4bfxULgEhHJwbbG5lJs9dtBIuKYsdH1uJzHbH88Fjjty4A9IBfINcZstd9/A1viDubzfC5wxBhjMcbUA2uxnftgPs8OXT2vPTrfgZKog3ZxAhER4HkgwxjzhMtD7wKOK783YqtdO7bfYL96vAAodfmK1SsYY+4zxow0xiRjO5efGmOuBTYCV9h3a3nMjt/FFfb9e1XL0xhzCjguIpPsm5YB+wni84yt5LFARAbY/587jjloz7OLrp7XD4HzRWSw/ZvI+fZt7vF3kd6luH4RkAkcAn7u73g8eFyLsH0t2g3ssv9chK029wmQBXwMDLHvL9h6wBwC9mC7ou734+jB8S8B1tlvjwO2AdnAv4BI+/Z+9vvZ9sfH+Tvubh7rLCDdfq7fBgYH+3kGHgQOAHuBvwGRwXaegVex1eDrsX1zurU75xW4xX7s2cDNXYlBh5ArpVSAC5TSh1JKqXZoolZKqQCniVoppQKcJmqllApwmqiVUirAaaJWSqkAp4laKaUC3P8HODOGgH+QAwoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes2)), step_sizes2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 30), dtype=float32, numpy=\n",
      "array([[ 3.4881892 ,  6.        ,  6.        ,  3.175901  , -6.        ,\n",
      "        -1.0908687 , -4.4888287 ,  6.        , -0.8854478 , -6.        ,\n",
      "        -0.5336058 , -6.        , -6.        ,  6.        , -6.        ,\n",
      "         1.3827236 ,  6.        , -6.        , -6.        , -6.        ,\n",
      "         6.        ,  6.        ,  6.        ,  6.        ,  2.5057468 ,\n",
      "         3.3253698 ,  6.        ,  6.        , -6.        , -1.6928031 ],\n",
      "       [ 6.        ,  6.        , -6.        ,  6.        ,  5.064812  ,\n",
      "         6.        ,  6.        , -3.4218678 ,  6.        ,  3.1529286 ,\n",
      "        -2.042216  , -2.2894542 ,  6.        , -6.        , -0.762521  ,\n",
      "        -4.4954195 ,  6.        , -4.8549685 ,  6.        ,  0.15746999,\n",
      "         6.        ,  6.        , -6.        , -6.        ,  2.6048872 ,\n",
      "        -6.        , -6.        ,  6.        ,  6.        , -6.        ],\n",
      "       [-6.        ,  6.        , -3.948919  ,  6.        , -6.        ,\n",
      "         6.        ,  6.        ,  5.6228147 ,  6.        , -6.        ,\n",
      "        -1.4238648 ,  6.        ,  6.        , -6.        ,  6.        ,\n",
      "        -5.346667  , -6.        ,  6.        ,  1.901058  ,  1.2226938 ,\n",
      "         1.1350775 ,  6.        ,  6.        ,  6.        ,  6.        ,\n",
      "        -6.        ,  1.9198973 , -4.18175   ,  6.        ,  6.        ],\n",
      "       [-1.9920133 , -6.        ,  6.        ,  6.        ,  6.        ,\n",
      "         2.0466423 , -6.        , -6.        ,  2.185824  ,  3.634882  ,\n",
      "         3.4512782 , -6.        ,  6.        , -6.        ,  0.05312943,\n",
      "         4.181388  ,  6.        , -6.        , -6.        ,  6.        ,\n",
      "        -6.        ,  6.        , -6.        ,  6.        ,  6.        ,\n",
      "        -6.        , -4.712142  ,  6.        ,  6.        ,  0.34873986]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(network2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: - 0.0586s/step - loss: 20.7884 - accuracy: 0.5000\n",
      "Epoch 2/500: - 0.0646s/step - loss: 20.8240 - accuracy: 0.5000\n",
      "Epoch 3/500: - 0.0594s/step - loss: 20.4959 - accuracy: 0.5000\n",
      "Epoch 4/500: - 0.0552s/step - loss: 20.3755 - accuracy: 0.5000\n",
      "Epoch 5/500: - 0.0538s/step - loss: 20.0259 - accuracy: 0.5000\n",
      "Epoch 6/500: - 0.0547s/step - loss: 19.7696 - accuracy: 0.5000\n",
      "Epoch 7/500: - 0.0532s/step - loss: 19.5285 - accuracy: 0.5000\n",
      "Epoch 8/500: - 0.0515s/step - loss: 19.7812 - accuracy: 0.5000\n",
      "Epoch 9/500: - 0.0500s/step - loss: 19.0776 - accuracy: 0.5000\n",
      "Epoch 10/500: - 0.0497s/step - loss: 18.7998 - accuracy: 0.5000\n",
      "Epoch 11/500: - 0.0498s/step - loss: 18.5214 - accuracy: 0.5000\n",
      "Epoch 12/500: - 0.0489s/step - loss: 18.3802 - accuracy: 0.5000\n",
      "Epoch 13/500: - 0.0482s/step - loss: 18.1696 - accuracy: 0.5000\n",
      "Epoch 14/500: - 0.0479s/step - loss: 18.6160 - accuracy: 0.5000\n",
      "Epoch 15/500: - 0.0483s/step - loss: 18.0278 - accuracy: 0.5000\n",
      "Epoch 16/500: - 0.0490s/step - loss: 18.7728 - accuracy: 0.5000\n",
      "Epoch 17/500: - 0.0489s/step - loss: 18.9659 - accuracy: 0.5000\n",
      "Epoch 18/500: - 0.0487s/step - loss: 18.5557 - accuracy: 0.5000\n",
      "Epoch 19/500: - 0.0514s/step - loss: 19.4963 - accuracy: 0.5000\n",
      "Epoch 20/500: - 0.0523s/step - loss: 19.1375 - accuracy: 0.5000\n",
      "Epoch 21/500: - 0.0525s/step - loss: 18.9093 - accuracy: 0.5000\n",
      "Epoch 22/500: - 0.0519s/step - loss: 18.6487 - accuracy: 0.5000\n",
      "Epoch 23/500: - 0.0513s/step - loss: 18.8736 - accuracy: 0.5000\n",
      "Epoch 24/500: - 0.0514s/step - loss: 18.7628 - accuracy: 0.5000\n",
      "Epoch 25/500: - 0.0511s/step - loss: 18.4100 - accuracy: 0.5000\n",
      "Epoch 26/500: - 0.0507s/step - loss: 17.4815 - accuracy: 0.7500\n",
      "Epoch 27/500: - 0.0502s/step - loss: 17.7434 - accuracy: 0.7500\n",
      "Epoch 28/500: - 0.0500s/step - loss: 17.8863 - accuracy: 0.5000\n",
      "Epoch 29/500: - 0.0501s/step - loss: 17.6105 - accuracy: 0.7500\n",
      "Epoch 30/500: - 0.0498s/step - loss: 18.0894 - accuracy: 0.5000\n",
      "Epoch 31/500: - 0.0500s/step - loss: 17.2848 - accuracy: 0.5000\n",
      "Epoch 32/500: - 0.0504s/step - loss: 17.5581 - accuracy: 0.5000\n",
      "Epoch 33/500: - 0.0509s/step - loss: 16.1430 - accuracy: 0.5000\n",
      "Epoch 34/500: - 0.0509s/step - loss: 15.6928 - accuracy: 0.5000\n",
      "Epoch 35/500: - 0.0507s/step - loss: 15.6499 - accuracy: 0.2500\n",
      "Epoch 36/500: - 0.0505s/step - loss: 15.3666 - accuracy: 0.2500\n",
      "Epoch 37/500: - 0.0506s/step - loss: 15.0997 - accuracy: 0.2500\n",
      "Epoch 38/500: - 0.0507s/step - loss: 14.9598 - accuracy: 0.2500\n",
      "Epoch 39/500: - 0.0507s/step - loss: 14.9458 - accuracy: 0.5000\n",
      "Epoch 40/500: - 0.0506s/step - loss: 14.6763 - accuracy: 0.5000\n",
      "Epoch 41/500: - 0.0509s/step - loss: 14.3561 - accuracy: 0.5000\n",
      "Epoch 42/500: - 0.0515s/step - loss: 14.1149 - accuracy: 0.5000\n",
      "Epoch 43/500: - 0.0514s/step - loss: 14.3237 - accuracy: 0.5000\n",
      "Epoch 44/500: - 0.0517s/step - loss: 14.0968 - accuracy: 0.5000\n",
      "Epoch 45/500: - 0.0518s/step - loss: 14.1758 - accuracy: 0.5000\n",
      "Epoch 46/500: - 0.0515s/step - loss: 13.9830 - accuracy: 0.7500\n",
      "Epoch 47/500: - 0.0513s/step - loss: 13.6291 - accuracy: 0.7500\n",
      "Epoch 48/500: - 0.0510s/step - loss: 13.4516 - accuracy: 0.5000\n",
      "Epoch 49/500: - 0.0508s/step - loss: 13.2837 - accuracy: 0.5000\n",
      "Epoch 50/500: - 0.0509s/step - loss: 13.1245 - accuracy: 0.5000\n",
      "Epoch 51/500: - 0.0510s/step - loss: 12.9731 - accuracy: 0.5000\n",
      "Epoch 52/500: - 0.0512s/step - loss: 13.0843 - accuracy: 0.5000\n",
      "Epoch 53/500: - 0.0522s/step - loss: 12.9494 - accuracy: 0.5000\n",
      "Epoch 54/500: - 0.0528s/step - loss: 12.8216 - accuracy: 0.7500\n",
      "Epoch 55/500: - 0.0534s/step - loss: 12.6997 - accuracy: 0.7500\n",
      "Epoch 56/500: - 0.0534s/step - loss: 12.5832 - accuracy: 0.7500\n",
      "Epoch 57/500: - 0.0534s/step - loss: 12.4716 - accuracy: 0.5000\n",
      "Epoch 58/500: - 0.0531s/step - loss: 12.3643 - accuracy: 0.5000\n",
      "Epoch 59/500: - 0.0529s/step - loss: 12.2610 - accuracy: 0.5000\n",
      "Epoch 60/500: - 0.0531s/step - loss: 12.1616 - accuracy: 0.5000\n",
      "Epoch 61/500: - 0.0531s/step - loss: 12.0656 - accuracy: 0.5000\n",
      "Epoch 62/500: - 0.0534s/step - loss: 11.9730 - accuracy: 0.5000\n",
      "Epoch 63/500: - 0.0533s/step - loss: 11.8835 - accuracy: 0.5000\n",
      "Epoch 64/500: - 0.0531s/step - loss: 11.7969 - accuracy: 0.5000\n",
      "Epoch 65/500: - 0.0529s/step - loss: 11.2611 - accuracy: 0.5000\n",
      "Epoch 66/500: - 0.0529s/step - loss: 11.5878 - accuracy: 0.5000\n",
      "Epoch 67/500: - 0.0527s/step - loss: 11.4773 - accuracy: 0.7500\n",
      "Epoch 68/500: - 0.0525s/step - loss: 11.3736 - accuracy: 0.7500\n",
      "Epoch 69/500: - 0.0523s/step - loss: 11.2751 - accuracy: 0.5000\n",
      "Epoch 70/500: - 0.0522s/step - loss: 11.1809 - accuracy: 0.5000\n",
      "Epoch 71/500: - 0.0522s/step - loss: 11.0902 - accuracy: 0.5000\n",
      "Epoch 72/500: - 0.0525s/step - loss: 11.0027 - accuracy: 0.5000\n",
      "Epoch 73/500: - 0.0526s/step - loss: 10.9182 - accuracy: 0.5000\n",
      "Epoch 74/500: - 0.0525s/step - loss: 10.8443 - accuracy: 0.5000\n",
      "Epoch 75/500: - 0.0526s/step - loss: 10.7467 - accuracy: 0.5000\n",
      "Epoch 76/500: - 0.0527s/step - loss: 11.3701 - accuracy: 0.5000\n",
      "Epoch 77/500: - 0.0526s/step - loss: 11.2794 - accuracy: 0.5000\n",
      "Epoch 78/500: - 0.0524s/step - loss: 11.1918 - accuracy: 0.5000\n",
      "Epoch 79/500: - 0.0522s/step - loss: 11.1070 - accuracy: 0.5000\n",
      "Epoch 80/500: - 0.0521s/step - loss: 11.0249 - accuracy: 0.5000\n",
      "Epoch 81/500: - 0.0521s/step - loss: 10.4214 - accuracy: 0.5000\n",
      "Epoch 82/500: - 0.0519s/step - loss: 10.3436 - accuracy: 0.5000\n",
      "Epoch 83/500: - 0.0518s/step - loss: 10.2683 - accuracy: 0.5000\n",
      "Epoch 84/500: - 0.0518s/step - loss: 10.1955 - accuracy: 0.5000\n",
      "Epoch 85/500: - 0.0520s/step - loss: 10.1248 - accuracy: 0.5000\n",
      "Epoch 86/500: - 0.0521s/step - loss: 10.0563 - accuracy: 0.5000\n",
      "Epoch 87/500: - 0.0521s/step - loss: 9.9737 - accuracy: 0.5000\n",
      "Epoch 88/500: - 0.0520s/step - loss: 9.8992 - accuracy: 0.5000\n",
      "Epoch 89/500: - 0.0519s/step - loss: 9.8270 - accuracy: 0.5000\n",
      "Epoch 90/500: - 0.0518s/step - loss: 9.7569 - accuracy: 0.5000\n",
      "Epoch 91/500: - 0.0517s/step - loss: 9.6887 - accuracy: 0.5000\n",
      "Epoch 92/500: - 0.0516s/step - loss: 9.6522 - accuracy: 0.5000\n",
      "Epoch 93/500: - 0.0516s/step - loss: 9.5747 - accuracy: 0.5000\n",
      "Epoch 94/500: - 0.0515s/step - loss: 9.6528 - accuracy: 0.5000\n",
      "Epoch 95/500: - 0.0514s/step - loss: 9.5893 - accuracy: 0.5000\n",
      "Epoch 96/500: - 0.0520s/step - loss: 9.5318 - accuracy: 0.5000\n",
      "Epoch 97/500: - 0.0521s/step - loss: 9.4635 - accuracy: 0.5000\n",
      "Epoch 98/500: - 0.0520s/step - loss: 9.3981 - accuracy: 0.5000\n",
      "Epoch 99/500: - 0.0520s/step - loss: 9.3350 - accuracy: 0.7500\n",
      "Epoch 100/500: - 0.0519s/step - loss: 9.2741 - accuracy: 0.7500\n",
      "Epoch 101/500: - 0.0518s/step - loss: 9.2150 - accuracy: 0.7500\n",
      "Epoch 102/500: - 0.0517s/step - loss: 9.1577 - accuracy: 0.7500\n",
      "Epoch 103/500: - 0.0516s/step - loss: 9.1019 - accuracy: 0.7500\n",
      "Epoch 104/500: - 0.0517s/step - loss: 9.0476 - accuracy: 0.7500\n",
      "Epoch 105/500: - 0.0516s/step - loss: 8.9947 - accuracy: 0.5000\n",
      "Epoch 106/500: - 0.0515s/step - loss: 8.9431 - accuracy: 0.5000\n",
      "Epoch 107/500: - 0.0515s/step - loss: 8.8927 - accuracy: 0.5000\n",
      "Epoch 108/500: - 0.0519s/step - loss: 8.8435 - accuracy: 0.5000\n",
      "Epoch 109/500: - 0.0520s/step - loss: 8.7954 - accuracy: 0.7500\n",
      "Epoch 110/500: - 0.0520s/step - loss: 8.7483 - accuracy: 0.7500\n",
      "Epoch 111/500: - 0.0519s/step - loss: 8.7023 - accuracy: 0.7500\n",
      "Epoch 112/500: - 0.0518s/step - loss: 8.6573 - accuracy: 0.7500\n",
      "Epoch 113/500: - 0.0518s/step - loss: 8.6132 - accuracy: 0.7500\n",
      "Epoch 114/500: - 0.0519s/step - loss: 8.5700 - accuracy: 0.7500\n",
      "Epoch 115/500: - 0.0519s/step - loss: 8.5277 - accuracy: 0.7500\n",
      "Epoch 116/500: - 0.0519s/step - loss: 8.4862 - accuracy: 0.7500\n",
      "Epoch 117/500: - 0.0521s/step - loss: 8.4456 - accuracy: 0.7500\n",
      "Epoch 118/500: - 0.0520s/step - loss: 8.4057 - accuracy: 0.7500\n",
      "Epoch 119/500: - 0.0519s/step - loss: 8.3666 - accuracy: 0.7500\n",
      "Epoch 120/500: - 0.0519s/step - loss: 8.3282 - accuracy: 0.7500\n",
      "Epoch 121/500: - 0.0519s/step - loss: 8.2905 - accuracy: 0.7500\n",
      "Epoch 122/500: - 0.0518s/step - loss: 8.2535 - accuracy: 0.7500\n",
      "Epoch 123/500: - 0.0518s/step - loss: 8.2171 - accuracy: 0.7500\n",
      "Epoch 124/500: - 0.0517s/step - loss: 8.1814 - accuracy: 0.7500\n",
      "Epoch 125/500: - 0.0516s/step - loss: 8.1463 - accuracy: 0.7500\n",
      "Epoch 126/500: - 0.0515s/step - loss: 8.1118 - accuracy: 0.7500\n",
      "Epoch 127/500: - 0.0515s/step - loss: 8.0779 - accuracy: 0.7500\n",
      "Epoch 128/500: - 0.0514s/step - loss: 8.0445 - accuracy: 0.7500\n",
      "Epoch 129/500: - 0.0514s/step - loss: 8.0117 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500: - 0.0515s/step - loss: 7.9794 - accuracy: 0.7500\n",
      "Epoch 131/500: - 0.0515s/step - loss: 7.9476 - accuracy: 0.7500\n",
      "Epoch 132/500: - 0.0517s/step - loss: 7.9163 - accuracy: 0.7500\n",
      "Epoch 133/500: - 0.0519s/step - loss: 7.8855 - accuracy: 0.7500\n",
      "Epoch 134/500: - 0.0522s/step - loss: 7.8552 - accuracy: 0.7500\n",
      "Epoch 135/500: - 0.0523s/step - loss: 7.8253 - accuracy: 0.7500\n",
      "Epoch 136/500: - 0.0523s/step - loss: 7.7959 - accuracy: 0.7500\n",
      "Epoch 137/500: - 0.0522s/step - loss: 7.7669 - accuracy: 0.7500\n",
      "Epoch 138/500: - 0.0522s/step - loss: 7.7383 - accuracy: 0.7500\n",
      "Epoch 139/500: - 0.0522s/step - loss: 7.7101 - accuracy: 0.7500\n",
      "Epoch 140/500: - 0.0521s/step - loss: 7.6823 - accuracy: 0.7500\n",
      "Epoch 141/500: - 0.0521s/step - loss: 7.6550 - accuracy: 0.7500\n",
      "Epoch 142/500: - 0.0520s/step - loss: 7.6280 - accuracy: 0.7500\n",
      "Epoch 143/500: - 0.0519s/step - loss: 7.6013 - accuracy: 0.7500\n",
      "Epoch 144/500: - 0.0519s/step - loss: 7.5751 - accuracy: 0.7500\n",
      "Epoch 145/500: - 0.0519s/step - loss: 7.5491 - accuracy: 0.7500\n",
      "Epoch 146/500: - 0.0518s/step - loss: 7.5236 - accuracy: 0.7500\n",
      "Epoch 147/500: - 0.0518s/step - loss: 7.4983 - accuracy: 0.7500\n",
      "Epoch 148/500: - 0.0517s/step - loss: 7.4734 - accuracy: 0.7500\n",
      "Epoch 149/500: - 0.0517s/step - loss: 7.4488 - accuracy: 1.0000\n",
      "Epoch 150/500: - 0.0517s/step - loss: 7.4245 - accuracy: 1.0000\n",
      "Epoch 151/500: - 0.0517s/step - loss: 7.4006 - accuracy: 1.0000\n",
      "Epoch 152/500: - 0.0517s/step - loss: 7.3769 - accuracy: 1.0000\n",
      "Epoch 153/500: - 0.0516s/step - loss: 7.3535 - accuracy: 1.0000\n",
      "Epoch 154/500: - 0.0517s/step - loss: 7.3304 - accuracy: 1.0000\n",
      "Epoch 155/500: - 0.0518s/step - loss: 7.3076 - accuracy: 1.0000\n",
      "Epoch 156/500: - 0.0517s/step - loss: 7.2850 - accuracy: 1.0000\n",
      "Epoch 157/500: - 0.0516s/step - loss: 7.2628 - accuracy: 1.0000\n",
      "Epoch 158/500: - 0.0515s/step - loss: 7.2408 - accuracy: 1.0000\n",
      "Epoch 159/500: - 0.0515s/step - loss: 7.2190 - accuracy: 1.0000\n",
      "Epoch 160/500: - 0.0515s/step - loss: 7.1975 - accuracy: 1.0000\n",
      "Epoch 161/500: - 0.0514s/step - loss: 7.1762 - accuracy: 1.0000\n",
      "Epoch 162/500: - 0.0515s/step - loss: 7.1552 - accuracy: 1.0000\n",
      "Epoch 163/500: - 0.0516s/step - loss: 7.1344 - accuracy: 1.0000\n",
      "Epoch 164/500: - 0.0516s/step - loss: 7.1139 - accuracy: 1.0000\n",
      "Epoch 165/500: - 0.0516s/step - loss: 7.0935 - accuracy: 1.0000\n",
      "Epoch 166/500: - 0.0516s/step - loss: 7.0734 - accuracy: 1.0000\n",
      "Epoch 167/500: - 0.0516s/step - loss: 7.0535 - accuracy: 1.0000\n",
      "Epoch 168/500: - 0.0516s/step - loss: 7.0339 - accuracy: 1.0000\n",
      "Epoch 169/500: - 0.0516s/step - loss: 7.0144 - accuracy: 1.0000\n",
      "Epoch 170/500: - 0.0517s/step - loss: 6.9951 - accuracy: 1.0000\n",
      "Epoch 171/500: - 0.0518s/step - loss: 6.9761 - accuracy: 1.0000\n",
      "Epoch 172/500: - 0.0519s/step - loss: 6.9572 - accuracy: 1.0000\n",
      "Epoch 173/500: - 0.0522s/step - loss: 6.9386 - accuracy: 1.0000\n",
      "Epoch 174/500: - 0.0523s/step - loss: 6.9201 - accuracy: 1.0000\n",
      "Epoch 175/500: - 0.0524s/step - loss: 6.9018 - accuracy: 1.0000\n",
      "Epoch 176/500: - 0.0524s/step - loss: 6.8837 - accuracy: 1.0000\n",
      "Epoch 177/500: - 0.0523s/step - loss: 6.8658 - accuracy: 1.0000\n",
      "Epoch 178/500: - 0.0522s/step - loss: 6.8481 - accuracy: 1.0000\n",
      "Epoch 179/500: - 0.0522s/step - loss: 6.8305 - accuracy: 1.0000\n",
      "Epoch 180/500: - 0.0523s/step - loss: 6.8132 - accuracy: 1.0000\n",
      "Epoch 181/500: - 0.0523s/step - loss: 6.7959 - accuracy: 1.0000\n",
      "Epoch 182/500: - 0.0523s/step - loss: 6.7789 - accuracy: 1.0000\n",
      "Epoch 183/500: - 0.0522s/step - loss: 6.7620 - accuracy: 1.0000\n",
      "Epoch 184/500: - 0.0522s/step - loss: 6.7453 - accuracy: 1.0000\n",
      "Epoch 185/500: - 0.0522s/step - loss: 6.7287 - accuracy: 1.0000\n",
      "Epoch 186/500: - 0.0524s/step - loss: 6.7123 - accuracy: 1.0000\n",
      "Epoch 187/500: - 0.0524s/step - loss: 6.6961 - accuracy: 1.0000\n",
      "Epoch 188/500: - 0.0525s/step - loss: 6.6800 - accuracy: 1.0000\n",
      "Epoch 189/500: - 0.0525s/step - loss: 6.6640 - accuracy: 1.0000\n",
      "Epoch 190/500: - 0.0525s/step - loss: 6.6482 - accuracy: 1.0000\n",
      "Epoch 191/500: - 0.0526s/step - loss: 6.6325 - accuracy: 1.0000\n",
      "Epoch 192/500: - 0.0527s/step - loss: 6.6170 - accuracy: 1.0000\n",
      "Epoch 193/500: - 0.0526s/step - loss: 6.6016 - accuracy: 1.0000\n",
      "Epoch 194/500: - 0.0525s/step - loss: 6.5864 - accuracy: 1.0000\n",
      "Epoch 195/500: - 0.0525s/step - loss: 6.5713 - accuracy: 1.0000\n",
      "Epoch 196/500: - 0.0526s/step - loss: 6.5563 - accuracy: 1.0000\n",
      "Epoch 197/500: - 0.0526s/step - loss: 6.5415 - accuracy: 1.0000\n",
      "Epoch 198/500: - 0.0526s/step - loss: 6.5268 - accuracy: 1.0000\n",
      "Epoch 199/500: - 0.0526s/step - loss: 6.5122 - accuracy: 1.0000\n",
      "Epoch 200/500: - 0.0526s/step - loss: 6.4977 - accuracy: 1.0000\n",
      "Epoch 201/500: - 0.0526s/step - loss: 6.4834 - accuracy: 1.0000\n",
      "Epoch 202/500: - 0.0526s/step - loss: 6.4692 - accuracy: 1.0000\n",
      "Epoch 203/500: - 0.0526s/step - loss: 6.4551 - accuracy: 1.0000\n",
      "Epoch 204/500: - 0.0527s/step - loss: 6.4411 - accuracy: 1.0000\n",
      "Epoch 205/500: - 0.0528s/step - loss: 6.4273 - accuracy: 1.0000\n",
      "Epoch 206/500: - 0.0528s/step - loss: 6.4135 - accuracy: 1.0000\n",
      "Epoch 207/500: - 0.0527s/step - loss: 6.3999 - accuracy: 1.0000\n",
      "Epoch 208/500: - 0.0527s/step - loss: 6.3864 - accuracy: 1.0000\n",
      "Epoch 209/500: - 0.0528s/step - loss: 6.3730 - accuracy: 1.0000\n",
      "Epoch 210/500: - 0.0529s/step - loss: 6.3597 - accuracy: 1.0000\n",
      "Epoch 211/500: - 0.0528s/step - loss: 6.3465 - accuracy: 1.0000\n",
      "Epoch 212/500: - 0.0528s/step - loss: 6.3335 - accuracy: 1.0000\n",
      "Epoch 213/500: - 0.0528s/step - loss: 6.3205 - accuracy: 1.0000\n",
      "Epoch 214/500: - 0.0528s/step - loss: 6.3076 - accuracy: 1.0000\n",
      "Epoch 215/500: - 0.0528s/step - loss: 6.2949 - accuracy: 1.0000\n",
      "Epoch 216/500: - 0.0529s/step - loss: 6.2822 - accuracy: 1.0000\n",
      "Epoch 217/500: - 0.0529s/step - loss: 6.2696 - accuracy: 1.0000\n",
      "Epoch 218/500: - 0.0529s/step - loss: 6.2572 - accuracy: 1.0000\n",
      "Epoch 219/500: - 0.0528s/step - loss: 6.2448 - accuracy: 1.0000\n",
      "Epoch 220/500: - 0.0528s/step - loss: 6.2325 - accuracy: 1.0000\n",
      "Epoch 221/500: - 0.0528s/step - loss: 6.2203 - accuracy: 1.0000\n",
      "Epoch 222/500: - 0.0529s/step - loss: 6.2083 - accuracy: 1.0000\n",
      "Epoch 223/500: - 0.0529s/step - loss: 6.1963 - accuracy: 1.0000\n",
      "Epoch 224/500: - 0.0529s/step - loss: 6.1844 - accuracy: 1.0000\n",
      "Epoch 225/500: - 0.0529s/step - loss: 6.1726 - accuracy: 1.0000\n",
      "Epoch 226/500: - 0.0530s/step - loss: 6.1608 - accuracy: 1.0000\n",
      "Epoch 227/500: - 0.0531s/step - loss: 6.1492 - accuracy: 1.0000\n",
      "Epoch 228/500: - 0.0530s/step - loss: 6.1377 - accuracy: 1.0000\n",
      "Epoch 229/500: - 0.0531s/step - loss: 6.1262 - accuracy: 1.0000\n",
      "Epoch 230/500: - 0.0531s/step - loss: 6.1148 - accuracy: 1.0000\n",
      "Epoch 231/500: - 0.0531s/step - loss: 6.1035 - accuracy: 1.0000\n",
      "Epoch 232/500: - 0.0530s/step - loss: 6.0923 - accuracy: 1.0000\n",
      "Epoch 233/500: - 0.0530s/step - loss: 6.0812 - accuracy: 1.0000\n",
      "Epoch 234/500: - 0.0530s/step - loss: 6.0702 - accuracy: 1.0000\n",
      "Epoch 235/500: - 0.0531s/step - loss: 6.0592 - accuracy: 1.0000\n",
      "Epoch 236/500: - 0.0531s/step - loss: 6.0483 - accuracy: 1.0000\n",
      "Epoch 237/500: - 0.0531s/step - loss: 6.0375 - accuracy: 1.0000\n",
      "Epoch 238/500: - 0.0531s/step - loss: 6.0268 - accuracy: 1.0000\n",
      "Epoch 239/500: - 0.0532s/step - loss: 6.0161 - accuracy: 1.0000\n",
      "Epoch 240/500: - 0.0535s/step - loss: 6.0055 - accuracy: 1.0000\n",
      "Epoch 241/500: - 0.0539s/step - loss: 5.9950 - accuracy: 1.0000\n",
      "Epoch 242/500: - 0.0539s/step - loss: 5.9846 - accuracy: 1.0000\n",
      "Epoch 243/500: - 0.0540s/step - loss: 5.9742 - accuracy: 1.0000\n",
      "Epoch 244/500: - 0.0540s/step - loss: 5.9639 - accuracy: 1.0000\n",
      "Epoch 245/500: - 0.0541s/step - loss: 5.9537 - accuracy: 1.0000\n",
      "Epoch 246/500: - 0.0541s/step - loss: 5.9436 - accuracy: 1.0000\n",
      "Epoch 247/500: - 0.0541s/step - loss: 5.9335 - accuracy: 1.0000\n",
      "Epoch 248/500: - 0.0540s/step - loss: 5.9235 - accuracy: 1.0000\n",
      "Epoch 249/500: - 0.0540s/step - loss: 5.9136 - accuracy: 1.0000\n",
      "Epoch 250/500: - 0.0540s/step - loss: 5.9037 - accuracy: 1.0000\n",
      "Epoch 251/500: - 0.0540s/step - loss: 5.8939 - accuracy: 1.0000\n",
      "Epoch 252/500: - 0.0539s/step - loss: 5.8842 - accuracy: 1.0000\n",
      "Epoch 253/500: - 0.0539s/step - loss: 5.8745 - accuracy: 1.0000\n",
      "Epoch 254/500: - 0.0539s/step - loss: 5.8649 - accuracy: 1.0000\n",
      "Epoch 255/500: - 0.0540s/step - loss: 5.8553 - accuracy: 1.0000\n",
      "Epoch 256/500: - 0.0539s/step - loss: 5.8458 - accuracy: 1.0000\n",
      "Epoch 257/500: - 0.0539s/step - loss: 5.8364 - accuracy: 1.0000\n",
      "Epoch 258/500: - 0.0539s/step - loss: 5.8271 - accuracy: 1.0000\n",
      "Epoch 259/500: - 0.0539s/step - loss: 5.8178 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260/500: - 0.0540s/step - loss: 5.8085 - accuracy: 1.0000\n",
      "Epoch 261/500: - 0.0539s/step - loss: 5.7994 - accuracy: 1.0000\n",
      "Epoch 262/500: - 0.0539s/step - loss: 5.7903 - accuracy: 1.0000\n",
      "Epoch 263/500: - 0.0539s/step - loss: 5.7812 - accuracy: 1.0000\n",
      "Epoch 264/500: - 0.0538s/step - loss: 5.7722 - accuracy: 1.0000\n",
      "Epoch 265/500: - 0.0538s/step - loss: 5.7633 - accuracy: 1.0000\n",
      "Epoch 266/500: - 0.0539s/step - loss: 5.7544 - accuracy: 1.0000\n",
      "Epoch 267/500: - 0.0539s/step - loss: 5.7456 - accuracy: 1.0000\n",
      "Epoch 268/500: - 0.0539s/step - loss: 5.7368 - accuracy: 1.0000\n",
      "Epoch 269/500: - 0.0539s/step - loss: 5.7281 - accuracy: 1.0000\n",
      "Epoch 270/500: - 0.0539s/step - loss: 5.7194 - accuracy: 1.0000\n",
      "Epoch 271/500: - 0.0540s/step - loss: 5.7108 - accuracy: 1.0000\n",
      "Epoch 272/500: - 0.0539s/step - loss: 5.7022 - accuracy: 1.0000\n",
      "Epoch 273/500: - 0.0540s/step - loss: 5.6937 - accuracy: 1.0000\n",
      "Epoch 274/500: - 0.0539s/step - loss: 5.6853 - accuracy: 1.0000\n",
      "Epoch 275/500: - 0.0539s/step - loss: 5.6769 - accuracy: 1.0000\n",
      "Epoch 276/500: - 0.0539s/step - loss: 5.6686 - accuracy: 1.0000\n",
      "Epoch 277/500: - 0.0538s/step - loss: 5.6603 - accuracy: 1.0000\n",
      "Epoch 278/500: - 0.0539s/step - loss: 5.6520 - accuracy: 1.0000\n",
      "Epoch 279/500: - 0.0539s/step - loss: 5.6438 - accuracy: 1.0000\n",
      "Epoch 280/500: - 0.0539s/step - loss: 5.6357 - accuracy: 1.0000\n",
      "Epoch 281/500: - 0.0539s/step - loss: 5.6276 - accuracy: 1.0000\n",
      "Epoch 282/500: - 0.0538s/step - loss: 5.6196 - accuracy: 1.0000\n",
      "Epoch 283/500: - 0.0538s/step - loss: 5.6116 - accuracy: 1.0000\n",
      "Epoch 284/500: - 0.0539s/step - loss: 5.6036 - accuracy: 1.0000\n",
      "Epoch 285/500: - 0.0539s/step - loss: 5.5957 - accuracy: 1.0000\n",
      "Epoch 286/500: - 0.0539s/step - loss: 5.5879 - accuracy: 1.0000\n",
      "Epoch 287/500: - 0.0538s/step - loss: 5.5801 - accuracy: 1.0000\n",
      "Epoch 288/500: - 0.0538s/step - loss: 5.5723 - accuracy: 1.0000\n",
      "Epoch 289/500: - 0.0538s/step - loss: 5.5646 - accuracy: 1.0000\n",
      "Epoch 290/500: - 0.0538s/step - loss: 5.5569 - accuracy: 1.0000\n",
      "Epoch 291/500: - 0.0537s/step - loss: 5.5493 - accuracy: 1.0000\n",
      "Epoch 292/500: - 0.0537s/step - loss: 5.5417 - accuracy: 1.0000\n",
      "Epoch 293/500: - 0.0538s/step - loss: 5.5342 - accuracy: 1.0000\n",
      "Epoch 294/500: - 0.0537s/step - loss: 5.5267 - accuracy: 1.0000\n",
      "Epoch 295/500: - 0.0537s/step - loss: 5.5192 - accuracy: 1.0000\n",
      "Epoch 296/500: - 0.0537s/step - loss: 5.5118 - accuracy: 1.0000\n",
      "Epoch 297/500: - 0.0537s/step - loss: 5.5045 - accuracy: 1.0000\n",
      "Epoch 298/500: - 0.0538s/step - loss: 5.4972 - accuracy: 1.0000\n",
      "Epoch 299/500: - 0.0538s/step - loss: 5.4899 - accuracy: 1.0000\n",
      "Epoch 300/500: - 0.0538s/step - loss: 5.4826 - accuracy: 1.0000\n",
      "Epoch 301/500: - 0.0538s/step - loss: 5.4754 - accuracy: 1.0000\n",
      "Epoch 302/500: - 0.0538s/step - loss: 5.4683 - accuracy: 1.0000\n",
      "Epoch 303/500: - 0.0538s/step - loss: 5.4612 - accuracy: 1.0000\n",
      "Epoch 304/500: - 0.0538s/step - loss: 5.4541 - accuracy: 1.0000\n",
      "Epoch 305/500: - 0.0538s/step - loss: 5.4471 - accuracy: 1.0000\n",
      "Epoch 306/500: - 0.0537s/step - loss: 5.4401 - accuracy: 1.0000\n",
      "Epoch 307/500: - 0.0537s/step - loss: 5.4331 - accuracy: 1.0000\n",
      "Epoch 308/500: - 0.0537s/step - loss: 5.4262 - accuracy: 1.0000\n",
      "Epoch 309/500: - 0.0536s/step - loss: 5.4193 - accuracy: 1.0000\n",
      "Epoch 310/500: - 0.0536s/step - loss: 5.4125 - accuracy: 1.0000\n",
      "Epoch 311/500: - 0.0536s/step - loss: 5.4056 - accuracy: 1.0000\n",
      "Epoch 312/500: - 0.0536s/step - loss: 5.3989 - accuracy: 1.0000\n",
      "Epoch 313/500: - 0.0536s/step - loss: 5.3921 - accuracy: 1.0000\n",
      "Epoch 314/500: - 0.0536s/step - loss: 5.3855 - accuracy: 1.0000\n",
      "Epoch 315/500: - 0.0535s/step - loss: 5.3788 - accuracy: 1.0000\n",
      "Epoch 316/500: - 0.0536s/step - loss: 5.3722 - accuracy: 1.0000\n",
      "Epoch 317/500: - 0.0536s/step - loss: 5.3656 - accuracy: 1.0000\n",
      "Epoch 318/500: - 0.0537s/step - loss: 5.3590 - accuracy: 1.0000\n",
      "Epoch 319/500: - 0.0537s/step - loss: 5.3525 - accuracy: 1.0000\n",
      "Epoch 320/500: - 0.0537s/step - loss: 5.3460 - accuracy: 1.0000\n",
      "Epoch 321/500: - 0.0536s/step - loss: 5.3396 - accuracy: 1.0000\n",
      "Epoch 322/500: - 0.0536s/step - loss: 5.3332 - accuracy: 1.0000\n",
      "Epoch 323/500: - 0.0536s/step - loss: 5.3268 - accuracy: 1.0000\n",
      "Epoch 324/500: - 0.0535s/step - loss: 5.3205 - accuracy: 1.0000\n",
      "Epoch 325/500: - 0.0535s/step - loss: 5.3141 - accuracy: 1.0000\n",
      "Epoch 326/500: - 0.0535s/step - loss: 5.3079 - accuracy: 1.0000\n",
      "Epoch 327/500: - 0.0535s/step - loss: 5.3016 - accuracy: 1.0000\n",
      "Epoch 328/500: - 0.0534s/step - loss: 5.2954 - accuracy: 1.0000\n",
      "Epoch 329/500: - 0.0534s/step - loss: 5.2892 - accuracy: 1.0000\n",
      "Epoch 330/500: - 0.0534s/step - loss: 5.2831 - accuracy: 1.0000\n",
      "Epoch 331/500: - 0.0534s/step - loss: 5.2770 - accuracy: 1.0000\n",
      "Epoch 332/500: - 0.0534s/step - loss: 5.2709 - accuracy: 1.0000\n",
      "Epoch 333/500: - 0.0535s/step - loss: 5.2648 - accuracy: 1.0000\n",
      "Epoch 334/500: - 0.0536s/step - loss: 5.2588 - accuracy: 1.0000\n",
      "Epoch 335/500: - 0.0535s/step - loss: 5.2528 - accuracy: 1.0000\n",
      "Epoch 336/500: - 0.0535s/step - loss: 5.2469 - accuracy: 1.0000\n",
      "Epoch 337/500: - 0.0535s/step - loss: 5.2409 - accuracy: 1.0000\n",
      "Epoch 338/500: - 0.0535s/step - loss: 5.2350 - accuracy: 1.0000\n",
      "Epoch 339/500: - 0.0535s/step - loss: 5.2292 - accuracy: 1.0000\n",
      "Epoch 340/500: - 0.0535s/step - loss: 5.2233 - accuracy: 1.0000\n",
      "Epoch 341/500: - 0.0535s/step - loss: 5.2175 - accuracy: 1.0000\n",
      "Epoch 342/500: - 0.0535s/step - loss: 5.2117 - accuracy: 1.0000\n",
      "Epoch 343/500: - 0.0535s/step - loss: 5.2060 - accuracy: 1.0000\n",
      "Epoch 344/500: - 0.0534s/step - loss: 5.2003 - accuracy: 1.0000\n",
      "Epoch 345/500: - 0.0534s/step - loss: 5.1946 - accuracy: 1.0000\n",
      "Epoch 346/500: - 0.0534s/step - loss: 5.1889 - accuracy: 1.0000\n",
      "Epoch 347/500: - 0.0534s/step - loss: 5.1833 - accuracy: 1.0000\n",
      "Epoch 348/500: - 0.0534s/step - loss: 5.1776 - accuracy: 1.0000\n",
      "Epoch 349/500: - 0.0534s/step - loss: 5.1721 - accuracy: 1.0000\n",
      "Epoch 350/500: - 0.0533s/step - loss: 5.1665 - accuracy: 1.0000\n",
      "Epoch 351/500: - 0.0533s/step - loss: 5.1610 - accuracy: 1.0000\n",
      "Epoch 352/500: - 0.0533s/step - loss: 5.1555 - accuracy: 1.0000\n",
      "Epoch 353/500: - 0.0533s/step - loss: 5.1500 - accuracy: 1.0000\n",
      "Epoch 354/500: - 0.0532s/step - loss: 5.1446 - accuracy: 1.0000\n",
      "Epoch 355/500: - 0.0532s/step - loss: 5.1391 - accuracy: 1.0000\n",
      "Epoch 356/500: - 0.0532s/step - loss: 5.1338 - accuracy: 1.0000\n",
      "Epoch 357/500: - 0.0532s/step - loss: 5.1284 - accuracy: 1.0000\n",
      "Epoch 358/500: - 0.0532s/step - loss: 5.1230 - accuracy: 1.0000\n",
      "Epoch 359/500: - 0.0533s/step - loss: 5.1177 - accuracy: 1.0000\n",
      "Epoch 360/500: - 0.0534s/step - loss: 5.1124 - accuracy: 1.0000\n",
      "Epoch 361/500: - 0.0534s/step - loss: 5.1072 - accuracy: 1.0000\n",
      "Epoch 362/500: - 0.0533s/step - loss: 5.1019 - accuracy: 1.0000\n",
      "Epoch 363/500: - 0.0533s/step - loss: 5.0967 - accuracy: 1.0000\n",
      "Epoch 364/500: - 0.0533s/step - loss: 5.0915 - accuracy: 1.0000\n",
      "Epoch 365/500: - 0.0532s/step - loss: 5.0864 - accuracy: 1.0000\n",
      "Epoch 366/500: - 0.0532s/step - loss: 5.0812 - accuracy: 1.0000\n",
      "Epoch 367/500: - 0.0532s/step - loss: 5.0761 - accuracy: 1.0000\n",
      "Epoch 368/500: - 0.0532s/step - loss: 5.0710 - accuracy: 1.0000\n",
      "Epoch 369/500: - 0.0531s/step - loss: 5.0659 - accuracy: 1.0000\n",
      "Epoch 370/500: - 0.0531s/step - loss: 5.0609 - accuracy: 1.0000\n",
      "Epoch 371/500: - 0.0531s/step - loss: 5.0559 - accuracy: 1.0000\n",
      "Epoch 372/500: - 0.0531s/step - loss: 5.0509 - accuracy: 1.0000\n",
      "Epoch 373/500: - 0.0531s/step - loss: 5.0459 - accuracy: 1.0000\n",
      "Epoch 374/500: - 0.0531s/step - loss: 5.0410 - accuracy: 1.0000\n",
      "Epoch 375/500: - 0.0531s/step - loss: 5.0360 - accuracy: 1.0000\n",
      "Epoch 376/500: - 0.0531s/step - loss: 5.0311 - accuracy: 1.0000\n",
      "Epoch 377/500: - 0.0531s/step - loss: 5.0262 - accuracy: 1.0000\n",
      "Epoch 378/500: - 0.0531s/step - loss: 5.0214 - accuracy: 1.0000\n",
      "Epoch 379/500: - 0.0531s/step - loss: 5.0165 - accuracy: 1.0000\n",
      "Epoch 380/500: - 0.0530s/step - loss: 5.0117 - accuracy: 1.0000\n",
      "Epoch 381/500: - 0.0530s/step - loss: 5.0069 - accuracy: 1.0000\n",
      "Epoch 382/500: - 0.0530s/step - loss: 5.0022 - accuracy: 1.0000\n",
      "Epoch 383/500: - 0.0529s/step - loss: 4.9974 - accuracy: 1.0000\n",
      "Epoch 384/500: - 0.0529s/step - loss: 4.9927 - accuracy: 1.0000\n",
      "Epoch 385/500: - 0.0529s/step - loss: 4.9880 - accuracy: 1.0000\n",
      "Epoch 386/500: - 0.0529s/step - loss: 4.9833 - accuracy: 1.0000\n",
      "Epoch 387/500: - 0.0529s/step - loss: 4.9786 - accuracy: 1.0000\n",
      "Epoch 388/500: - 0.0529s/step - loss: 4.9740 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/500: - 0.0529s/step - loss: 4.9693 - accuracy: 1.0000\n",
      "Epoch 390/500: - 0.0528s/step - loss: 4.9647 - accuracy: 1.0000\n",
      "Epoch 391/500: - 0.0528s/step - loss: 4.9602 - accuracy: 1.0000\n",
      "Epoch 392/500: - 0.0528s/step - loss: 4.9556 - accuracy: 1.0000\n",
      "Epoch 393/500: - 0.0528s/step - loss: 4.9511 - accuracy: 1.0000\n",
      "Epoch 394/500: - 0.0528s/step - loss: 4.9465 - accuracy: 1.0000\n",
      "Epoch 395/500: - 0.0528s/step - loss: 4.9420 - accuracy: 1.0000\n",
      "Epoch 396/500: - 0.0529s/step - loss: 4.9375 - accuracy: 1.0000\n",
      "Epoch 397/500: - 0.0529s/step - loss: 4.9331 - accuracy: 1.0000\n",
      "Epoch 398/500: - 0.0530s/step - loss: 4.9286 - accuracy: 1.0000\n",
      "Epoch 399/500: - 0.0530s/step - loss: 4.9242 - accuracy: 1.0000\n",
      "Epoch 400/500: - 0.0531s/step - loss: 4.9198 - accuracy: 1.0000\n",
      "Epoch 401/500: - 0.0531s/step - loss: 4.9154 - accuracy: 1.0000\n",
      "Epoch 402/500: - 0.0531s/step - loss: 4.9111 - accuracy: 1.0000\n",
      "Epoch 403/500: - 0.0531s/step - loss: 4.9067 - accuracy: 1.0000\n",
      "Epoch 404/500: - 0.0531s/step - loss: 4.9024 - accuracy: 1.0000\n",
      "Epoch 405/500: - 0.0531s/step - loss: 4.8981 - accuracy: 1.0000\n",
      "Epoch 406/500: - 0.0530s/step - loss: 4.8938 - accuracy: 1.0000\n",
      "Epoch 407/500: - 0.0530s/step - loss: 4.8895 - accuracy: 1.0000\n",
      "Epoch 408/500: - 0.0530s/step - loss: 4.8853 - accuracy: 1.0000\n",
      "Epoch 409/500: - 0.0530s/step - loss: 4.8810 - accuracy: 1.0000\n",
      "Epoch 410/500: - 0.0531s/step - loss: 4.8768 - accuracy: 1.0000\n",
      "Epoch 411/500: - 0.0531s/step - loss: 4.8726 - accuracy: 1.0000\n",
      "Epoch 412/500: - 0.0531s/step - loss: 4.8684 - accuracy: 1.0000\n",
      "Epoch 413/500: - 0.0530s/step - loss: 4.8643 - accuracy: 1.0000\n",
      "Epoch 414/500: - 0.0530s/step - loss: 4.8601 - accuracy: 1.0000\n",
      "Epoch 415/500: - 0.0530s/step - loss: 4.8560 - accuracy: 1.0000\n",
      "Epoch 416/500: - 0.0530s/step - loss: 4.8519 - accuracy: 1.0000\n",
      "Epoch 417/500: - 0.0530s/step - loss: 4.8478 - accuracy: 1.0000\n",
      "Epoch 418/500: - 0.0530s/step - loss: 4.8437 - accuracy: 1.0000\n",
      "Epoch 419/500: - 0.0530s/step - loss: 4.8397 - accuracy: 1.0000\n",
      "Epoch 420/500: - 0.0530s/step - loss: 4.8356 - accuracy: 1.0000\n",
      "Epoch 421/500: - 0.0530s/step - loss: 4.8316 - accuracy: 1.0000\n",
      "Epoch 422/500: - 0.0530s/step - loss: 4.8276 - accuracy: 1.0000\n",
      "Epoch 423/500: - 0.0530s/step - loss: 4.8236 - accuracy: 1.0000\n",
      "Epoch 424/500: - 0.0530s/step - loss: 4.8196 - accuracy: 1.0000\n",
      "Epoch 425/500: - 0.0530s/step - loss: 4.8157 - accuracy: 1.0000\n",
      "Epoch 426/500: - 0.0530s/step - loss: 4.8117 - accuracy: 1.0000\n",
      "Epoch 427/500: - 0.0530s/step - loss: 4.8078 - accuracy: 1.0000\n",
      "Epoch 428/500: - 0.0530s/step - loss: 4.8039 - accuracy: 1.0000\n",
      "Epoch 429/500: - 0.0530s/step - loss: 4.8000 - accuracy: 1.0000\n",
      "Epoch 430/500: - 0.0530s/step - loss: 4.7961 - accuracy: 1.0000\n",
      "Epoch 431/500: - 0.0530s/step - loss: 4.7922 - accuracy: 1.0000\n",
      "Epoch 432/500: - 0.0530s/step - loss: 4.7884 - accuracy: 1.0000\n",
      "Epoch 433/500: - 0.0529s/step - loss: 4.7846 - accuracy: 1.0000\n",
      "Epoch 434/500: - 0.0529s/step - loss: 4.7808 - accuracy: 1.0000\n",
      "Epoch 435/500: - 0.0530s/step - loss: 4.7770 - accuracy: 1.0000\n",
      "Epoch 436/500: - 0.0530s/step - loss: 4.7732 - accuracy: 1.0000\n",
      "Epoch 437/500: - 0.0530s/step - loss: 4.7694 - accuracy: 1.0000\n",
      "Epoch 438/500: - 0.0530s/step - loss: 4.7657 - accuracy: 1.0000\n",
      "Epoch 439/500: - 0.0530s/step - loss: 4.7619 - accuracy: 1.0000\n",
      "Epoch 440/500: - 0.0530s/step - loss: 4.7582 - accuracy: 1.0000\n",
      "Epoch 441/500: - 0.0529s/step - loss: 4.7545 - accuracy: 1.0000\n",
      "Epoch 442/500: - 0.0529s/step - loss: 4.7508 - accuracy: 1.0000\n",
      "Epoch 443/500: - 0.0529s/step - loss: 4.7471 - accuracy: 1.0000\n",
      "Epoch 444/500: - 0.0529s/step - loss: 4.7434 - accuracy: 1.0000\n",
      "Epoch 445/500: - 0.0529s/step - loss: 4.7398 - accuracy: 1.0000\n",
      "Epoch 446/500: - 0.0529s/step - loss: 4.7362 - accuracy: 1.0000\n",
      "Epoch 447/500: - 0.0529s/step - loss: 4.7325 - accuracy: 1.0000\n",
      "Epoch 448/500: - 0.0529s/step - loss: 4.7289 - accuracy: 1.0000\n",
      "Epoch 449/500: - 0.0529s/step - loss: 4.7253 - accuracy: 1.0000\n",
      "Epoch 450/500: - 0.0529s/step - loss: 4.7218 - accuracy: 1.0000\n",
      "Epoch 451/500: - 0.0529s/step - loss: 4.7182 - accuracy: 1.0000\n",
      "Epoch 452/500: - 0.0528s/step - loss: 4.7146 - accuracy: 1.0000\n",
      "Epoch 453/500: - 0.0529s/step - loss: 4.7111 - accuracy: 1.0000\n",
      "Epoch 454/500: - 0.0529s/step - loss: 4.7076 - accuracy: 1.0000\n",
      "Epoch 455/500: - 0.0529s/step - loss: 4.7041 - accuracy: 1.0000\n",
      "Epoch 456/500: - 0.0529s/step - loss: 4.7006 - accuracy: 1.0000\n",
      "Epoch 457/500: - 0.0529s/step - loss: 4.6971 - accuracy: 1.0000\n",
      "Epoch 458/500: - 0.0530s/step - loss: 4.6936 - accuracy: 1.0000\n",
      "Epoch 459/500: - 0.0530s/step - loss: 4.6902 - accuracy: 1.0000\n",
      "Epoch 460/500: - 0.0529s/step - loss: 4.6867 - accuracy: 1.0000\n",
      "Epoch 461/500: - 0.0529s/step - loss: 4.6833 - accuracy: 1.0000\n",
      "Epoch 462/500: - 0.0529s/step - loss: 4.6799 - accuracy: 1.0000\n",
      "Epoch 463/500: - 0.0530s/step - loss: 4.6765 - accuracy: 1.0000\n",
      "Epoch 464/500: - 0.0530s/step - loss: 4.6731 - accuracy: 1.0000\n",
      "Epoch 465/500: - 0.0530s/step - loss: 4.6697 - accuracy: 1.0000\n",
      "Epoch 466/500: - 0.0531s/step - loss: 4.6664 - accuracy: 1.0000\n",
      "Epoch 467/500: - 0.0531s/step - loss: 4.6630 - accuracy: 1.0000\n",
      "Epoch 468/500: - 0.0531s/step - loss: 4.6597 - accuracy: 1.0000\n",
      "Epoch 469/500: - 0.0531s/step - loss: 4.6564 - accuracy: 1.0000\n",
      "Epoch 470/500: - 0.0531s/step - loss: 4.6531 - accuracy: 1.0000\n",
      "Epoch 471/500: - 0.0531s/step - loss: 4.6498 - accuracy: 1.0000\n",
      "Epoch 472/500: - 0.0531s/step - loss: 4.6465 - accuracy: 1.0000\n",
      "Epoch 473/500: - 0.0532s/step - loss: 4.6432 - accuracy: 1.0000\n",
      "Epoch 474/500: - 0.0532s/step - loss: 4.6399 - accuracy: 1.0000\n",
      "Epoch 475/500: - 0.0532s/step - loss: 4.6367 - accuracy: 1.0000\n",
      "Epoch 476/500: - 0.0532s/step - loss: 4.6334 - accuracy: 1.0000\n",
      "Epoch 477/500: - 0.0532s/step - loss: 4.6302 - accuracy: 1.0000\n",
      "Epoch 478/500: - 0.0531s/step - loss: 4.6270 - accuracy: 1.0000\n",
      "Epoch 479/500: - 0.0531s/step - loss: 4.6238 - accuracy: 1.0000\n",
      "Epoch 480/500: - 0.0531s/step - loss: 4.6206 - accuracy: 1.0000\n",
      "Epoch 481/500: - 0.0531s/step - loss: 4.6174 - accuracy: 1.0000\n",
      "Epoch 482/500: - 0.0531s/step - loss: 4.6143 - accuracy: 1.0000\n",
      "Epoch 483/500: - 0.0531s/step - loss: 4.6111 - accuracy: 1.0000\n",
      "Epoch 484/500: - 0.0531s/step - loss: 4.6080 - accuracy: 1.0000\n",
      "Epoch 485/500: - 0.0531s/step - loss: 4.6048 - accuracy: 1.0000\n",
      "Epoch 486/500: - 0.0531s/step - loss: 4.6017 - accuracy: 1.0000\n",
      "Epoch 487/500: - 0.0531s/step - loss: 4.5986 - accuracy: 1.0000\n",
      "Epoch 488/500: - 0.0531s/step - loss: 4.5955 - accuracy: 1.0000\n",
      "Epoch 489/500: - 0.0531s/step - loss: 4.5924 - accuracy: 1.0000\n",
      "Epoch 490/500: - 0.0531s/step - loss: 4.5894 - accuracy: 1.0000\n",
      "Epoch 491/500: - 0.0531s/step - loss: 4.5863 - accuracy: 1.0000\n",
      "Epoch 492/500: - 0.0531s/step - loss: 4.5832 - accuracy: 1.0000\n",
      "Epoch 493/500: - 0.0531s/step - loss: 4.5802 - accuracy: 1.0000\n",
      "Epoch 494/500: - 0.0531s/step - loss: 4.5772 - accuracy: 1.0000\n",
      "Epoch 495/500: - 0.0531s/step - loss: 4.5742 - accuracy: 1.0000\n",
      "Epoch 496/500: - 0.0531s/step - loss: 4.5712 - accuracy: 1.0000\n",
      "Epoch 497/500: - 0.0531s/step - loss: 4.5682 - accuracy: 1.0000\n",
      "Epoch 498/500: - 0.0531s/step - loss: 4.5652 - accuracy: 1.0000\n",
      "Epoch 499/500: - 0.0531s/step - loss: 4.5622 - accuracy: 1.0000\n",
      "Epoch 500/500: - 0.0531s/step - loss: 4.5592 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model2.update_weights(images, network2[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #               zip(train_ds, network)]\n",
    "        \n",
    "        network_new2 = []\n",
    "        kernels_new2 = []\n",
    "        for (images, labels), net, hmc_kernel in zip(train_ds, network2, kernels2):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            net_current = net_current[0]\n",
    "        \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.split(new_state, [30], axis = 1)   \n",
    "            network_new2.append(net_new)\n",
    "            \n",
    "            ker_new = model2.generate_hmc_kernel(images, labels, new_step_size)\n",
    "            kernels_new2.append(ker_new)\n",
    "            \n",
    "        network2 = network_new2\n",
    "        kernels2 = kernels_new2\n",
    "        \n",
    "        loss += -1 * tf.reduce_mean(model2.target_log_prob(images, network2[bs], labels))\n",
    "    \n",
    "    preds = [model2.get_predictions(images) for images, labels in train_ds]\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
