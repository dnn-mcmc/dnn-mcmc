{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, MaxPooling2D\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12665 training images.\n",
      "There are 2115 test images.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select binary data\n",
    "label_sub = [0,1]\n",
    "x_train_sub = np.array([x for x, y in zip(x_train, y_train) if y in label_sub])\n",
    "y_train_sub = np.array([y for y in y_train if y in label_sub])\n",
    "x_test_sub = np.array([x for x, y in zip(x_test, y_test) if y in label_sub])\n",
    "y_test_sub = np.array([y for y in y_test if y in label_sub])\n",
    "\n",
    "print('There are', len(x_train_sub), 'training images.')\n",
    "print('There are', len(x_test_sub), 'test images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_sub shape: (12665, 28, 28, 1)\n",
      "Number of images in x_train_sub 12665\n",
      "Number of images in x_test_sub 2115\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train_sub = x_train_sub.reshape(x_train_sub.shape[0], 28, 28, 1)\n",
    "x_test_sub = x_test_sub.reshape(x_test_sub.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train_sub = x_train_sub.astype('float32')\n",
    "x_test_sub = x_test_sub.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train_sub /= 255\n",
    "x_test_sub /= 255\n",
    "print('x_train_sub shape:', x_train_sub.shape)\n",
    "print('Number of images in x_train_sub', x_train_sub.shape[0])\n",
    "print('Number of images in x_test_sub', x_test_sub.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_sub, y_train_sub)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test_sub, y_test_sub)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "\n",
    "    t = [tf.math.sigmoid(i) for i in x]\n",
    "    \n",
    "    return t\n",
    "\n",
    "def rerange(x, r = 6.0):\n",
    "    \n",
    "    out_of_range = tf.cast(tf.math.greater(tf.math.abs(x), r), tf.float32)\n",
    "    sign = tf.math.sign(x)\n",
    "    \n",
    "    return x * (1 - out_of_range) + sign * r * out_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN structure\n",
    "class StochasticCNN(Model):\n",
    "    \n",
    "    def __init__(self, n_outputs=10):\n",
    "        # preferred architecture: 784 - [32C5-P2] - [64C5-P2] - 128 - 10 with 40% dropout. \n",
    "        \n",
    "        super(StochasticCNN, self).__init__()\n",
    "        self.conv1 = Conv2D(filters = 32, kernel_size = (5, 5), input_shape = input_shape, activation = 'sigmoid') # define here\n",
    "        self.conv2 = Conv2D(filters = 64, kernel_size = (5, 5), activation = 'sigmoid')\n",
    "        self.max1 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.max2 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.dropout = Dropout((0.4))\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(128) \n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        # first conv + max layer\n",
    "        x = self.conv1(x)\n",
    "        x = tfp.distributions.Bernoulli(logits=x).sample()\n",
    "        network.append(x)\n",
    "        x = self.max1(x)\n",
    "        \n",
    "        # second conv + max layer\n",
    "        x = self.conv2(x)\n",
    "        x = tfp.distributions.Bernoulli(logits=x).sample()\n",
    "        network.append(x)\n",
    "        x = self.max2(x)\n",
    "        \n",
    "        # dropout and flatten layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # dense layer\n",
    "        x = self.fc1(x)\n",
    "        x = tfp.distributions.Bernoulli(logits=x).sample()\n",
    "        network.append(x)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "#        net = self.conv1(x)\n",
    "#        net = tfp.distributions.Bernoulli(logits=net).sample() # may object to shape of net (but probably not)\n",
    "#        net = None # max pool applied to net\n",
    "#        net = self.conv2(net)\n",
    "#        net = tfp.distributions.Bernoulli(logits=net).sample() # again, need to make sure logits can have arbitrary sh\n",
    "#        net = None # max pool appied to net\n",
    "#        net = tf.flatten(net)\n",
    "#        net = self.fc1(net)\n",
    "#        net = tfp.distributions.Bernoulli(logits=net).sample()\n",
    "#        net = self.output_layer(net)\n",
    "            \n",
    "        return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layers, n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # difine the layers here\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        \n",
    "    def call(self, x):\n",
    "            \n",
    "        network = []  \n",
    "            \n",
    "        logits = self.hidden_layers[0](x)\n",
    "        x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "        network.append(x)\n",
    "        x = self.hidden_layers[1](x)\n",
    "        x = self.hidden_layers[2](x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        y = [[i] for i in y]\n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_previous = [x]\n",
    "        \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, [self.hidden_layers[0]])):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "        \n",
    "        \n",
    "        f_logits = self.hidden_layers[2](self.hidden_layers[1](self.hidden_layers[0](x)))\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(f_logits))\n",
    "        \n",
    "        nlog_prob += tf.reduce_sum(fce, axis = 1)   \n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def target_log_prob2(self, x, h, y):\n",
    "        \n",
    "        #x = Flatten()(x)\n",
    "        y = [[i] for i in y]\n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_current = tf.reshape(h_current, [1, np.shape(h_current)[0], 26, 26, 28])\n",
    "        #net_new = tf.reshape(new_state, orig_shape)\n",
    "        \n",
    "        h_previous = [x]\n",
    "        \n",
    "        nlog_prob = 0.\n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, [self.hidden_layers[0]])):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "        \n",
    "        f_logits = self.hidden_layers[2](self.hidden_layers[1](self.hidden_layers[0](x)))\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(f_logits))\n",
    "        \n",
    "        nlog_prob += tf.reduce_sum(fce, axis = 1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_ker, update_ker = False):\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        orig_shape = np.shape(h_current)\n",
    "        h_current = tf.reshape(h_current, [orig_shape[1], -1]) # reshape to one dimension\n",
    "   \n",
    "        # initialize the HMC transition kernel\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = pow(1000, -1/4)),\n",
    "            num_adaptation_steps=int(100*0.8))\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 100\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps, \n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = adaptive_hmc,\n",
    "            trace_fn = None)\n",
    "        \n",
    "        h_new = tf.reshape(samples[0], orig_shape)\n",
    "\n",
    "        return(h_new)\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        #x = Flatten()(x)\n",
    "        logits = 0.0\n",
    "        for layer in self.hidden_layers[0]:\n",
    "            logits = layer(x)\n",
    "            x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tf.math.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tf.math.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hidden layers\n",
    "# conv2D_layer = Conv2D(filters = 28, kernel_size = (3, 3), input_shape = input_shape, activation = 'sigmoid')\n",
    "# maxpooling_layer = MaxPooling2D(pool_size=(2, 2))\n",
    "# flatten_layer = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layers = [conv2D_layer, maxpooling_layer, flatten_layer], n_outputs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "x = [tf.reshape(network[i], [np.shape(network[i])[1], -1]) for i in range(np.shape(network)[0]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlp = [model.target_log_prob(images, network[bs], labels) for bs, (images, labels) in enumerate(train_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py:74: UserWarning: TensorFloat-32 matmul/conv are enabled for NVIDIA Ampere+ GPUs. The resulting loss of precision may hinder MCMC convergence. To turn off, run `tf.config.experimental.enable_tensor_float_32_execution(False)`. For more detail, see https://github.com/tensorflow/community/pull/287.\n",
      "  'TensorFloat-32 matmul/conv are enabled for NVIDIA Ampere+ GPUs. The '\n"
     ]
    }
   ],
   "source": [
    "kernels = [model.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Step 0 - time 110.3135\n",
      "1\n",
      "Step 1 - time 110.3027\n",
      "2\n",
      "Step 2 - time 108.6763\n",
      "3\n",
      "Step 3 - time 106.0513\n",
      "4\n",
      "Step 4 - time 105.1981\n",
      "5\n",
      "Step 5 - time 106.6704\n",
      "6\n",
      "Step 6 - time 107.5867\n",
      "7\n",
      "Step 7 - time 106.5847\n",
      "8\n",
      "Step 8 - time 105.4451\n",
      "9\n",
      "Step 9 - time 105.1081\n",
      "10\n",
      "Step 10 - time 107.3652\n",
      "11\n",
      "Step 11 - time 106.5939\n",
      "12\n",
      "Step 12 - time 105.9476\n",
      "13\n",
      "Step 13 - time 106.4900\n",
      "14\n",
      "Step 14 - time 106.7593\n",
      "15\n",
      "Step 15 - time 106.4734\n",
      "16\n",
      "Step 16 - time 105.4809\n",
      "17\n",
      "Step 17 - time 105.1106\n",
      "18\n",
      "Step 18 - time 132.1748\n",
      "19\n",
      "Step 19 - time 173.4112\n"
     ]
    }
   ],
   "source": [
    "burnin = 20\n",
    "step_sizes = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    print(i)\n",
    "    network_new = []\n",
    "    kernels_new = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network, kernels):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        orig_shape = np.shape(net_current)\n",
    "        net_current = tf.reshape(net_current, [orig_shape[1], -1])\n",
    "\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2].new_step_size.numpy()\n",
    "        step_sizes.append(new_step_size)\n",
    "\n",
    "        new_state = rerange(samples[0][0])\n",
    "        net_new = tf.reshape(new_state, orig_shape)\n",
    "        network_new.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new.append(ker_new)\n",
    "            \n",
    "    network = network_new\n",
    "    kernels = kernels_new\n",
    "    \n",
    "    print(\"Step %d - time %.4f\" % (i, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes)), step_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(images, network[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #              zip(train_ds, network)]\n",
    "        \n",
    "        network_new = []\n",
    "        #kernels_new = []\n",
    "        for net, hmc_kernel in zip(network, kernels):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            orig_shape = np.shape(net_current)\n",
    "            net_current = tf.reshape(net_current, [orig_shape[1], -1])\n",
    "            \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "\n",
    "            #new_step_size = samples[2].new_step_size.numpy()\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.reshape(new_state, orig_shape)   \n",
    "            network_new.append(net_new)\n",
    "            \n",
    "            #ker_new = model.generate_hmc_kernel(images2, labels2, new_step_size)\n",
    "            #kernels_new.append(ker_new)\n",
    "            \n",
    "        network = network_new\n",
    "        #kernels = kernels_new\n",
    "\n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(images, network[bs], labels))\n",
    "       \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    #print(preds)\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
