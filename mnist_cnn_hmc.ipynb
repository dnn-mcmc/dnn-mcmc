{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, MaxPooling2D\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12665 training images.\n",
      "There are 2115 test images.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select binary data\n",
    "label_sub = [0,1]\n",
    "x_train_sub = np.array([x for x, y in zip(x_train, y_train) if y in label_sub])\n",
    "y_train_sub = np.array([y for y in y_train if y in label_sub])\n",
    "x_test_sub = np.array([x for x, y in zip(x_test, y_test) if y in label_sub])\n",
    "y_test_sub = np.array([y for y in y_test if y in label_sub])\n",
    "\n",
    "print('There are', len(x_train_sub), 'training images.')\n",
    "print('There are', len(x_test_sub), 'test images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_sub shape: (12665, 28, 28, 1)\n",
      "Number of images in x_train_sub 12665\n",
      "Number of images in x_test_sub 2115\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train_sub = x_train_sub.reshape(x_train_sub.shape[0], 28, 28, 1)\n",
    "x_test_sub = x_test_sub.reshape(x_test_sub.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train_sub = x_train_sub.astype('float32')\n",
    "x_test_sub = x_test_sub.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train_sub /= 255\n",
    "x_test_sub /= 255\n",
    "print('x_train_sub shape:', x_train_sub.shape)\n",
    "print('Number of images in x_train_sub', x_train_sub.shape[0])\n",
    "print('Number of images in x_test_sub', x_test_sub.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_sub, y_train_sub)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test_sub, y_test_sub)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "\n",
    "    t = [tf.math.sigmoid(i) for i in x]\n",
    "    \n",
    "    return t\n",
    "\n",
    "def rerange(x, r = 6.0):\n",
    "    \n",
    "    out_of_range = tf.cast(tf.math.greater(tf.math.abs(x), r), tf.float32)\n",
    "    sign = tf.math.sign(x)\n",
    "    \n",
    "    return x * (1 - out_of_range) + sign * r * out_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layers, n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # difine the layers here\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        \n",
    "    def call(self, x):\n",
    "            \n",
    "        network = []  \n",
    "            \n",
    "        logits = self.hidden_layers[0](x)\n",
    "        x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "        network.append(x)\n",
    "        x = self.hidden_layers[1](x)\n",
    "        x = self.hidden_layers[2](x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        y = [[i] for i in y]\n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_previous = [x]\n",
    "        \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, [self.hidden_layers[0]])):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "        \n",
    "        \n",
    "        f_logits = self.hidden_layers[2](self.hidden_layers[1](self.hidden_layers[0](x)))\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(f_logits))\n",
    "        \n",
    "        nlog_prob += tf.reduce_sum(fce, axis = 1)   \n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def target_log_prob2(self, x, h, y):\n",
    "        \n",
    "        #x = Flatten()(x)\n",
    "        y = [[i] for i in y]\n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_current = tf.reshape(h_current, [1, np.shape(h_current)[0], 26, 26, 28])\n",
    "        #net_new = tf.reshape(new_state, orig_shape)\n",
    "        \n",
    "        h_previous = [x]\n",
    "        \n",
    "        nlog_prob = 0.\n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, [self.hidden_layers[0]])):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "        \n",
    "        f_logits = self.hidden_layers[2](self.hidden_layers[1](self.hidden_layers[0](x)))\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(f_logits))\n",
    "        \n",
    "        nlog_prob += tf.reduce_sum(fce, axis = 1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_ker, update_ker = False):\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        orig_shape = np.shape(h_current)\n",
    "        h_current = tf.reshape(h_current, [orig_shape[1], -1]) # reshape to one dimension\n",
    "   \n",
    "        # initialize the HMC transition kernel\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = pow(1000, -1/4)),\n",
    "            num_adaptation_steps=int(100*0.8))\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 100\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps, \n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = adaptive_hmc,\n",
    "            trace_fn = None)\n",
    "        \n",
    "        h_new = tf.reshape(samples[0], orig_shape)\n",
    "\n",
    "        return(h_new)\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        #x = Flatten()(x)\n",
    "        logits = 0.0\n",
    "        for layer in self.hidden_layers[0]:\n",
    "            logits = layer(x)\n",
    "            x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tf.math.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tf.math.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hidden layers\n",
    "conv2D_layer = Conv2D(filters = 28, kernel_size = (3, 3), input_shape = input_shape, activation = 'sigmoid')\n",
    "maxpooling_layer = MaxPooling2D(pool_size=(2, 2))\n",
    "flatten_layer = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layers = [conv2D_layer, maxpooling_layer, flatten_layer], n_outputs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 26, 26, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(network[395])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "x = [tf.reshape(network[i], [np.shape(network[i])[1], -1]) for i in range(np.shape(network)[0]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlp = [model.target_log_prob(images, network[bs], labels) for bs, (images, labels) in enumerate(train_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py:74: UserWarning: TensorFloat-32 matmul/conv are enabled for NVIDIA Ampere+ GPUs. The resulting loss of precision may hinder MCMC convergence. To turn off, run `tf.config.experimental.enable_tensor_float_32_execution(False)`. For more detail, see https://github.com/tensorflow/community/pull/287.\n",
      "  'TensorFloat-32 matmul/conv are enabled for NVIDIA Ampere+ GPUs. The '\n"
     ]
    }
   ],
   "source": [
    "kernels = [model.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n",
      "(1, 32, 26, 26, 28)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9840cfc8ee7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtrace_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             return_final_kernel_results = True)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#print(samples[2].new_step_size.numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/sample.py\u001b[0m in \u001b[0;36msample_chain\u001b[0;34m(num_results, current_state, previous_kernel_results, kernel, num_burnin_steps, num_steps_between_results, trace_fn, return_final_kernel_results, parallel_iterations, seed, name)\u001b[0m\n\u001b[1;32m    372\u001b[0m             seed_state_and_results[1], trace_fn(*seed_state_and_results[1:])),\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# pylint: enable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         parallel_iterations=parallel_iterations)\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_final_kernel_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\u001b[0m in \u001b[0;36mtrace_scan\u001b[0;34m(loop_fn, initial_state, elems, trace_fn, trace_criterion_fn, static_trace_allocation_size, parallel_iterations, name)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         parallel_iterations=parallel_iterations)\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;31m# unflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 605\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2497\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2733\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2734\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\u001b[0m in \u001b[0;36m_body\u001b[0;34m(i, state, num_steps_traced, trace_arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps_traced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m       \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melems_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       trace_arrays, num_steps_traced = ps.cond(\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/sample.py\u001b[0m in \u001b[0;36m_trace_scan_fn\u001b[0;34m(seed_state_and_results, num_steps)\u001b[0m\n\u001b[1;32m    356\u001b[0m           \u001b[0mbody_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_seeded_one_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m           \u001b[0minitial_loop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_state_and_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m           parallel_iterations=parallel_iterations)\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_kernel_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\u001b[0m in \u001b[0;36msmart_for_loop\u001b[0;34m(loop_num_iter, body_fn, initial_loop_vars, parallel_iterations, unroll_threshold, name)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minitial_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m       )[1:]\n\u001b[1;32m    354\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_loop_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 605\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2497\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2733\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2734\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m       return tf.while_loop(\n\u001b[1;32m    349\u001b[0m           \u001b[0mcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mloop_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minitial_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/sample.py\u001b[0m in \u001b[0;36m_seeded_one_step\u001b[0;34m(seed, *state_and_results)\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0mone_step_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_seed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_seeded\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m       return [passalong_seed] + list(\n\u001b[0;32m--> 351\u001b[0;31m           kernel.one_step(*state_and_results, **one_step_kwargs))\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_trace_scan_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_state_and_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/simple_step_size_adaptation.py\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(self, current_state, previous_kernel_results, seed)\u001b[0m\n\u001b[1;32m    346\u001b[0m       \u001b[0minner_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m       new_state, new_inner_results = self.inner_kernel.one_step(\n\u001b[0;32m--> 348\u001b[0;31m           current_state, inner_results, **inner_kwargs)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m       \u001b[0;31m# Get the new step size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/hmc.py\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(self, current_state, previous_kernel_results, seed)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_step_size_assign\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m       next_state, kernel_results = self._impl.one_step(\n\u001b[0;32m--> 549\u001b[0;31m           current_state, previous_kernel_results, seed=seed)\n\u001b[0m\u001b[1;32m    550\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size_update_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         step_size_assign = self.step_size_update_fn(  # pylint: disable=not-callable\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/metropolis_hastings.py\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(self, current_state, previous_kernel_results, seed)\u001b[0m\n\u001b[1;32m    196\u001b[0m           \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m           \u001b[0mprevious_kernel_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccepted_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m           **inner_kwargs)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       if (not has_target_log_prob(proposed_results) or\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/hmc.py\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(self, current_state, previous_kernel_results, seed)\u001b[0m\n\u001b[1;32m    726\u001b[0m                      \u001b[0mcurrent_state_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                      \u001b[0mcurrent_target_log_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                      current_target_log_prob_grad_parts)\n\u001b[0m\u001b[1;32m    729\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_gradients_are_stopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mnext_state_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_state_parts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/leapfrog_integrator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, momentum_parts, state_parts, target, target_grad_parts, kinetic_energy_fn, name)\u001b[0m\n\u001b[1;32m    304\u001b[0m               \u001b[0mstate_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m               \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m               \u001b[0mtarget_grad_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m           ])\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 605\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2497\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2733\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2734\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/leapfrog_integrator.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, *args)\u001b[0m\n\u001b[1;32m    298\u001b[0m           \u001b[0mcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m           body=lambda i, *args: [i + 1] + list(_one_step(  # pylint: disable=no-value-for-parameter,g-long-lambda\n\u001b[0;32m--> 300\u001b[0;31m               self.target_fn, self.step_sizes, get_velocity_parts, *args)),\n\u001b[0m\u001b[1;32m    301\u001b[0m           loop_vars=[\n\u001b[1;32m    302\u001b[0m               \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/leapfrog_integrator.py\u001b[0m in \u001b[0;36m_one_step\u001b[0;34m(target_fn, step_sizes, get_velocity_parts, half_next_momentum_parts, state_parts, target, target_grad_parts)\u001b[0m\n\u001b[1;32m    342\u001b[0m           tf.cast(velocity_part, state_part.dtype))\n\u001b[1;32m    343\u001b[0m     [next_target, next_target_grad_parts] = mcmc_util.maybe_call_fn_and_grads(\n\u001b[0;32m--> 344\u001b[0;31m         target_fn, next_state_parts)\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_target_grad_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m       raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\u001b[0m in \u001b[0;36mmaybe_call_fn_and_grads\u001b[0;34m(fn, fn_arg_list, result, grads, check_non_none_grads, name)\u001b[0m\n\u001b[1;32m    289\u001b[0m     fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list)\n\u001b[1;32m    290\u001b[0m                    else [fn_arg_list])\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_value_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_arg_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     if not all(dtype_util.is_floating(r.dtype)\n\u001b[1;32m    293\u001b[0m                for r in (result if is_list_like(result) else [result])):  # pylint: disable=superfluous-parens\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\u001b[0m in \u001b[0;36m_value_and_gradients\u001b[0;34m(fn, fn_arg_list, result, grads, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m       ]\n\u001b[1;32m    275\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp_math_value_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_arg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/math/gradient.py\u001b[0m in \u001b[0;36mvalue_and_gradient\u001b[0;34m(f, output_gradients, use_gradient_tape, auto_unpack_single_arg, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mauto_unpack_single_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_unpack_single_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mexpand_tf_modules_as_trainable_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/math/gradient.py\u001b[0m in \u001b[0;36m_value_and_grad_impl\u001b[0;34m(f, grad_fn, output_gradients, auto_unpack_single_arg, expand_tf_modules_as_trainable_vars, *args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m   y, dydx = grad_fn(lambda: f(*args, **kwargs) if _has_args(f) else f(),\n\u001b[1;32m    333\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpand_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_kwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                     output_gradients)\n\u001b[0m\u001b[1;32m    335\u001b[0m   dydx_args, dydx_kwargs = tf.nest.pack_sequence_as(\n\u001b[1;32m    336\u001b[0m       [expand_args, expand_kwargs], dydx)\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/math/gradient.py\u001b[0m in \u001b[0;36m_gradient_new\u001b[0;34m(f, xs, grad_ys)\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_StridedSliceGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_axis_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice_grad\u001b[0;34m(shape, begin, end, strides, dy, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10637\u001b[0m         \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10638\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_axis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shrink_axis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10639\u001b[0;31m         shrink_axis_mask)\n\u001b[0m\u001b[1;32m  10640\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10641\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "burnin = 20\n",
    "step_sizes = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    print(i)\n",
    "    network_new = []\n",
    "    kernels_new = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network, kernels):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        orig_shape = np.shape(net_current)\n",
    "        net_current = tf.reshape(net_current, [orig_shape[1], -1])\n",
    "\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2].new_step_size.numpy()\n",
    "        step_sizes.append(new_step_size)\n",
    "\n",
    "        new_state = rerange(samples[0][0])\n",
    "        net_new = tf.reshape(new_state, orig_shape)\n",
    "        network_new.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new.append(ker_new)\n",
    "            \n",
    "    network = network_new\n",
    "    kernels = kernels_new\n",
    "    \n",
    "    print(\"Step %d - time %.4f\" % (i, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes)), step_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(images, network[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #              zip(train_ds, network)]\n",
    "        \n",
    "        network_new = []\n",
    "        #kernels_new = []\n",
    "        for net, hmc_kernel in zip(network, kernels):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            orig_shape = np.shape(net_current)\n",
    "            net_current = tf.reshape(net_current, [orig_shape[1], -1])\n",
    "            \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "\n",
    "            #new_step_size = samples[2].new_step_size.numpy()\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.reshape(new_state, orig_shape)   \n",
    "            network_new.append(net_new)\n",
    "            \n",
    "            #ker_new = model.generate_hmc_kernel(images2, labels2, new_step_size)\n",
    "            #kernels_new.append(ker_new)\n",
    "            \n",
    "        network = network_new\n",
    "        #kernels = kernels_new\n",
    "\n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(images, network[bs], labels))\n",
    "       \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    #print(preds)\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
