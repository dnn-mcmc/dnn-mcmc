{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X, Y = make_moons(200, noise = 0.1)\n",
    "\n",
    "# Split into test and training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=73)\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(*x_train[y_train.flatten() == 1, :].T)\n",
    "plt.scatter(*x_train[y_train.flatten() == 0, :].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (150, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train), np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard BP\n",
    "model_bp = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(2,)),\n",
    "        layers.Dense(100, activation = \"sigmoid\"),\n",
    "        #layers.Dense(50, activation = \"sigmoid\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "model_bp.summary()\n",
    "#model_bp.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "st = time.time()\n",
    "model_bp.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model_bp.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tm.sigmoid(i) for i in x]\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerange(x, r = 6.0):\n",
    "    \n",
    "    out_of_range = tf.cast(tm.greater(tm.abs(x), r), tf.float32)\n",
    "    sign = tm.sign(x)\n",
    "    \n",
    "    return x * (1 - out_of_range) + sign * r * out_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        #x = Flatten()(x)\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            logits = layer(pv)\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=logits)\n",
    "            \n",
    "            ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "\n",
    "    def target_log_prob2(self, x, h, y):\n",
    "\n",
    "        h_current = convert2_zero_one(tf.split(h, self.hidden_layer_sizes, axis = 1))\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        nlog_prob = 0.\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            logits = layer(pv)\n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=logits)\n",
    "            \n",
    "            ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_kernel, is_update_kernel = True):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat(h_current, axis = 1)\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_burnin_steps = 0\n",
    "        num_results = 1\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "    \n",
    "        # Generate new states of chains\n",
    "        #h_state = rerange(samples[0][0])\n",
    "        h_state = samples[0][0]\n",
    "        h_new = tf.split(h_state, self.hidden_layer_sizes, axis = 1) \n",
    "        \n",
    "        # Update the kernel if necesssary\n",
    "        if is_update_kernel:\n",
    "            new_step_size = samples[2].new_step_size.numpy()\n",
    "            ker_new = self.generate_hmc_kernel(x, y, new_step_size)\n",
    "            return(h_new, ker_new)\n",
    "        else:\n",
    "            return h_new\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layer_sizes = [5], n_outputs=1)\n",
    "model2 = StochasticMLP(hidden_layer_sizes = [5], n_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = [model.call(x) for x, y in train_ds]\n",
    "network2 = [model2.call(x) for x, y in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [model.generate_hmc_kernel(x, y) for x, y in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.get_weights()\n",
    "weight2 = model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.09006822, -0.22363245, -0.6246875 , -0.7322852 ,  0.68983495],\n",
       "        [-0.6195729 ,  0.88146317, -0.66768885,  0.67063725,  0.2162267 ]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.01589036],\n",
       "        [-0.69562626],\n",
       "        [ 0.74916196],\n",
       "        [ 0.0925169 ],\n",
       "        [-0.7242551 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.08395678,  0.36137366, -0.0949806 ,  0.04049188, -0.18412155],\n",
       "        [ 0.6710048 , -0.37701875,  0.56360435,  0.8476174 , -0.26150644]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.6077876 ],\n",
       "        [ 0.25538087],\n",
       "        [ 0.72847176],\n",
       "        [-0.50435424],\n",
       "        [-0.18243718]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00611144,  0.13774121, -0.7196681 , -0.6917933 ,  0.5057134 ],\n",
       "       [ 0.05143189,  0.5044444 , -0.10408449,  1.5182546 , -0.04527974]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight[0] + weight2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "burnin = 500\n",
    "for i in range(burnin):\n",
    "    \n",
    "    if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "        \n",
    "    network_new = []\n",
    "    kernels_new = []\n",
    "    \n",
    "    res = [model.propose_new_state_hamiltonian(x, net, y, ker) \n",
    "               for (x, y), net, ker in zip(train_ds, network, kernels)]\n",
    "    \n",
    "    network_new, kernels_new = zip(*res)\n",
    "         \n",
    "    network = network_new\n",
    "    kernels = kernels_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (x, y) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(x, network[bs], y, 0.1)\n",
    "        res = [model.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel = False) \\\n",
    "                   for (x, y), net, ker in zip(train_ds, network, kernels)]\n",
    "        network = res\n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(x, network[bs], y))\n",
    "    \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    train_acc = accuracy_score(np.concatenate(preds), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
