{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, MaxPooling2D\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12665 training images.\n",
      "There are 2115 test images.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select binary data\n",
    "label_sub = [0,1]\n",
    "x_train_sub = np.array([x for x, y in zip(x_train, y_train) if y in label_sub])\n",
    "y_train_sub = np.array([y for y in y_train if y in label_sub])\n",
    "x_test_sub = np.array([x for x, y in zip(x_test, y_test) if y in label_sub])\n",
    "y_test_sub = np.array([y for y in y_test if y in label_sub])\n",
    "\n",
    "print('There are', len(x_train_sub), 'training images.')\n",
    "print('There are', len(x_test_sub), 'test images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_sub shape: (12665, 28, 28, 1)\n",
      "Number of images in x_train_sub 12665\n",
      "Number of images in x_test_sub 2115\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train_sub = x_train_sub.reshape(x_train_sub.shape[0], 28, 28, 1)\n",
    "x_test_sub = x_test_sub.reshape(x_test_sub.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train_sub = x_train_sub.astype('float32')\n",
    "x_test_sub = x_test_sub.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train_sub /= 255\n",
    "x_test_sub /= 255\n",
    "print('x_train_sub shape:', x_train_sub.shape)\n",
    "print('Number of images in x_train_sub', x_train_sub.shape[0])\n",
    "print('Number of images in x_test_sub', x_test_sub.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_sub, y_train_sub)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test_sub, y_test_sub)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "\n",
    "    t = [tf.math.sigmoid(i) for i in x]\n",
    "    \n",
    "    return t\n",
    "\n",
    "def rerange(x, r = 6.0):\n",
    "    \n",
    "    out_of_range = tf.cast(tf.math.greater(tf.math.abs(x), r), tf.float32)\n",
    "    sign = tf.math.sign(x)\n",
    "    \n",
    "    return x * (1 - out_of_range) + sign * r * out_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class StochasticCNN(Model):\n",
    "    \n",
    "    def __init__(self, n_outputs=10):\n",
    "        # preferred architecture: 784 - [32C5-P2] - [64C5-P2] - 128 - 10 with 40% dropout. \n",
    "        \n",
    "        super(StochasticCNN, self).__init__()\n",
    "        # define here\n",
    "        self.conv1 = Conv2D(filters = 32, kernel_size = (5, 5), input_shape = input_shape, activation = 'sigmoid')\n",
    "        self.conv2 = Conv2D(filters = 64, kernel_size = (5, 5), activation = 'sigmoid')\n",
    "        self.max1 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.max2 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.dropout = Dropout((0.4))\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(128) \n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        #--- first conv + max layer ---\n",
    "        net = self.conv1(x)\n",
    "        net = tfp.distributions.Bernoulli(logits=net).sample()\n",
    "        \n",
    "        net0_flat = tf.reshape(net, [np.shape(net)[0], -1])\n",
    "        self.net0_flat_shape = np.shape(net0_flat)\n",
    "        \n",
    "        network.append(net)  \n",
    "        net = tf.cast(net, dtype=tf.float32)\n",
    "        net = self.max1(net)\n",
    "\n",
    "        \n",
    "        #--- second conv + max layer ---\n",
    "        net = self.conv2(net)\n",
    "        net = tfp.distributions.Bernoulli(logits=net).sample()\n",
    "        \n",
    "        net1_flat = tf.reshape(net, [np.shape(net)[0], -1])\n",
    "        self.net1_flat_shape = np.shape(net1_flat)\n",
    "        \n",
    "        network.append(net)  \n",
    "        net = tf.cast(net, dtype=tf.float32)\n",
    "        net = self.max2(net)\n",
    "        \n",
    "        #--- dropout and flatten layer ---\n",
    "        net = self.dropout(net)\n",
    "        net = self.flatten(net)\n",
    "        \n",
    "        #--- dense layer ---\n",
    "        net = self.fc1(net)\n",
    "        net = tfp.distributions.Bernoulli(logits=net).sample()\n",
    "        self.net2_flat_shape = np.shape(net)\n",
    "        \n",
    "        network.append(net)\n",
    "        \n",
    "        #--- output layer ---\n",
    "        net = self.output_layer(net)\n",
    "       \n",
    "        # define the shape of network\n",
    "        \n",
    "        self.net0_shape = np.shape(network[0])[1:]\n",
    "        self.net1_shape = np.shape(network[1])[1:]\n",
    "        self.net2_shape = np.shape(network[2])[1:]\n",
    "     \n",
    "        return network\n",
    "    \n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        y = [[i] for i in y]\n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        \n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, [self.conv1, self.conv2, self.fc1])):\n",
    "            \n",
    "            if i == 0:\n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=cv, logits=layer(pv))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "                \n",
    "            if i == 1:\n",
    "                \n",
    "                max1 = self.max1(pv)\n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=cv, logits=layer(max1))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "        \n",
    "            if i == 2:\n",
    "                \n",
    "                max2 = self.max2(pv)\n",
    "                dropout = self.dropout(max2)\n",
    "                flat = self.flatten(dropout)\n",
    "                \n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=cv, logits=layer(flat))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        \n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)   \n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def target_log_prob2(self, x, h, y):\n",
    "        \n",
    "        #x = Flatten()(x)\n",
    "        y = [[i] for i in y]\n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        \n",
    "        # self.net1_flat_shape = [32,24*24*32]\n",
    "        # self.net2_flat_shape = [32,8*8*64]\n",
    "        # self.net1_flat_shape = [32,128]\n",
    "        \n",
    "        \n",
    "        split0, split1, split2 = tf.split(h_current, [self.net0_flat_shape[1], \\\n",
    "                                                      self.net1_flat_shape[1], self.net2_flat_shape[1]], 1)\n",
    "        batch_size = np.shape(split0)[0]\n",
    "        \n",
    "        \n",
    "        s0_l = self.net0_shape.as_list()\n",
    "        s0_l.insert(0, batch_size)\n",
    "        \n",
    "        s1_l = self.net1_shape.as_list()\n",
    "        s1_l.insert(0, batch_size)\n",
    "        \n",
    "        s2_l = self.net2_shape.as_list()\n",
    "        s2_l.insert(0, batch_size)\n",
    "\n",
    "        \n",
    "        split0 = tf.reshape(split0, s0_l)\n",
    "        split1 = tf.reshape(split1, s1_l)\n",
    "        split2 = tf.reshape(split2, s2_l)\n",
    "        \n",
    "        h_current = [split0, split1, split2] \n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        nlog_prob = 0.\n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, [self.conv1, self.conv2, self.fc1])):\n",
    "            \n",
    "            if i == 0:\n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=cv, logits=layer(pv))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "                \n",
    "            if i == 1:\n",
    "                \n",
    "                max1 = self.max1(pv)\n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=cv, logits=layer(max1))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = (1,2,3))\n",
    "        \n",
    "            if i == 2:\n",
    "                \n",
    "                max2 = self.max2(pv)\n",
    "                dropout = self.dropout(max2)\n",
    "                flat = self.flatten(dropout)\n",
    "                \n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=cv, logits=layer(flat))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        \n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)   \n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_ker, update_ker = False):\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        orig_shape = np.shape(h_current)\n",
    "        h_current = tf.reshape(h_current, [orig_shape[1], -1]) # reshape to one dimension\n",
    "   \n",
    "        # initialize the HMC transition kernel\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = pow(1000, -1/4)),\n",
    "            num_adaptation_steps=int(100*0.8))\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 100\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps, \n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = adaptive_hmc,\n",
    "            trace_fn = None)\n",
    "        \n",
    "        h_new = tf.reshape(samples[0], orig_shape)\n",
    "\n",
    "        return(h_new)\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    def get_predictions(self, x):\n",
    "\n",
    "     \n",
    "        logits = self.conv1(x)\n",
    "        x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.max1(x)\n",
    "        x = tf.math.sigmoid(logits)\n",
    "            \n",
    "        logits = self.conv2(x)\n",
    "        x = tf.math.sigmoid(logits)\n",
    "            \n",
    "        logits = self.max2(x)\n",
    "        x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.dropout(x)\n",
    "        x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.flatten(x)\n",
    "        x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.fc1(x)\n",
    "        x = tf.math.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tf.math.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tf.math.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticCNN(n_outputs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlp = [model.target_log_prob(images, network[bs], labels) for bs, (images, labels) in enumerate(train_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py:74: UserWarning: TensorFloat-32 matmul/conv are enabled for NVIDIA Ampere+ GPUs. The resulting loss of precision may hinder MCMC convergence. To turn off, run `tf.config.experimental.enable_tensor_float_32_execution(False)`. For more detail, see https://github.com/tensorflow/community/pull/287.\n",
      "  'TensorFloat-32 matmul/conv are enabled for NVIDIA Ampere+ GPUs. The '\n"
     ]
    }
   ],
   "source": [
    "kernels = [model.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Step 2 - time 157.3531\n",
      "1\n",
      "Step 2 - time 154.8043\n",
      "2\n",
      "Step 2 - time 151.8502\n",
      "3\n",
      "Step 2 - time 153.1443\n",
      "4\n",
      "Step 2 - time 154.6346\n",
      "5\n",
      "Step 2 - time 394.6344\n",
      "6\n",
      "Step 2 - time 155.5563\n",
      "7\n",
      "Step 2 - time 153.8152\n",
      "8\n",
      "Step 2 - time 153.8025\n",
      "9\n",
      "Step 2 - time 157.5750\n",
      "10\n",
      "Step 2 - time 740.5149\n",
      "11\n",
      "Step 2 - time 153.9175\n",
      "12\n",
      "Step 2 - time 153.5590\n",
      "13\n",
      "Step 2 - time 153.7977\n",
      "14\n",
      "Step 2 - time 1643.7689\n",
      "15\n",
      "Step 2 - time 155.2559\n",
      "16\n",
      "Step 2 - time 151.2132\n",
      "17\n",
      "Step 2 - time 153.5892\n",
      "18\n",
      "Step 2 - time 152.5355\n",
      "19\n",
      "Step 2 - time 5340.0578\n"
     ]
    }
   ],
   "source": [
    "burnin = 20\n",
    "step_sizes = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    print(i)\n",
    "    network_new = []\n",
    "    kernels_new = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network, kernels):\n",
    "        net_current = net      \n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        \n",
    "        # flatten the 3-layer network\n",
    "        for i in range(3):\n",
    "            orig_shape = np.shape(net_current[i])\n",
    "            net_current[i] = tf.reshape(net_current[i], [orig_shape[0], -1])\n",
    "            \n",
    "        net_current = tf.concat(net_current, axis = 1)\n",
    "                \n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2].new_step_size.numpy()\n",
    "        step_sizes.append(new_step_size)\n",
    "    \n",
    "        new_state = rerange(samples[0][0])\n",
    "        \n",
    "        # reshape new network\n",
    "        split0, split1, split2 = tf.split(new_state, [18432, 4096, 128], 1)\n",
    "        batch_size = np.shape(new_state)[0]\n",
    "        split0 = tf.reshape(split0, [batch_size, 24, 24, 32])\n",
    "        split1 = tf.reshape(split1, [batch_size, 8, 8, 64])\n",
    "        split2 = tf.reshape(split2, [batch_size, 128])\n",
    "        \n",
    "        net_new = [split0, split1, split2] \n",
    "        network_new.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new.append(ker_new)\n",
    "            \n",
    "    network = network_new\n",
    "    kernels = kernels_new\n",
    "    \n",
    "    print(\"Step %d - time %.4f\" % (i, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes)), step_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(images, network[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #              zip(train_ds, network)]\n",
    "        \n",
    "        network_new = []\n",
    "        #kernels_new = []\n",
    "        for net, hmc_kernel in zip(network, kernels):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            \n",
    "            # flatten the 3-layer network\n",
    "            for i in range(3):\n",
    "                orig_shape = np.shape(net_current[i])\n",
    "                net_current[i] = tf.reshape(net_current[i], [orig_shape[0], -1])\n",
    "            \n",
    "            net_current = tf.concat(net_current, axis = 1)\n",
    "            \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "\n",
    "            #new_step_size = samples[2].new_step_size.numpy()\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            # reshape new network\n",
    "            split0, split1, split2 = tf.split(new_state, [18432, 4096, 128], 1)\n",
    "            batch_size = np.shape(new_state)[0]\n",
    "            split0 = tf.reshape(split0, [batch_size, 24, 24, 32])\n",
    "            split1 = tf.reshape(split1, [batch_size, 8, 8, 64])\n",
    "            split2 = tf.reshape(split2, [batch_size, 128])\n",
    "        \n",
    "            net_new = [split0, split1, split2] \n",
    "            network_new.append(net_new)\n",
    "            \n",
    "            #ker_new = model.generate_hmc_kernel(images2, labels2, new_step_size)\n",
    "            #kernels_new.append(ker_new)\n",
    "            \n",
    "        network = network_new\n",
    "        #kernels = kernels_new\n",
    "\n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(images, network[bs], labels))\n",
    "       \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    #print(preds)\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
