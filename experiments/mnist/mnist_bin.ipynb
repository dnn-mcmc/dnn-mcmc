{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8091cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f775a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]    \n",
    "    return t\n",
    "\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4dbbf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10, lr=1e-3):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y, is_gibbs = False, is_hmc = False, is_loss = False):\n",
    "        \n",
    "        # get current state\n",
    "        if is_hmc:\n",
    "            h_current = tf.split(h, self.hidden_layer_sizes, axis = 1)\n",
    "        else:    \n",
    "            h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_current = convert2_zero_one(h_current)\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        if not is_loss:\n",
    "            for i, (cv, pv, layer) in enumerate(zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "                logits = layer(pv)\n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(labels = cv, logits = logits)\n",
    "                if not is_gibbs:\n",
    "                    ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def gibbs_new_state(self, x, h, y):\n",
    "        \n",
    "        '''\n",
    "            generate a new state for the network node by node in Gibbs setting.\n",
    "        '''\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "\n",
    "            # node by node\n",
    "            \n",
    "            nodes = tf.transpose(cv)\n",
    "            prob_parents = tm.sigmoid(in_layer(pv))\n",
    "            \n",
    "            out_layer_weights = out_layer.get_weights()[0]\n",
    "            \n",
    "            next_logits = out_layer(cv)\n",
    "            \n",
    "            new_layer = []\n",
    "            \n",
    "            for j, node in enumerate(nodes):\n",
    "                \n",
    "                # get info for current node (i, j)\n",
    "                \n",
    "                prob_parents_j = prob_parents[:, j]\n",
    "                out_layer_weights_j = out_layer_weights[j]\n",
    "                \n",
    "                # calculate logits and logprob for node is 0 or 1\n",
    "                next_logits_if_node_0 = next_logits[:, :] - node[:, None] * out_layer_weights_j[None, :]\n",
    "                next_logits_if_node_1 = next_logits[:, :] + (1 - node[:, None]) * out_layer_weights_j[None, :]\n",
    "                \n",
    "                #print(next_logits_if_node_0, next_logits_if_node_1)\n",
    "                \n",
    "                logprob_children_if_node_0 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_0), axis = -1)\n",
    "                \n",
    "                logprob_children_if_node_1 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_1), axis = -1)\n",
    "                \n",
    "                # calculate prob for node (i, j)\n",
    "                prob_0 = (1 - prob_parents_j) * tm.exp(logprob_children_if_node_0)\n",
    "                prob_1 = prob_parents_j * tm.exp(logprob_children_if_node_1)\n",
    "                prob_j = prob_1 / (prob_1 + prob_0)\n",
    "            \n",
    "                # sample new state with prob_j for node (i, j)\n",
    "                new_node = tfp.distributions.Bernoulli(probs = prob_j).sample() # MAY BE SLOW\n",
    "                \n",
    "                # update nodes and logits for following calculation\n",
    "                new_node_casted = tf.cast(new_node, dtype = \"float32\")\n",
    "                next_logits = next_logits_if_node_0 * (1 - new_node_casted)[:, None] \\\n",
    "                            + next_logits_if_node_1 * new_node_casted[:, None] \n",
    "                \n",
    "                # keep track of new node values (in prev/curr/next_vals and h_new)\n",
    "                new_layer.append(new_node)\n",
    "           \n",
    "            new_layer = tf.transpose(new_layer)\n",
    "            h_current[i] = new_layer\n",
    "            prev_vals = [x] + h_current[:-1]\n",
    "            curr_vals = h_current\n",
    "            next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        return h_current\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob(x, v, y, is_hmc = True),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_kernel, is_update_kernel = True):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat(h_current, axis = 1)\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_burnin_steps = 0\n",
    "        num_results = 1\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "    \n",
    "        # Generate new states of chains\n",
    "        #h_state = rerange(samples[0][0])\n",
    "        h_state = samples[0][0]\n",
    "        h_new = tf.split(h_state, self.hidden_layer_sizes, axis = 1) \n",
    "        \n",
    "        # Update the kernel if necesssary\n",
    "        if is_update_kernel:\n",
    "            new_step_size = samples[2].new_step_size.numpy()\n",
    "            ker_new = self.generate_hmc_kernel(x, y, new_step_size)\n",
    "            return(h_new, ker_new)\n",
    "        else:\n",
    "            return h_new\n",
    "    \n",
    "    def update_weights(self, x, h, y, is_gibbs = False):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y, is_gibbs = is_gibbs))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, tf.float32), logits = logits)\n",
    "\n",
    "        return tf.reduce_sum(loss, axis = -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab3525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_backprop(size, dat_train, dat_val, epochs):\n",
    "    '''\n",
    "    Standard Backpropogation training\n",
    "    '''\n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    print(\"Start Standard Backprop\")\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(784,)),\n",
    "            layers.Dense(size, activation = \"sigmoid\"),\n",
    "            layers.Dense(1, activation = \"sigmoid\")\n",
    "        ]\n",
    "    )   \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "    st = time.time()\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "    history = model.fit(dat_train, batch_size = batch_size, epochs = epochs, validation_data = dat_val)\n",
    "    train_time = time.time() - st\n",
    "    \n",
    "    return train_time, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd364ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(size, dat_train, dat_val, epochs, burnin = 500):\n",
    "    '''\n",
    "    HMC training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    target_val = np.concatenate([target for data, target in dat_val.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start HMC\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs = 1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    kernels = [model.generate_hmc_kernel(data, target) for data, target in dat_train]  \n",
    "    \n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start HMC Burning\")\n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "        \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.propose_new_state_hamiltonian(data, network[bs], target, kernels[bs]))\n",
    "            burnin_loss += -1 * tf.reduce_sum(model.target_log_prob(data, network[bs], target))\n",
    "    \n",
    "        network, kernels = zip(*res)\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    print(network[0])\n",
    "    \n",
    "    # Training\n",
    "    print(\"Start HMC Training\")\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target)\n",
    "            network = [model.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel = False) \\\n",
    "                       for (x, y), net, ker in zip(dat_train, network, kernels)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        #print(train_loss)\n",
    "        train_loss /= (bs + 1)\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc)        \n",
    "        \n",
    "        # validate\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_val):\n",
    "            val_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        val_loss /= (bs + 1)\n",
    "        val_losses.append(val_loss)  \n",
    "        \n",
    "        val_preds = [model.get_predictions(data) for data, target in dat_val]\n",
    "        val_acc = accuracy_score(np.concatenate(val_preds), target_val)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f - val_loss: %.4f - val_acc: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return network, burnin_losses, train_time, {\"train_acc\": train_accs, \"train_loss\": train_losses,\n",
    "                             \"val_acc\": val_accs, \"val_loss\": val_losses}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd65d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(size, dat_train, dat_val, epochs, burnin = 500):\n",
    "    '''\n",
    "    Gibbs Training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    target_val = np.concatenate([target for data, target in dat_val.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start Gibbs\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs = 1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start Gibbs Burning\")    \n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "    \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.gibbs_new_state(data, network[bs], target))\n",
    "            burnin_loss += -1 * tf.reduce_sum(model.target_log_prob(data, network[bs], target, is_gibbs = True))\n",
    "            \n",
    "        network = res\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    # Training\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target, is_gibbs = True)\n",
    "            network = [model.gibbs_new_state(x, net, y) for (x, y), net in zip(dat_train, network)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        train_loss /= (bs + 1)\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc)        \n",
    "        \n",
    "        # validate\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_val):\n",
    "            val_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        val_loss /= (bs + 1)\n",
    "        val_losses.append(val_loss)  \n",
    "        \n",
    "        val_preds = [model.get_predictions(data) for data, target in dat_val]\n",
    "        val_acc = accuracy_score(np.concatenate(val_preds), target_val)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f - val_loss: %.4f - val_acc: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return network, burnin_losses, train_time, {\"train_acc\": train_accs, \"train_loss\": train_losses,\n",
    "                             \"val_acc\": val_accs, \"val_loss\": val_losses}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b775a293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12665 training images.\n",
      "There are 2115 validation images.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "\n",
    "# Select binary data\n",
    "label_bin = [0,1]\n",
    "x_train_bin = [x.reshape(-1) for x, y in zip(x_train, y_train) if y in label_bin]\n",
    "y_train_bin = [y.reshape(-1) for y in y_train if y in label_bin]\n",
    "x_val_bin = [x.reshape(-1) for x, y in zip(x_val, y_val) if y in label_bin]\n",
    "y_val_bin = [y.reshape(-1) for y in y_val if y in label_bin]\n",
    "\n",
    "print('There are', len(x_train_bin), 'training images.')\n",
    "print('There are', len(x_val_bin), 'validation images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753f30fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 23:58:51.886291: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "x_train_sub = x_train_bin[:32]\n",
    "y_train_sub = y_train_bin[:32]\n",
    "x_val_sub = x_val_bin[:32]\n",
    "y_val_sub = y_val_bin[:32]\n",
    "\n",
    "x_train_sub = [(x - 127.5) / 255 for x in x_train_sub]\n",
    "x_val_sub = [(x - 127.5) / 255 for x in x_val_sub]\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_sub, y_train_sub)).batch(32)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val_sub, y_val_sub)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb310d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Standard Backprop\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7616 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 23:59:03.616083: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 456ms/step - loss: 0.7616 - accuracy: 0.5625 - val_loss: 0.6849 - val_accuracy: 0.6250\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7539 - accuracy: 0.5625 - val_loss: 0.6800 - val_accuracy: 0.6250\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7466 - accuracy: 0.5625 - val_loss: 0.6754 - val_accuracy: 0.6250\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7396 - accuracy: 0.5625 - val_loss: 0.6710 - val_accuracy: 0.6250\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7329 - accuracy: 0.5625 - val_loss: 0.6668 - val_accuracy: 0.6250\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7265 - accuracy: 0.5625 - val_loss: 0.6628 - val_accuracy: 0.6250\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7203 - accuracy: 0.5625 - val_loss: 0.6589 - val_accuracy: 0.6250\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7144 - accuracy: 0.5625 - val_loss: 0.6551 - val_accuracy: 0.6250\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7088 - accuracy: 0.5625 - val_loss: 0.6515 - val_accuracy: 0.6250\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7033 - accuracy: 0.5625 - val_loss: 0.6480 - val_accuracy: 0.6250\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6981 - accuracy: 0.5625 - val_loss: 0.6446 - val_accuracy: 0.6250\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6930 - accuracy: 0.5625 - val_loss: 0.6413 - val_accuracy: 0.6250\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6880 - accuracy: 0.5625 - val_loss: 0.6380 - val_accuracy: 0.6250\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6833 - accuracy: 0.5625 - val_loss: 0.6349 - val_accuracy: 0.6250\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6786 - accuracy: 0.5625 - val_loss: 0.6318 - val_accuracy: 0.6250\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6741 - accuracy: 0.5625 - val_loss: 0.6287 - val_accuracy: 0.6250\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6697 - accuracy: 0.5625 - val_loss: 0.6257 - val_accuracy: 0.6250\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6654 - accuracy: 0.5625 - val_loss: 0.6227 - val_accuracy: 0.6250\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6612 - accuracy: 0.5625 - val_loss: 0.6198 - val_accuracy: 0.6250\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6571 - accuracy: 0.5625 - val_loss: 0.6169 - val_accuracy: 0.6250\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6531 - accuracy: 0.5625 - val_loss: 0.6141 - val_accuracy: 0.6250\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6491 - accuracy: 0.5625 - val_loss: 0.6112 - val_accuracy: 0.6250\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6453 - accuracy: 0.5625 - val_loss: 0.6084 - val_accuracy: 0.6250\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6415 - accuracy: 0.5625 - val_loss: 0.6057 - val_accuracy: 0.6250\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6377 - accuracy: 0.5625 - val_loss: 0.6029 - val_accuracy: 0.6250\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6340 - accuracy: 0.5625 - val_loss: 0.6002 - val_accuracy: 0.6250\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6304 - accuracy: 0.5625 - val_loss: 0.5975 - val_accuracy: 0.6250\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6268 - accuracy: 0.5625 - val_loss: 0.5948 - val_accuracy: 0.6250\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6233 - accuracy: 0.5625 - val_loss: 0.5921 - val_accuracy: 0.6250\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6198 - accuracy: 0.5625 - val_loss: 0.5894 - val_accuracy: 0.6250\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6164 - accuracy: 0.5625 - val_loss: 0.5867 - val_accuracy: 0.6250\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6130 - accuracy: 0.5625 - val_loss: 0.5841 - val_accuracy: 0.6250\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6096 - accuracy: 0.5938 - val_loss: 0.5814 - val_accuracy: 0.6250\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6063 - accuracy: 0.5938 - val_loss: 0.5788 - val_accuracy: 0.6250\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6030 - accuracy: 0.5938 - val_loss: 0.5762 - val_accuracy: 0.6562\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5997 - accuracy: 0.5938 - val_loss: 0.5736 - val_accuracy: 0.6562\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5965 - accuracy: 0.5938 - val_loss: 0.5710 - val_accuracy: 0.6562\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5933 - accuracy: 0.5938 - val_loss: 0.5684 - val_accuracy: 0.6562\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5901 - accuracy: 0.5938 - val_loss: 0.5659 - val_accuracy: 0.6562\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5870 - accuracy: 0.5938 - val_loss: 0.5633 - val_accuracy: 0.6562\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5839 - accuracy: 0.5938 - val_loss: 0.5607 - val_accuracy: 0.6562\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5808 - accuracy: 0.6562 - val_loss: 0.5582 - val_accuracy: 0.6562\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5777 - accuracy: 0.6562 - val_loss: 0.5557 - val_accuracy: 0.6562\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5747 - accuracy: 0.6562 - val_loss: 0.5531 - val_accuracy: 0.6562\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5716 - accuracy: 0.6562 - val_loss: 0.5506 - val_accuracy: 0.7188\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5687 - accuracy: 0.6562 - val_loss: 0.5481 - val_accuracy: 0.7188\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5657 - accuracy: 0.6562 - val_loss: 0.5456 - val_accuracy: 0.7188\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5627 - accuracy: 0.6562 - val_loss: 0.5431 - val_accuracy: 0.7188\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5598 - accuracy: 0.6875 - val_loss: 0.5407 - val_accuracy: 0.7188\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5569 - accuracy: 0.6875 - val_loss: 0.5382 - val_accuracy: 0.7188\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "[<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
      "array([[-0.24138567,  0.544178  ,  3.3020082 , ..., -0.94257164,\n",
      "        -0.36821908, -0.12111327],\n",
      "       [ 2.9746544 , -1.5964426 , -1.256089  , ...,  2.5058753 ,\n",
      "        -0.17157307,  0.0446507 ],\n",
      "       [ 3.3083377 , -3.4334152 , -0.7278921 , ...,  3.3040092 ,\n",
      "        -4.0737658 , -0.9095729 ],\n",
      "       ...,\n",
      "       [-4.7526803 ,  1.2149917 , -0.17058396, ...,  0.06281587,\n",
      "         1.1722705 ,  0.49800792],\n",
      "       [ 2.941402  , -0.2612195 ,  2.8431423 , ..., -1.096411  ,\n",
      "        -0.11681882, -1.7000206 ],\n",
      "       [-0.6974861 ,  4.682668  ,  2.15629   , ...,  0.01689705,\n",
      "        -0.16595903, -0.39314413]], dtype=float32)>]\n",
      "Start HMC Training\n",
      "Epoch 1/50: - 0.0614s/step - train_loss: 0.6879 - train_acc: 0.5625 - val_loss: 0.6947 - val_acc: 0.6250\n",
      "Epoch 2/50: - 0.0591s/step - train_loss: 0.6876 - train_acc: 0.5625 - val_loss: 0.6926 - val_acc: 0.6250\n",
      "Epoch 3/50: - 0.0586s/step - train_loss: 0.6877 - train_acc: 0.5625 - val_loss: 0.6926 - val_acc: 0.6250\n",
      "Epoch 4/50: - 0.0589s/step - train_loss: 0.6878 - train_acc: 0.5625 - val_loss: 0.6929 - val_acc: 0.6250\n",
      "Epoch 5/50: - 0.0588s/step - train_loss: 0.6882 - train_acc: 0.5625 - val_loss: 0.6938 - val_acc: 0.6250\n",
      "Epoch 6/50: - 0.0572s/step - train_loss: 0.6884 - train_acc: 0.5625 - val_loss: 0.6947 - val_acc: 0.6250\n",
      "Epoch 7/50: - 0.0565s/step - train_loss: 0.6886 - train_acc: 0.5625 - val_loss: 0.6956 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: - 0.0558s/step - train_loss: 0.6881 - train_acc: 0.5625 - val_loss: 0.6954 - val_acc: 0.6250\n",
      "Epoch 9/50: - 0.0559s/step - train_loss: 0.6872 - train_acc: 0.5625 - val_loss: 0.6946 - val_acc: 0.6250\n",
      "Epoch 10/50: - 0.0561s/step - train_loss: 0.6866 - train_acc: 0.5625 - val_loss: 0.6942 - val_acc: 0.6250\n",
      "Epoch 11/50: - 0.0559s/step - train_loss: 0.6856 - train_acc: 0.5625 - val_loss: 0.6931 - val_acc: 0.6250\n",
      "Epoch 12/50: - 0.0558s/step - train_loss: 0.6852 - train_acc: 0.5625 - val_loss: 0.6929 - val_acc: 0.6250\n",
      "Epoch 13/50: - 0.0559s/step - train_loss: 0.6850 - train_acc: 0.5625 - val_loss: 0.6930 - val_acc: 0.6250\n",
      "Epoch 14/50: - 0.0554s/step - train_loss: 0.6848 - train_acc: 0.5625 - val_loss: 0.6929 - val_acc: 0.6250\n",
      "Epoch 15/50: - 0.0552s/step - train_loss: 0.6845 - train_acc: 0.5625 - val_loss: 0.6931 - val_acc: 0.6250\n",
      "Epoch 16/50: - 0.0553s/step - train_loss: 0.6843 - train_acc: 0.5625 - val_loss: 0.6935 - val_acc: 0.6250\n",
      "Epoch 17/50: - 0.0555s/step - train_loss: 0.6836 - train_acc: 0.5625 - val_loss: 0.6934 - val_acc: 0.6250\n",
      "Epoch 18/50: - 0.0553s/step - train_loss: 0.6833 - train_acc: 0.5625 - val_loss: 0.6933 - val_acc: 0.6250\n",
      "Epoch 19/50: - 0.0551s/step - train_loss: 0.6829 - train_acc: 0.5625 - val_loss: 0.6927 - val_acc: 0.6250\n",
      "Epoch 20/50: - 0.0547s/step - train_loss: 0.6822 - train_acc: 0.5625 - val_loss: 0.6916 - val_acc: 0.6250\n",
      "Epoch 21/50: - 0.0546s/step - train_loss: 0.6813 - train_acc: 0.5625 - val_loss: 0.6906 - val_acc: 0.6250\n",
      "Epoch 22/50: - 0.0543s/step - train_loss: 0.6805 - train_acc: 0.5625 - val_loss: 0.6899 - val_acc: 0.6250\n",
      "Epoch 23/50: - 0.0541s/step - train_loss: 0.6796 - train_acc: 0.5625 - val_loss: 0.6892 - val_acc: 0.6250\n",
      "Epoch 24/50: - 0.0538s/step - train_loss: 0.6788 - train_acc: 0.5625 - val_loss: 0.6883 - val_acc: 0.6250\n",
      "Epoch 25/50: - 0.0537s/step - train_loss: 0.6778 - train_acc: 0.5625 - val_loss: 0.6870 - val_acc: 0.6250\n",
      "Epoch 26/50: - 0.0535s/step - train_loss: 0.6770 - train_acc: 0.5625 - val_loss: 0.6859 - val_acc: 0.6250\n",
      "Epoch 27/50: - 0.0533s/step - train_loss: 0.6768 - train_acc: 0.5625 - val_loss: 0.6859 - val_acc: 0.6250\n",
      "Epoch 28/50: - 0.0531s/step - train_loss: 0.6763 - train_acc: 0.5625 - val_loss: 0.6856 - val_acc: 0.6250\n",
      "Epoch 29/50: - 0.0529s/step - train_loss: 0.6756 - train_acc: 0.5625 - val_loss: 0.6848 - val_acc: 0.6250\n",
      "Epoch 30/50: - 0.0528s/step - train_loss: 0.6751 - train_acc: 0.5625 - val_loss: 0.6841 - val_acc: 0.6250\n",
      "Epoch 31/50: - 0.0526s/step - train_loss: 0.6750 - train_acc: 0.5625 - val_loss: 0.6841 - val_acc: 0.6250\n",
      "Epoch 32/50: - 0.0524s/step - train_loss: 0.6750 - train_acc: 0.5625 - val_loss: 0.6841 - val_acc: 0.6250\n",
      "Epoch 33/50: - 0.0523s/step - train_loss: 0.6746 - train_acc: 0.5625 - val_loss: 0.6835 - val_acc: 0.6250\n",
      "Epoch 34/50: - 0.0521s/step - train_loss: 0.6744 - train_acc: 0.5625 - val_loss: 0.6833 - val_acc: 0.6250\n",
      "Epoch 35/50: - 0.0521s/step - train_loss: 0.6743 - train_acc: 0.5625 - val_loss: 0.6832 - val_acc: 0.6250\n",
      "Epoch 36/50: - 0.0520s/step - train_loss: 0.6744 - train_acc: 0.5625 - val_loss: 0.6836 - val_acc: 0.6250\n",
      "Epoch 37/50: - 0.0519s/step - train_loss: 0.6743 - train_acc: 0.5625 - val_loss: 0.6837 - val_acc: 0.6250\n",
      "Epoch 38/50: - 0.0517s/step - train_loss: 0.6738 - train_acc: 0.5625 - val_loss: 0.6834 - val_acc: 0.6250\n",
      "Epoch 39/50: - 0.0516s/step - train_loss: 0.6731 - train_acc: 0.5625 - val_loss: 0.6829 - val_acc: 0.6250\n",
      "Epoch 40/50: - 0.0516s/step - train_loss: 0.6722 - train_acc: 0.5625 - val_loss: 0.6823 - val_acc: 0.6250\n",
      "Epoch 41/50: - 0.0515s/step - train_loss: 0.6711 - train_acc: 0.5625 - val_loss: 0.6816 - val_acc: 0.6250\n",
      "Epoch 42/50: - 0.0514s/step - train_loss: 0.6706 - train_acc: 0.5625 - val_loss: 0.6812 - val_acc: 0.6250\n",
      "Epoch 43/50: - 0.0514s/step - train_loss: 0.6694 - train_acc: 0.5625 - val_loss: 0.6804 - val_acc: 0.6250\n",
      "Epoch 44/50: - 0.0513s/step - train_loss: 0.6686 - train_acc: 0.5625 - val_loss: 0.6798 - val_acc: 0.6250\n",
      "Epoch 45/50: - 0.0513s/step - train_loss: 0.6674 - train_acc: 0.5625 - val_loss: 0.6788 - val_acc: 0.6250\n",
      "Epoch 46/50: - 0.0512s/step - train_loss: 0.6659 - train_acc: 0.5625 - val_loss: 0.6777 - val_acc: 0.6250\n",
      "Epoch 47/50: - 0.0511s/step - train_loss: 0.6644 - train_acc: 0.5625 - val_loss: 0.6759 - val_acc: 0.6250\n",
      "Epoch 48/50: - 0.0511s/step - train_loss: 0.6626 - train_acc: 0.5625 - val_loss: 0.6740 - val_acc: 0.6250\n",
      "Epoch 49/50: - 0.0510s/step - train_loss: 0.6611 - train_acc: 0.5625 - val_loss: 0.6723 - val_acc: 0.6250\n",
      "Epoch 50/50: - 0.0510s/step - train_loss: 0.6601 - train_acc: 0.5625 - val_loss: 0.6717 - val_acc: 0.6250\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/50: - 0.1130s/step - train_loss: 0.6623 - train_acc: 0.4375 - val_loss: 0.6712 - val_acc: 0.3750\n",
      "Epoch 2/50: - 0.1107s/step - train_loss: 0.6619 - train_acc: 0.4375 - val_loss: 0.6705 - val_acc: 0.4062\n",
      "Epoch 3/50: - 0.1090s/step - train_loss: 0.6632 - train_acc: 0.4375 - val_loss: 0.6715 - val_acc: 0.4062\n",
      "Epoch 4/50: - 0.1094s/step - train_loss: 0.6630 - train_acc: 0.4375 - val_loss: 0.6707 - val_acc: 0.4062\n",
      "Epoch 5/50: - 0.1104s/step - train_loss: 0.6627 - train_acc: 0.4688 - val_loss: 0.6695 - val_acc: 0.4375\n",
      "Epoch 6/50: - 0.1108s/step - train_loss: 0.6608 - train_acc: 0.5000 - val_loss: 0.6662 - val_acc: 0.5000\n",
      "Epoch 7/50: - 0.1104s/step - train_loss: 0.6606 - train_acc: 0.5312 - val_loss: 0.6653 - val_acc: 0.5000\n",
      "Epoch 8/50: - 0.1101s/step - train_loss: 0.6607 - train_acc: 0.5312 - val_loss: 0.6646 - val_acc: 0.5000\n",
      "Epoch 9/50: - 0.1106s/step - train_loss: 0.6619 - train_acc: 0.5938 - val_loss: 0.6650 - val_acc: 0.5312\n",
      "Epoch 10/50: - 0.1107s/step - train_loss: 0.6604 - train_acc: 0.6875 - val_loss: 0.6622 - val_acc: 0.6562\n",
      "Epoch 11/50: - 0.1103s/step - train_loss: 0.6599 - train_acc: 0.7188 - val_loss: 0.6605 - val_acc: 0.6875\n",
      "Epoch 12/50: - 0.1105s/step - train_loss: 0.6594 - train_acc: 0.7500 - val_loss: 0.6593 - val_acc: 0.7500\n",
      "Epoch 13/50: - 0.1110s/step - train_loss: 0.6599 - train_acc: 0.7500 - val_loss: 0.6595 - val_acc: 0.7500\n",
      "Epoch 14/50: - 0.1110s/step - train_loss: 0.6597 - train_acc: 0.8125 - val_loss: 0.6584 - val_acc: 0.8125\n",
      "Epoch 15/50: - 0.1108s/step - train_loss: 0.6589 - train_acc: 0.8750 - val_loss: 0.6565 - val_acc: 0.8125\n",
      "Epoch 16/50: - 0.1105s/step - train_loss: 0.6583 - train_acc: 0.9062 - val_loss: 0.6548 - val_acc: 0.7812\n",
      "Epoch 17/50: - 0.1102s/step - train_loss: 0.6581 - train_acc: 0.9062 - val_loss: 0.6544 - val_acc: 0.8125\n",
      "Epoch 18/50: - 0.1101s/step - train_loss: 0.6587 - train_acc: 0.8750 - val_loss: 0.6544 - val_acc: 0.8438\n",
      "Epoch 19/50: - 0.1099s/step - train_loss: 0.6585 - train_acc: 0.8750 - val_loss: 0.6536 - val_acc: 0.8750\n",
      "Epoch 20/50: - 0.1098s/step - train_loss: 0.6595 - train_acc: 0.8438 - val_loss: 0.6547 - val_acc: 0.8438\n",
      "Epoch 21/50: - 0.1096s/step - train_loss: 0.6610 - train_acc: 0.8438 - val_loss: 0.6564 - val_acc: 0.7812\n",
      "Epoch 22/50: - 0.1096s/step - train_loss: 0.6593 - train_acc: 0.8750 - val_loss: 0.6535 - val_acc: 0.8750\n",
      "Epoch 23/50: - 0.1094s/step - train_loss: 0.6583 - train_acc: 0.8438 - val_loss: 0.6518 - val_acc: 0.9062\n",
      "Epoch 24/50: - 0.1093s/step - train_loss: 0.6586 - train_acc: 0.8438 - val_loss: 0.6519 - val_acc: 0.8750\n",
      "Epoch 25/50: - 0.1091s/step - train_loss: 0.6593 - train_acc: 0.8438 - val_loss: 0.6533 - val_acc: 0.8750\n",
      "Epoch 26/50: - 0.1090s/step - train_loss: 0.6588 - train_acc: 0.8438 - val_loss: 0.6518 - val_acc: 0.8750\n",
      "Epoch 27/50: - 0.1088s/step - train_loss: 0.6578 - train_acc: 0.7812 - val_loss: 0.6497 - val_acc: 0.9062\n",
      "Epoch 28/50: - 0.1087s/step - train_loss: 0.6587 - train_acc: 0.8125 - val_loss: 0.6509 - val_acc: 0.9062\n",
      "Epoch 29/50: - 0.1086s/step - train_loss: 0.6580 - train_acc: 0.7812 - val_loss: 0.6495 - val_acc: 0.9062\n",
      "Epoch 30/50: - 0.1086s/step - train_loss: 0.6583 - train_acc: 0.7500 - val_loss: 0.6496 - val_acc: 0.8438\n",
      "Epoch 31/50: - 0.1085s/step - train_loss: 0.6571 - train_acc: 0.7188 - val_loss: 0.6474 - val_acc: 0.8438\n",
      "Epoch 32/50: - 0.1085s/step - train_loss: 0.6565 - train_acc: 0.7188 - val_loss: 0.6464 - val_acc: 0.8438\n",
      "Epoch 33/50: - 0.1084s/step - train_loss: 0.6569 - train_acc: 0.7188 - val_loss: 0.6468 - val_acc: 0.8438\n",
      "Epoch 34/50: - 0.1085s/step - train_loss: 0.6561 - train_acc: 0.6875 - val_loss: 0.6451 - val_acc: 0.8125\n",
      "Epoch 35/50: - 0.1084s/step - train_loss: 0.6562 - train_acc: 0.6562 - val_loss: 0.6448 - val_acc: 0.7500\n",
      "Epoch 36/50: - 0.1085s/step - train_loss: 0.6564 - train_acc: 0.6562 - val_loss: 0.6444 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: - 0.1087s/step - train_loss: 0.6565 - train_acc: 0.6250 - val_loss: 0.6440 - val_acc: 0.7500\n",
      "Epoch 38/50: - 0.1089s/step - train_loss: 0.6575 - train_acc: 0.6250 - val_loss: 0.6448 - val_acc: 0.7500\n",
      "Epoch 39/50: - 0.1088s/step - train_loss: 0.6570 - train_acc: 0.6250 - val_loss: 0.6442 - val_acc: 0.7500\n",
      "Epoch 40/50: - 0.1088s/step - train_loss: 0.6567 - train_acc: 0.6250 - val_loss: 0.6438 - val_acc: 0.7500\n",
      "Epoch 41/50: - 0.1088s/step - train_loss: 0.6571 - train_acc: 0.6250 - val_loss: 0.6437 - val_acc: 0.7500\n",
      "Epoch 42/50: - 0.1090s/step - train_loss: 0.6575 - train_acc: 0.6250 - val_loss: 0.6439 - val_acc: 0.7188\n",
      "Epoch 43/50: - 0.1091s/step - train_loss: 0.6581 - train_acc: 0.6250 - val_loss: 0.6450 - val_acc: 0.7500\n",
      "Epoch 44/50: - 0.1091s/step - train_loss: 0.6572 - train_acc: 0.6250 - val_loss: 0.6431 - val_acc: 0.7188\n",
      "Epoch 45/50: - 0.1090s/step - train_loss: 0.6565 - train_acc: 0.6250 - val_loss: 0.6421 - val_acc: 0.7188\n",
      "Epoch 46/50: - 0.1091s/step - train_loss: 0.6559 - train_acc: 0.6250 - val_loss: 0.6410 - val_acc: 0.6875\n",
      "Epoch 47/50: - 0.1092s/step - train_loss: 0.6571 - train_acc: 0.6250 - val_loss: 0.6427 - val_acc: 0.7188\n",
      "Epoch 48/50: - 0.1093s/step - train_loss: 0.6567 - train_acc: 0.6250 - val_loss: 0.6418 - val_acc: 0.6875\n",
      "Epoch 49/50: - 0.1093s/step - train_loss: 0.6572 - train_acc: 0.6250 - val_loss: 0.6421 - val_acc: 0.6875\n",
      "Epoch 50/50: - 0.1093s/step - train_loss: 0.6571 - train_acc: 0.5938 - val_loss: 0.6419 - val_acc: 0.6875\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "burnin = 20\n",
    "size = 32\n",
    "\n",
    "# only run once\n",
    "\n",
    "time_bp, history_bp = standard_backprop(size, train_ds, val_ds, epochs)\n",
    "net_hmc, burnin_loss_hmc, time_hmc, history_hmc = hmc(size, train_ds, val_ds, epochs, burnin)\n",
    "net_gibbs, burnin_loss_gibbs, time_gibbs, history_gibbs = gibbs(size, train_ds, val_ds, epochs, burnin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0d2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bp = {\"train_acc\": history_bp.history['accuracy'], \"train_loss\": history_bp.history['loss'], \n",
    "               \"val_acc\": history_bp.history['val_accuracy'], \"val_loss\": history_bp.history['val_loss']}\n",
    "res_bp = {'time': time_bp, 'history': hist_bp}\n",
    "res_hmc = {'time': time_hmc, 'burnin': burnin_loss_hmc, 'history': history_hmc, 'net': net_hmc}\n",
    "res_gibbs = {'time': time_gibbs, 'burnin': burnin_loss_gibbs, 'history': history_gibbs, 'net': net_gibbs}\n",
    "\n",
    "res_all = [res_bp, res_hmc, res_gibbs]\n",
    "\n",
    "with open('res_mnist_32.npy', 'wb') as f:\n",
    "    np.save(f, res_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68ef4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load('res_mnist_32.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b66123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4767d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(32, 32), dtype=int32, numpy=\n",
       "  array([[1, 1, 0, ..., 1, 1, 1],\n",
       "         [0, 1, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 0, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 1, 1, 1],\n",
       "         [1, 0, 0, ..., 0, 1, 1],\n",
       "         [1, 1, 1, ..., 1, 0, 1]], dtype=int32)>]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[2]['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b7e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
