{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb2baee",
   "metadata": {},
   "source": [
    "### Compare three methods(SGD, HMC, Gibbs) and 2 models(narrow, wider)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a537a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6b5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]    \n",
    "    return t\n",
    "\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb761182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10, lr=1e-3):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y, is_gibbs = False, is_hmc = False, is_loss = False):\n",
    "        \n",
    "        # get current state\n",
    "        if is_hmc:\n",
    "            h_current = tf.split(h, self.hidden_layer_sizes, axis = 1)\n",
    "        else:    \n",
    "            h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_current = convert2_zero_one(h_current)\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        if not is_loss:\n",
    "            for i, (cv, pv, layer) in enumerate(zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "                logits = layer(pv)\n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(labels = cv, logits = logits)\n",
    "                if not is_gibbs:\n",
    "                    ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def gibbs_new_state(self, x, h, y):\n",
    "        \n",
    "        '''\n",
    "            generate a new state for the network node by node in Gibbs setting.\n",
    "        '''\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "\n",
    "            # node by node\n",
    "            \n",
    "            nodes = tf.transpose(cv)\n",
    "            prob_parents = tm.sigmoid(in_layer(pv))\n",
    "            \n",
    "            out_layer_weights = out_layer.get_weights()[0]\n",
    "            \n",
    "            next_logits = out_layer(cv)\n",
    "            \n",
    "            new_layer = []\n",
    "            \n",
    "            for j, node in enumerate(nodes):\n",
    "                \n",
    "                # get info for current node (i, j)\n",
    "                \n",
    "                prob_parents_j = prob_parents[:, j]\n",
    "                out_layer_weights_j = out_layer_weights[j]\n",
    "                \n",
    "                # calculate logits and logprob for node is 0 or 1\n",
    "                next_logits_if_node_0 = next_logits[:, :] - node[:, None] * out_layer_weights_j[None, :]\n",
    "                next_logits_if_node_1 = next_logits[:, :] + (1 - node[:, None]) * out_layer_weights_j[None, :]\n",
    "                \n",
    "                #print(next_logits_if_node_0, next_logits_if_node_1)\n",
    "                \n",
    "                logprob_children_if_node_0 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_0), axis = -1)\n",
    "                \n",
    "                logprob_children_if_node_1 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_1), axis = -1)\n",
    "                \n",
    "                # calculate prob for node (i, j)\n",
    "                prob_0 = (1 - prob_parents_j) * tm.exp(logprob_children_if_node_0)\n",
    "                prob_1 = prob_parents_j * tm.exp(logprob_children_if_node_1)\n",
    "                prob_j = prob_1 / (prob_1 + prob_0)\n",
    "            \n",
    "                # sample new state with prob_j for node (i, j)\n",
    "                new_node = tfp.distributions.Bernoulli(probs = prob_j).sample() # MAY BE SLOW\n",
    "                \n",
    "                # update nodes and logits for following calculation\n",
    "                new_node_casted = tf.cast(new_node, dtype = \"float32\")\n",
    "                next_logits = next_logits_if_node_0 * (1 - new_node_casted)[:, None] \\\n",
    "                            + next_logits_if_node_1 * new_node_casted[:, None] \n",
    "                \n",
    "                # keep track of new node values (in prev/curr/next_vals and h_new)\n",
    "                new_layer.append(new_node)\n",
    "           \n",
    "            new_layer = tf.transpose(new_layer)\n",
    "            h_current[i] = new_layer\n",
    "            prev_vals = [x] + h_current[:-1]\n",
    "            curr_vals = h_current\n",
    "            next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        return h_current\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob(x, v, y, is_hmc = True),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps = int(1000 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_kernel, is_update_kernel = True):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat(h_current, axis = 1)\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_burnin_steps = 0\n",
    "        num_results = 1\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "    \n",
    "        # Generate new states of chains\n",
    "        #h_state = rerange(samples[0][0])\n",
    "        h_state = samples[0][0]\n",
    "        h_new = tf.split(h_state, self.hidden_layer_sizes, axis = 1) \n",
    "        \n",
    "        # Update the kernel if necesssary\n",
    "        if is_update_kernel:\n",
    "            new_step_size = samples[2].new_step_size.numpy()\n",
    "            ker_new = self.generate_hmc_kernel(x, y, new_step_size)\n",
    "            return(h_new, ker_new)\n",
    "        else:\n",
    "            return h_new\n",
    "    \n",
    "    def update_weights(self, x, h, y, is_gibbs = False):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y, is_gibbs = is_gibbs))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, tf.float32), logits = logits)\n",
    "        \n",
    "        return tf.reduce_sum(loss, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41bdd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_backprop(size, dat, epochs):\n",
    "    '''\n",
    "    Standard Backpropogation training\n",
    "    '''\n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    print(\"Start Standard Backprop\")\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(2,)),\n",
    "            layers.Dense(size, activation = \"sigmoid\"),\n",
    "            layers.Dense(1, activation = \"sigmoid\")\n",
    "        ]\n",
    "    )   \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    st = time.time()\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    history = model.fit(dat, batch_size=batch_size, epochs=epochs)\n",
    "    train_time = time.time() - st\n",
    "    \n",
    "    return train_time, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4415353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(size, dat, epochs, burnin = 500):\n",
    "    '''\n",
    "    HMC training\n",
    "    '''\n",
    "    \n",
    "    targets = np.concatenate([target for data, target in dat.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start HMC\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs=1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat]\n",
    "    kernels = [model.generate_hmc_kernel(data, target) for data, target in dat]\n",
    "    \n",
    "    print(\"Start HMC Burning\")\n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "        \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "            res.append(model.propose_new_state_hamiltonian(data, network[bs], target, kernels[bs]))\n",
    "            burnin_loss += -1 * tf.reduce_sum(model.target_log_prob(data, network[bs], target))\n",
    "    \n",
    "        network, kernels = zip(*res)\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "        \n",
    "    \n",
    "    print(\"Start HMC Training\")\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target)\n",
    "            network = [model.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel = False) \\\n",
    "                       for (x, y), net, ker in zip(dat, network, kernels)]\n",
    "            \n",
    "        loss = 0.0\n",
    "        for data, target in dat:\n",
    "            loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        loss /= (bs + 1)\n",
    "        losses.append(loss)       \n",
    "        \n",
    "        preds = [model.get_predictions(data) for data, target in dat]\n",
    "        acc = accuracy_score(np.concatenate(preds), targets)\n",
    "        accs.append(acc)\n",
    "    \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"acc\": accs, \"loss\": losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b9d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(size, dat, epochs, burnin = 500):\n",
    "    '''\n",
    "    Gibbs Training\n",
    "    '''\n",
    "    \n",
    "    targets = np.concatenate([target for data, target in dat.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start Gibbs\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs=1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat]\n",
    "\n",
    "    print(\"Start Gibbs Burning\")    \n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "    \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "            res.append(model.gibbs_new_state(data, network[bs], target))\n",
    "            burnin_loss += -1 * tf.reduce_sum(model.target_log_prob(data, network[bs], target, is_gibbs = True))\n",
    "            \n",
    "        network = res\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    # Training\n",
    "    losses = []\n",
    "    accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target, is_gibbs = True)\n",
    "            network = [model.gibbs_new_state(x, net, y) for (x, y), net in zip(dat, network)]\n",
    "            \n",
    "        loss = 0.0\n",
    "        for data, target in dat:\n",
    "            loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        loss /= (bs + 1)\n",
    "        losses.append(loss)       \n",
    "        \n",
    "        preds = [model.get_predictions(data) for data, target in dat]\n",
    "        acc = accuracy_score(np.concatenate(preds), targets)\n",
    "        accs.append(acc)\n",
    "    \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "              % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"acc\": accs, \"loss\": losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd95184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 13:19:14.994248: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[0, 0],\n",
    "           [0, 1],\n",
    "           [1, 0],\n",
    "           [1, 1]])\n",
    "y_train = np.array([[0],\n",
    "           [1],\n",
    "           [1],\n",
    "           [0]])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae7265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if size == 2:\n",
    "        \n",
    "    # set weights for size = 2\n",
    "#    w_0 = np.array([[1, -1], [1, -1]], dtype = \"float32\")\n",
    "#    b_0 = np.array([-0.5, 1], dtype = \"float32\")\n",
    "#    l_0 = [w_0, b_0]\n",
    "\n",
    "#    w_1 = np.array([[1], [1]], dtype = \"float32\")\n",
    "#    b_1 = np.array([-1], dtype = \"float32\")\n",
    "#    l_1 = [w_1, b_1]\n",
    "        \n",
    "#    model_bp.layers[0].set_weights(l_0)\n",
    "#    model_bp.layers[1].set_weights(l_1)\n",
    "        \n",
    "#    network = [model.call(images) for images, labels in train_ds] # initial the shape of the weights \n",
    "#    model.fc_layers[0].set_weights(l_0)\n",
    "#    model.output_layer.set_weights(l_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a2401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Running 0\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.6965 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6963 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6962 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6961 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6960 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6959 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6956 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6955 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0440s/step - loss: 0.7738 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0439s/step - loss: 0.7694 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0433s/step - loss: 0.7652 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0430s/step - loss: 0.7613 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0431s/step - loss: 0.7574 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0432s/step - loss: 0.7536 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0429s/step - loss: 0.7502 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0425s/step - loss: 0.7470 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0420s/step - loss: 0.7438 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0421s/step - loss: 0.7407 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.1045s/step - loss: 0.7546 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.1024s/step - loss: 0.7512 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.1002s/step - loss: 0.7475 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.1016s/step - loss: 0.7441 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.1018s/step - loss: 0.7410 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.1015s/step - loss: 0.7379 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.1015s/step - loss: 0.7352 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.1010s/step - loss: 0.7320 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.1001s/step - loss: 0.7298 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.1002s/step - loss: 0.7274 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 1\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.9113 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9034 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8957 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8882 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8810 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8739 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8671 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8606 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8542 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8480 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0412s/step - loss: 0.6970 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0396s/step - loss: 0.6967 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0389s/step - loss: 0.6963 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0384s/step - loss: 0.6960 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0383s/step - loss: 0.6957 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0389s/step - loss: 0.6955 - accuracy: 0.7500\n",
      "Epoch 7/10: - 0.0391s/step - loss: 0.6952 - accuracy: 0.7500\n",
      "Epoch 8/10: - 0.0390s/step - loss: 0.6950 - accuracy: 0.7500\n",
      "Epoch 9/10: - 0.0394s/step - loss: 0.6948 - accuracy: 0.7500\n",
      "Epoch 10/10: - 0.0393s/step - loss: 0.6946 - accuracy: 0.7500\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.0995s/step - loss: 0.7333 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.1011s/step - loss: 0.7308 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.1024s/step - loss: 0.7284 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.1026s/step - loss: 0.7263 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.1015s/step - loss: 0.7241 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.1012s/step - loss: 0.7220 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.1006s/step - loss: 0.7199 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.1009s/step - loss: 0.7183 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.1012s/step - loss: 0.7163 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.1012s/step - loss: 0.7147 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 2\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.7100 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7092 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7085 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7078 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7071 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7065 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7059 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7053 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7047 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7042 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0421s/step - loss: 0.7025 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0413s/step - loss: 0.7018 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0408s/step - loss: 0.7014 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0404s/step - loss: 0.7010 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0403s/step - loss: 0.7007 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0404s/step - loss: 0.7004 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0400s/step - loss: 0.7001 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0397s/step - loss: 0.6998 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0395s/step - loss: 0.6995 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0394s/step - loss: 0.6990 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.1002s/step - loss: 0.7139 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0956s/step - loss: 0.7123 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0943s/step - loss: 0.7114 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0957s/step - loss: 0.7104 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0964s/step - loss: 0.7096 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0972s/step - loss: 0.7083 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0978s/step - loss: 0.7075 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0978s/step - loss: 0.7067 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0976s/step - loss: 0.7058 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0980s/step - loss: 0.7050 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 3\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.8199 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8149 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8099 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8052 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8007 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7963 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7921 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7880 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7841 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7803 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0416s/step - loss: 0.7383 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0412s/step - loss: 0.7363 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0412s/step - loss: 0.7344 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0419s/step - loss: 0.7321 - accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: - 0.0451s/step - loss: 0.7300 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0465s/step - loss: 0.7279 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0462s/step - loss: 0.7262 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0458s/step - loss: 0.7245 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0453s/step - loss: 0.7230 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0449s/step - loss: 0.7216 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.0983s/step - loss: 0.7726 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0962s/step - loss: 0.7678 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0950s/step - loss: 0.7633 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0959s/step - loss: 0.7588 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0969s/step - loss: 0.7546 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0963s/step - loss: 0.7509 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0971s/step - loss: 0.7477 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0968s/step - loss: 0.7446 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0963s/step - loss: 0.7417 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0968s/step - loss: 0.7389 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 4\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.7541 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7515 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7491 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7467 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7445 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7423 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7402 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7382 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7363 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7345 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0384s/step - loss: 0.7903 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0378s/step - loss: 0.7847 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0373s/step - loss: 0.7794 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0372s/step - loss: 0.7746 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0372s/step - loss: 0.7700 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0381s/step - loss: 0.7658 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0392s/step - loss: 0.7620 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0398s/step - loss: 0.7583 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0399s/step - loss: 0.7545 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0399s/step - loss: 0.7510 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.0927s/step - loss: 0.7219 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0960s/step - loss: 0.7201 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0970s/step - loss: 0.7183 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0985s/step - loss: 0.7167 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0985s/step - loss: 0.7149 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0990s/step - loss: 0.7134 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0990s/step - loss: 0.7121 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0985s/step - loss: 0.7109 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0980s/step - loss: 0.7098 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0981s/step - loss: 0.7087 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 5\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.7313 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7296 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7280 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7264 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7249 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7235 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7222 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7209 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7197 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7185 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0421s/step - loss: 0.7227 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0412s/step - loss: 0.7204 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0409s/step - loss: 0.7180 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0408s/step - loss: 0.7158 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0409s/step - loss: 0.7139 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0408s/step - loss: 0.7120 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0408s/step - loss: 0.7104 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0408s/step - loss: 0.7089 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0409s/step - loss: 0.7075 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0409s/step - loss: 0.7062 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.0964s/step - loss: 0.7199 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0947s/step - loss: 0.7188 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0971s/step - loss: 0.7180 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0985s/step - loss: 0.7167 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0993s/step - loss: 0.7155 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0988s/step - loss: 0.7144 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0987s/step - loss: 0.7135 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0990s/step - loss: 0.7124 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0997s/step - loss: 0.7115 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0998s/step - loss: 0.7107 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 6\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.7292 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7277 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7263 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7249 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7236 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7223 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7211 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7199 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7188 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7177 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0379s/step - loss: 0.8024 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0376s/step - loss: 0.7959 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0373s/step - loss: 0.7896 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0372s/step - loss: 0.7838 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0371s/step - loss: 0.7784 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0381s/step - loss: 0.7732 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0381s/step - loss: 0.7685 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0379s/step - loss: 0.7636 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0381s/step - loss: 0.7591 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0383s/step - loss: 0.7551 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.0941s/step - loss: 0.7499 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0973s/step - loss: 0.7466 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0987s/step - loss: 0.7435 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0996s/step - loss: 0.7413 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0990s/step - loss: 0.7383 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0990s/step - loss: 0.7359 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0990s/step - loss: 0.7333 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0994s/step - loss: 0.7310 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0998s/step - loss: 0.7288 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0996s/step - loss: 0.7268 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 7\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 214ms/step - loss: 1.2760 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2602 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2445 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2291 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2139 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1990 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1844 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1700 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1559 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1420 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0468s/step - loss: 0.7015 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0445s/step - loss: 0.7010 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0425s/step - loss: 0.7006 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0415s/step - loss: 0.7002 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0408s/step - loss: 0.6997 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0409s/step - loss: 0.6993 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0405s/step - loss: 0.6989 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0403s/step - loss: 0.6985 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0401s/step - loss: 0.6981 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0398s/step - loss: 0.6978 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.1032s/step - loss: 0.7354 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.1014s/step - loss: 0.7337 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.1020s/step - loss: 0.7321 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.1043s/step - loss: 0.7305 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.1038s/step - loss: 0.7288 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.1039s/step - loss: 0.7277 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.1041s/step - loss: 0.7264 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.1033s/step - loss: 0.7249 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.1027s/step - loss: 0.7237 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.1019s/step - loss: 0.7225 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 8\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.8014 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7971 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7929 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7888 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7849 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7812 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7776 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7741 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7708 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7676 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0389s/step - loss: 0.7318 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0386s/step - loss: 0.7288 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0380s/step - loss: 0.7263 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0380s/step - loss: 0.7241 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0381s/step - loss: 0.7218 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0398s/step - loss: 0.7197 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0397s/step - loss: 0.7177 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0396s/step - loss: 0.7157 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0395s/step - loss: 0.7140 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0397s/step - loss: 0.7126 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.0944s/step - loss: 0.7862 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0959s/step - loss: 0.7815 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0956s/step - loss: 0.7762 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0964s/step - loss: 0.7714 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0968s/step - loss: 0.7669 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0976s/step - loss: 0.7621 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0981s/step - loss: 0.7585 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0986s/step - loss: 0.7545 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0994s/step - loss: 0.7506 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0995s/step - loss: 0.7471 - accuracy: 0.5000\n",
      "---------------------------------------\n",
      "Running 9\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.7720 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7688 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7658 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7629 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7601 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7574 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7548 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7523 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7499 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7476 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0406s/step - loss: 0.6992 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0410s/step - loss: 0.6989 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0402s/step - loss: 0.6985 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0401s/step - loss: 0.6982 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0400s/step - loss: 0.6979 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0402s/step - loss: 0.6975 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0400s/step - loss: 0.6972 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0398s/step - loss: 0.6969 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0397s/step - loss: 0.6967 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0396s/step - loss: 0.6964 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.1022s/step - loss: 0.7002 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0995s/step - loss: 0.6997 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0977s/step - loss: 0.6993 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0989s/step - loss: 0.6989 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0992s/step - loss: 0.6986 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0990s/step - loss: 0.6982 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0992s/step - loss: 0.6978 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0990s/step - loss: 0.6973 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0986s/step - loss: 0.6971 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0991s/step - loss: 0.6967 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "epochs = 10\n",
    "burnin = 10\n",
    "size = 32\n",
    "res_bp, res_hmc, res_gibbs = [], [], []\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Running {i}\")\n",
    "    \n",
    "    \n",
    "    time_bp, history_bp = standard_backprop(size, train_ds, epochs)\n",
    "    burnin_loss_hmc, time_hmc, history_hmc = hmc(size, train_ds, epochs, burnin)\n",
    "    burnin_loss_gibbs, time_gibbs, history_gibbs = gibbs(size, train_ds, epochs, burnin)\n",
    "    \n",
    "    hist_bp = {\"acc\": history_bp.history['accuracy'], \"loss\": history_bp.history['loss']}\n",
    "    rbp = {'time': time_bp, 'history': hist_bp}\n",
    "    rhmc = {'time': time_hmc, 'burnin': burnin_loss_hmc, 'history': history_hmc}\n",
    "    rgibbs = {'time': time_gibbs, 'burnin': burnin_loss_gibbs, 'history': history_gibbs}\n",
    "    \n",
    "    res_bp.append(rbp)\n",
    "    res_hmc.append(rhmc)\n",
    "    res_gibbs.append(rgibbs)\n",
    "\n",
    "res_all = [res_bp, res_hmc, res_gibbs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6fda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average curve for each method\n",
    "def cal_avg(res):\n",
    "    \n",
    "    metrics = ['acc', 'loss']\n",
    "    avg = {}\n",
    "    for metric in metrics:\n",
    "        arr_metric = np.zeros((N, epochs))\n",
    "        for i in range(N):\n",
    "            arr_metric[i] = np.array(res[i]['history'][metric])\n",
    "        avg_metric = np.mean(arr_metric, axis = 0)\n",
    "        avg[metric] = avg_metric\n",
    "        \n",
    "    return avg\n",
    "\n",
    "avg_bp = cal_avg(res_bp)\n",
    "avg_hmc = cal_avg(res_hmc)\n",
    "avg_gibbs = cal_avg(res_gibbs)\n",
    "avg_all = [avg_bp, avg_hmc, avg_gibbs]\n",
    "\n",
    "time_bp, time_hmc, time_gibbs = [], [], []\n",
    "for i in range(N):\n",
    "    time_bp.append(res_bp[i]['time'])\n",
    "    time_hmc.append(res_hmc[i]['time'])\n",
    "    time_gibbs.append(res_gibbs[i]['time'])\n",
    "    \n",
    "with open('data_xor.npy', 'wb') as f:\n",
    "    np.save(f, np.array(res_all))\n",
    "    np.save(f, np.array(avg_all))\n",
    "    np.save(f, np.array(time_bp))\n",
    "    np.save(f, np.array(time_hmc))\n",
    "    np.save(f, np.array(time_gibbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4aca692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the running times for each method\n",
    "def plot_all(res, method, metric):\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    nrow = 4\n",
    "    ncol = 3\n",
    "    \n",
    "    fig, ax = plt.subplots(nrow, ncol, sharex = True)\n",
    "    fig.suptitle(method + \"_\" + metric)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            if i * ncol + j < N:\n",
    "                ax[i, j].plot(res[i * ncol + j]['history'][metric])\n",
    "                ax[i, j].set_title(f\"Run {i * ncol + j}\")\n",
    "    plt.savefig(method + \"_\" + metric + '.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "methods = ['bp', 'hmc', 'gibbs']\n",
    "metrics = ['acc', 'loss']\n",
    "for i, method in enumerate(methods):\n",
    "    for metric in metrics:\n",
    "        plot_all(res_all[i], method, metric)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "for metric in metrics:\n",
    "    for i, method in enumerate(methods):\n",
    "        plt.plot(avg_all[i][metric], label = method)\n",
    "    plt.title(metric)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"average_\" + metric + '.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bp.history['accuracy'], label = 'BP')\n",
    "plt.plot(list(range(epochs)), history_hmc['acc'], label = 'HMC')\n",
    "plt.plot(list(range(epochs)), history_gibbs['acc'], label = 'Gibbs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e31bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bp.history['loss'], label = 'BP')\n",
    "plt.plot(list(range(epochs)), history_hmc['loss'], label = 'HMC')\n",
    "plt.plot(list(range(epochs)), history_gibbs['loss'], label = 'Gibbs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84defaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
