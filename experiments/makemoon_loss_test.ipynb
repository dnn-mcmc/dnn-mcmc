{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0475ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Gibbs to test the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330436ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410e7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]    \n",
    "    return t\n",
    "\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557b88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10, lr=1e-3):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y, is_gibbs = False, is_hmc = False):\n",
    "        \n",
    "        # get current state\n",
    "        if is_hmc:\n",
    "            h_current = tf.split(h, self.hidden_layer_sizes, axis = 1)\n",
    "        else:    \n",
    "            h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_current = convert2_zero_one(h_current)\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            logits = layer(pv)\n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(labels = cv, logits = logits)\n",
    "            if not is_gibbs:\n",
    "                ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def gibbs_new_state(self, x, h, y):\n",
    "        \n",
    "        '''\n",
    "            generate a new state for the network node by node in Gibbs setting.\n",
    "        '''\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "\n",
    "            # node by node\n",
    "            \n",
    "            nodes = tf.transpose(cv)\n",
    "            prob_parents = tm.sigmoid(in_layer(pv))\n",
    "            \n",
    "            out_layer_weights = out_layer.get_weights()[0]\n",
    "            \n",
    "            next_logits = out_layer(cv)\n",
    "            \n",
    "            new_layer = []\n",
    "            \n",
    "            for j, node in enumerate(nodes):\n",
    "                \n",
    "                # get info for current node (i, j)\n",
    "                \n",
    "                prob_parents_j = prob_parents[:, j]\n",
    "                out_layer_weights_j = out_layer_weights[j]\n",
    "                \n",
    "                # calculate logits and logprob for node is 0 or 1\n",
    "                next_logits_if_node_0 = next_logits[:, :] - node[:, None] * out_layer_weights_j[None, :]\n",
    "                next_logits_if_node_1 = next_logits[:, :] + (1 - node[:, None]) * out_layer_weights_j[None, :]\n",
    "                \n",
    "                #print(next_logits_if_node_0, next_logits_if_node_1)\n",
    "                \n",
    "                logprob_children_if_node_0 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_0), axis = -1)\n",
    "                \n",
    "                logprob_children_if_node_1 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_1), axis = -1)\n",
    "                \n",
    "                # calculate prob for node (i, j)\n",
    "                prob_0 = (1 - prob_parents_j) * tm.exp(logprob_children_if_node_0)\n",
    "                prob_1 = prob_parents_j * tm.exp(logprob_children_if_node_1)\n",
    "                prob_j = prob_1 / (prob_1 + prob_0)\n",
    "            \n",
    "                # sample new state with prob_j for node (i, j)\n",
    "                new_node = tfp.distributions.Bernoulli(probs = prob_j).sample() # MAY BE SLOW\n",
    "                \n",
    "                # update nodes and logits for following calculation\n",
    "                new_node_casted = tf.cast(new_node, dtype = \"float32\")\n",
    "                next_logits = next_logits_if_node_0 * (1 - new_node_casted)[:, None] \\\n",
    "                            + next_logits_if_node_1 * new_node_casted[:, None] \n",
    "                \n",
    "                # keep track of new node values (in prev/curr/next_vals and h_new)\n",
    "                new_layer.append(new_node)\n",
    "           \n",
    "            new_layer = tf.transpose(new_layer)\n",
    "            h_current[i] = new_layer\n",
    "            prev_vals = [x] + h_current[:-1]\n",
    "            curr_vals = h_current\n",
    "            next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        return h_current\n",
    "    \n",
    "    def update_weights(self, x, h, y, is_gibbs = False):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y, is_gibbs = is_gibbs))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, tf.float32), logits = logits)\n",
    "        \n",
    "        return tf.reduce_sum(loss, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b463e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(size, dat_train, epochs, burnin = 500):\n",
    "    '''\n",
    "    Gibbs Training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start Gibbs\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs=1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start Gibbs Burning\")    \n",
    "    for i in range(burnin):\n",
    "    \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.gibbs_new_state(data, network[bs], target))\n",
    " \n",
    "        network = res\n",
    "    \n",
    "    # Training\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target, is_gibbs = True)\n",
    "            network = [model.gibbs_new_state(x, net, y) for (x, y), net in zip(dat_train, network)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        train_loss /= (bs + 1)\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc) \n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f \" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc))\n",
    "    \n",
    "    y_logits = []\n",
    "    for data, target in dat_train:\n",
    "        \n",
    "        logit = 0.0\n",
    "        x = data\n",
    "        for layer in model.fc_layers:\n",
    "            logit = layer(x)\n",
    "            x = tm.sigmoid(logit)\n",
    "            \n",
    "        logit = model.output_layer(x)\n",
    "        y_logits.append(logit.numpy())\n",
    "    \n",
    "    print(y_logits)\n",
    "    y_train = [target for data, target in dat_train.as_numpy_iterator()]\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    true_loss = bce(y_train, y_logits).numpy()\n",
    "    \n",
    "    return {\"train_acc\": train_accs, \"train_loss\": train_losses}, true_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38aefc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 00:12:12.269401: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "X, Y = make_moons(200, noise = 0.3)\n",
    "\n",
    "# Split into test and training data\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, random_state=73)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5389839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 2.0823s/step - train_loss: 0.6949 - train_acc: 0.5188 \n",
      "Epoch 2/10: - 2.0978s/step - train_loss: 0.6886 - train_acc: 0.5000 \n",
      "Epoch 3/10: - 2.1034s/step - train_loss: 0.6831 - train_acc: 0.6312 \n",
      "Epoch 4/10: - 2.1094s/step - train_loss: 0.6784 - train_acc: 0.6875 \n",
      "Epoch 5/10: - 2.1054s/step - train_loss: 0.6756 - train_acc: 0.5563 \n",
      "Epoch 6/10: - 2.1046s/step - train_loss: 0.6717 - train_acc: 0.5875 \n",
      "Epoch 7/10: - 2.1044s/step - train_loss: 0.6682 - train_acc: 0.7312 \n",
      "Epoch 8/10: - 2.1044s/step - train_loss: 0.6680 - train_acc: 0.7250 \n",
      "Epoch 9/10: - 2.1028s/step - train_loss: 0.6685 - train_acc: 0.6687 \n",
      "Epoch 10/10: - 2.1005s/step - train_loss: 0.6686 - train_acc: 0.6000 \n",
      "[array([[-0.12767382],\n",
      "       [-0.02509977],\n",
      "       [-0.16293673],\n",
      "       [-0.08931442],\n",
      "       [-0.2111633 ],\n",
      "       [-0.18161868],\n",
      "       [-0.13277508],\n",
      "       [-0.03981177],\n",
      "       [ 0.02000271],\n",
      "       [-0.20347504],\n",
      "       [-0.11888413],\n",
      "       [-0.13537873],\n",
      "       [-0.15460671],\n",
      "       [-0.16723023],\n",
      "       [ 0.01176699],\n",
      "       [-0.19737832],\n",
      "       [-0.00773538],\n",
      "       [-0.08133446],\n",
      "       [-0.23083456],\n",
      "       [-0.21406226],\n",
      "       [-0.22559465],\n",
      "       [-0.13446112],\n",
      "       [-0.16115169],\n",
      "       [-0.07470761],\n",
      "       [-0.15321521],\n",
      "       [-0.13142021],\n",
      "       [-0.1898989 ],\n",
      "       [-0.01564212],\n",
      "       [-0.10560517],\n",
      "       [-0.20576234],\n",
      "       [-0.09757622],\n",
      "       [ 0.02879928]], dtype=float32), array([[-0.13976927],\n",
      "       [-0.13358189],\n",
      "       [-0.04382737],\n",
      "       [-0.24805342],\n",
      "       [-0.0734963 ],\n",
      "       [-0.10234345],\n",
      "       [ 0.00804286],\n",
      "       [-0.02966456],\n",
      "       [-0.20709328],\n",
      "       [-0.04917906],\n",
      "       [-0.17498295],\n",
      "       [-0.14271651],\n",
      "       [-0.09251417],\n",
      "       [-0.00583373],\n",
      "       [-0.14464323],\n",
      "       [-0.02475907],\n",
      "       [-0.20792122],\n",
      "       [-0.00929041],\n",
      "       [-0.20306377],\n",
      "       [-0.2089666 ],\n",
      "       [-0.01539065],\n",
      "       [-0.02899988],\n",
      "       [-0.16018747],\n",
      "       [-0.04522527],\n",
      "       [-0.21944089],\n",
      "       [-0.07479717],\n",
      "       [-0.19429694],\n",
      "       [-0.20395069],\n",
      "       [-0.11109652],\n",
      "       [-0.05315403],\n",
      "       [-0.19520961],\n",
      "       [-0.09286501]], dtype=float32), array([[-0.0285211 ],\n",
      "       [-0.08041771],\n",
      "       [-0.13558285],\n",
      "       [-0.24797629],\n",
      "       [-0.00556649],\n",
      "       [-0.18291979],\n",
      "       [-0.09109356],\n",
      "       [-0.1031297 ],\n",
      "       [-0.09131752],\n",
      "       [-0.09771468],\n",
      "       [-0.0589716 ],\n",
      "       [-0.11931713],\n",
      "       [-0.22312205],\n",
      "       [-0.15969874],\n",
      "       [-0.06711836],\n",
      "       [-0.1703531 ],\n",
      "       [-0.01340808],\n",
      "       [-0.09481446],\n",
      "       [-0.08603676],\n",
      "       [-0.11512871],\n",
      "       [-0.21202247],\n",
      "       [-0.15737338],\n",
      "       [-0.19196533],\n",
      "       [ 0.02891396],\n",
      "       [-0.10352652],\n",
      "       [ 0.03794064],\n",
      "       [ 0.11253871],\n",
      "       [-0.1511241 ],\n",
      "       [-0.03643666],\n",
      "       [-0.12288435],\n",
      "       [-0.0615762 ],\n",
      "       [-0.04719717]], dtype=float32), array([[-0.16510044],\n",
      "       [-0.18808435],\n",
      "       [-0.04774449],\n",
      "       [-0.04945768],\n",
      "       [-0.14340498],\n",
      "       [-0.11197175],\n",
      "       [-0.1036665 ],\n",
      "       [ 0.00605221],\n",
      "       [-0.16167565],\n",
      "       [-0.11995406],\n",
      "       [-0.20605813],\n",
      "       [-0.12266816],\n",
      "       [-0.12759666],\n",
      "       [ 0.00063385],\n",
      "       [-0.12457652],\n",
      "       [-0.07815908],\n",
      "       [-0.01395382],\n",
      "       [ 0.03647195],\n",
      "       [-0.01510606],\n",
      "       [-0.04639922],\n",
      "       [-0.05076231],\n",
      "       [-0.13211499],\n",
      "       [-0.11178969],\n",
      "       [-0.17494543],\n",
      "       [-0.05270792],\n",
      "       [-0.12767015],\n",
      "       [-0.14051442],\n",
      "       [ 0.0085267 ],\n",
      "       [-0.15773933],\n",
      "       [-0.09525089],\n",
      "       [-0.07712375],\n",
      "       [-0.25148156]], dtype=float32), array([[-0.01034964],\n",
      "       [-0.13059537],\n",
      "       [-0.02590803],\n",
      "       [-0.1789876 ],\n",
      "       [-0.20242928],\n",
      "       [-0.11629827],\n",
      "       [ 0.00379523],\n",
      "       [-0.07548992],\n",
      "       [-0.04330857],\n",
      "       [-0.00355632],\n",
      "       [-0.165805  ],\n",
      "       [-0.18334876],\n",
      "       [-0.05321778],\n",
      "       [-0.17015545],\n",
      "       [-0.06672029],\n",
      "       [-0.12487601],\n",
      "       [-0.11793233],\n",
      "       [-0.11134429],\n",
      "       [-0.21858059],\n",
      "       [-0.12675278],\n",
      "       [-0.02554337],\n",
      "       [-0.04790942],\n",
      "       [-0.21920906],\n",
      "       [ 0.00302722],\n",
      "       [-0.23585363],\n",
      "       [-0.20886032],\n",
      "       [-0.06231413],\n",
      "       [-0.04581289],\n",
      "       [-0.02077992],\n",
      "       [-0.06832908],\n",
      "       [-0.18278416],\n",
      "       [-0.23192595]], dtype=float32)]\n",
      "0.6686394\n",
      "0.66863936\n"
     ]
    }
   ],
   "source": [
    "size = 32\n",
    "epochs = 10\n",
    "burnin = 100\n",
    "\n",
    "hist, ce = gibbs(size, train_ds, epochs, burnin)\n",
    "print(hist['train_loss'][-1].numpy())\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb098b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
