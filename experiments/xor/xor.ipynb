{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb2baee",
   "metadata": {},
   "source": [
    "### Compare three methods(SGD, HMC, Gibbs) and 2 models(narrow, wider)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a537a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6b5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]    \n",
    "    return t\n",
    "\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb761182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10, lr=1e-3):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def get_weight(self):\n",
    "        \n",
    "        weights = []\n",
    "        for layer in self.fc_layers:\n",
    "            weights.append(layer.get_weights())\n",
    "        \n",
    "        weights.append(self.output_layer.get_weights())\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def target_log_prob(self, x, h, y, is_gibbs = False, is_hmc = False, is_loss = False):\n",
    "        \n",
    "        # get current state\n",
    "        if is_hmc:\n",
    "            h_current = tf.split(h, self.hidden_layer_sizes, axis = 1)\n",
    "        else:    \n",
    "            h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_current = convert2_zero_one(h_current)\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        if not is_loss:\n",
    "            for i, (cv, pv, layer) in enumerate(zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "                logits = layer(pv)\n",
    "                ce = tf.nn.sigmoid_cross_entropy_with_logits(labels = cv, logits = logits)\n",
    "                if not is_gibbs:\n",
    "                    ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "                nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def gibbs_new_state(self, x, h, y):\n",
    "        \n",
    "        '''\n",
    "            generate a new state for the network node by node in Gibbs setting.\n",
    "        '''\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "\n",
    "            # node by node\n",
    "            \n",
    "            nodes = tf.transpose(cv)\n",
    "            prob_parents = tm.sigmoid(in_layer(pv))\n",
    "            \n",
    "            out_layer_weights = out_layer.get_weights()[0]\n",
    "            \n",
    "            next_logits = out_layer(cv)\n",
    "            \n",
    "            new_layer = []\n",
    "            \n",
    "            for j, node in enumerate(nodes):\n",
    "                \n",
    "                # get info for current node (i, j)\n",
    "                \n",
    "                prob_parents_j = prob_parents[:, j]\n",
    "                out_layer_weights_j = out_layer_weights[j]\n",
    "                \n",
    "                # calculate logits and logprob for node is 0 or 1\n",
    "                next_logits_if_node_0 = next_logits[:, :] - node[:, None] * out_layer_weights_j[None, :]\n",
    "                next_logits_if_node_1 = next_logits[:, :] + (1 - node[:, None]) * out_layer_weights_j[None, :]\n",
    "                \n",
    "                #print(next_logits_if_node_0, next_logits_if_node_1)\n",
    "                \n",
    "                logprob_children_if_node_0 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_0), axis = -1)\n",
    "                \n",
    "                logprob_children_if_node_1 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_1), axis = -1)\n",
    "                \n",
    "                # calculate prob for node (i, j)\n",
    "                prob_0 = (1 - prob_parents_j) * tm.exp(logprob_children_if_node_0)\n",
    "                prob_1 = prob_parents_j * tm.exp(logprob_children_if_node_1)\n",
    "                prob_j = prob_1 / (prob_1 + prob_0)\n",
    "            \n",
    "                # sample new state with prob_j for node (i, j)\n",
    "                new_node = tfp.distributions.Bernoulli(probs = prob_j).sample() # MAY BE SLOW\n",
    "                \n",
    "                # update nodes and logits for following calculation\n",
    "                new_node_casted = tf.cast(new_node, dtype = \"float32\")\n",
    "                next_logits = next_logits_if_node_0 * (1 - new_node_casted)[:, None] \\\n",
    "                            + next_logits_if_node_1 * new_node_casted[:, None] \n",
    "                \n",
    "                # keep track of new node values (in prev/curr/next_vals and h_new)\n",
    "                new_layer.append(new_node)\n",
    "           \n",
    "            new_layer = tf.transpose(new_layer)\n",
    "            h_current[i] = new_layer\n",
    "            prev_vals = [x] + h_current[:-1]\n",
    "            curr_vals = h_current\n",
    "            next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        return h_current\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob(x, v, y, is_hmc = True),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps = int(1000 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_kernel, is_update_kernel = True):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat(h_current, axis = 1)\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_burnin_steps = 0\n",
    "        num_results = 1\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "    \n",
    "        # Generate new states of chains\n",
    "        #h_state = rerange(samples[0][0])\n",
    "        h_state = samples[0][0]\n",
    "        h_new = tf.split(h_state, self.hidden_layer_sizes, axis = 1) \n",
    "        \n",
    "        # Update the kernel if necesssary\n",
    "        if is_update_kernel:\n",
    "            new_step_size = samples[2].new_step_size.numpy()\n",
    "            ker_new = self.generate_hmc_kernel(x, y, new_step_size)\n",
    "            return(h_new, ker_new)\n",
    "        else:\n",
    "            return h_new\n",
    "    \n",
    "    def update_weights(self, x, h, y, is_gibbs = False):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y, is_gibbs = is_gibbs))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, tf.float32), logits = logits)\n",
    "        \n",
    "        return tf.reduce_sum(loss, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41bdd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_backprop(size, dat, epochs):\n",
    "    '''\n",
    "    Standard Backpropogation training\n",
    "    '''\n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    print(\"Start Standard Backprop\")\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(2,)),\n",
    "            layers.Dense(size, activation = \"sigmoid\"),\n",
    "            layers.Dense(1, activation = \"sigmoid\")\n",
    "        ]\n",
    "    )   \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    st = time.time()\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    history = model.fit(dat, batch_size=batch_size, epochs=epochs)\n",
    "    train_time = time.time() - st\n",
    "    \n",
    "    return train_time, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4415353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(size, dat, epochs, burnin = 500, nrec = 5):\n",
    "    '''\n",
    "    HMC training\n",
    "    '''\n",
    "    \n",
    "    targets = np.concatenate([target for data, target in dat.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start HMC\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs=1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat]\n",
    "    kernels = [model.generate_hmc_kernel(data, target) for data, target in dat]\n",
    "    \n",
    "    print(\"Start HMC Burning\")\n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "        \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "            res.append(model.propose_new_state_hamiltonian(data, network[bs], target, kernels[bs]))\n",
    "            burnin_loss += -1 * tf.reduce_sum(model.target_log_prob(data, network[bs], target))\n",
    "    \n",
    "        network, kernels = zip(*res)\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "        \n",
    "    \n",
    "    print(\"Start HMC Training\")\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    start_time = time.time()\n",
    "    samples = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target)\n",
    "            network = [model.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel = False) \\\n",
    "                       for (x, y), net, ker in zip(dat, network, kernels)]\n",
    "        \n",
    "        if epoch >= epochs - nrec:\n",
    "            sample = {\"net\": network, \"weight\": model.get_weight()}\n",
    "            samples.append(sample)\n",
    "        \n",
    "        loss = 0.0\n",
    "        for data, target in dat:\n",
    "            loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        loss /= (bs + 1)\n",
    "        losses.append(loss)       \n",
    "        \n",
    "        preds = [model.get_predictions(data) for data, target in dat]\n",
    "        acc = accuracy_score(np.concatenate(preds), targets)\n",
    "        accs.append(acc)\n",
    "    \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return samples, burnin_losses, train_time, {\"acc\": accs, \"loss\": losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b9d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(size, dat, epochs, burnin = 500):\n",
    "    '''\n",
    "    Gibbs Training\n",
    "    '''\n",
    "    \n",
    "    targets = np.concatenate([target for data, target in dat.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start Gibbs\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs=1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat]\n",
    "    \n",
    "    \n",
    "    print(\"Start Gibbs Burning\")    \n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "    \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "            res.append(model.gibbs_new_state(data, network[bs], target))\n",
    "            burnin_loss += -1 * tf.reduce_sum(model.target_log_prob(data, network[bs], target, is_gibbs = True))\n",
    "            \n",
    "        network = res\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    # Training\n",
    "    losses = []\n",
    "    accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target, is_gibbs = True)\n",
    "            network = [model.gibbs_new_state(x, net, y) for (x, y), net in zip(dat, network)]\n",
    "            \n",
    "        loss = 0.0\n",
    "        for data, target in dat:\n",
    "            loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        loss /= (bs + 1)\n",
    "        losses.append(loss)       \n",
    "        \n",
    "        preds = [model.get_predictions(data) for data, target in dat]\n",
    "        acc = accuracy_score(np.concatenate(preds), targets)\n",
    "        accs.append(acc)\n",
    "    \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "              % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"acc\": accs, \"loss\": losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd95184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 23:05:51.350045: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[0, 0],\n",
    "           [0, 1],\n",
    "           [1, 0],\n",
    "           [1, 1]])\n",
    "y_train = np.array([[0],\n",
    "           [1],\n",
    "           [1],\n",
    "           [0]])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if size == 2:\n",
    "        \n",
    "    # set weights for size = 2\n",
    "#    w_0 = np.array([[1, -1], [1, -1]], dtype = \"float32\")\n",
    "#    b_0 = np.array([-0.5, 1], dtype = \"float32\")\n",
    "#    l_0 = [w_0, b_0]\n",
    "\n",
    "#    w_1 = np.array([[1], [1]], dtype = \"float32\")\n",
    "#    b_1 = np.array([-1], dtype = \"float32\")\n",
    "#    l_1 = [w_1, b_1]\n",
    "        \n",
    "#    model_bp.layers[0].set_weights(l_0)\n",
    "#    model_bp.layers[1].set_weights(l_1)\n",
    "        \n",
    "#    network = [model.call(images) for images, labels in train_ds] # initial the shape of the weights \n",
    "#    model.fc_layers[0].set_weights(l_0)\n",
    "#    model.output_layer.set_weights(l_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a2401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Running 0\n",
      "Start Standard Backprop\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.7619 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7590 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7562 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7536 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7510 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7485 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7461 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7439 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7417 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7396 - accuracy: 0.5000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/10: - 0.0489s/step - loss: 0.7685 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0469s/step - loss: 0.7637 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0449s/step - loss: 0.7594 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.0434s/step - loss: 0.7553 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.0428s/step - loss: 0.7515 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.0431s/step - loss: 0.7480 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.0434s/step - loss: 0.7445 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.0434s/step - loss: 0.7414 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.0435s/step - loss: 0.7385 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.0442s/step - loss: 0.7358 - accuracy: 0.5000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/10: - 0.0965s/step - loss: 0.6996 - accuracy: 0.5000\n",
      "Epoch 2/10: - 0.0954s/step - loss: 0.6995 - accuracy: 0.5000\n",
      "Epoch 3/10: - 0.0988s/step - loss: 0.6991 - accuracy: 0.5000\n",
      "Epoch 4/10: - 0.1016s/step - loss: 0.6987 - accuracy: 0.5000\n",
      "Epoch 5/10: - 0.1018s/step - loss: 0.6983 - accuracy: 0.5000\n",
      "Epoch 6/10: - 0.1028s/step - loss: 0.6981 - accuracy: 0.5000\n",
      "Epoch 7/10: - 0.1034s/step - loss: 0.6979 - accuracy: 0.5000\n",
      "Epoch 8/10: - 0.1023s/step - loss: 0.6976 - accuracy: 0.5000\n",
      "Epoch 9/10: - 0.1019s/step - loss: 0.6975 - accuracy: 0.5000\n",
      "Epoch 10/10: - 0.1023s/step - loss: 0.6974 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "N = 1\n",
    "epochs = 10\n",
    "burnin = 10\n",
    "nrec = 3\n",
    "size = 32\n",
    "res_bp, res_hmc, res_gibbs = [], [], []\n",
    "samples_hmc = []\n",
    "samples_gibbs = []\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Running {i}\")\n",
    "    \n",
    "    \n",
    "    time_bp, history_bp = standard_backprop(size, train_ds, epochs)\n",
    "    s_hmc, burnin_loss_hmc, time_hmc, history_hmc = hmc(size, train_ds, epochs, burnin, nrec)\n",
    "    s_gibbs, burnin_loss_gibbs, time_gibbs, history_gibbs = gibbs(size, train_ds, epochs, burnin)\n",
    "    \n",
    "    hist_bp = {\"acc\": history_bp.history['accuracy'], \"loss\": history_bp.history['loss']}\n",
    "    rbp = {'time': time_bp, 'history': hist_bp}\n",
    "    rhmc = {'time': time_hmc, 'burnin': burnin_loss_hmc, 'history': history_hmc}\n",
    "    rgibbs = {'time': time_gibbs, 'burnin': burnin_loss_gibbs, 'history': history_gibbs}\n",
    "    \n",
    "    res_bp.append(rbp)\n",
    "    res_hmc.append(rhmc)\n",
    "    res_gibbs.append(rgibbs)\n",
    "    \n",
    "    samples_hmc.append(s_hmc)\n",
    "    samples_gibbs.append(s_gibbs)\n",
    "\n",
    "res_all = [res_bp, res_hmc, res_gibbs]\n",
    "samples_all = [samples_hmc, samples_gibbs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6fda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average curve for each method\n",
    "def cal_avg(res):\n",
    "    \n",
    "    metrics = ['acc', 'loss']\n",
    "    avg = {}\n",
    "    for metric in metrics:\n",
    "        arr_metric = np.zeros((N, epochs))\n",
    "        for i in range(N):\n",
    "            arr_metric[i] = np.array(res[i]['history'][metric])\n",
    "        avg_metric = np.mean(arr_metric, axis = 0)\n",
    "        avg[metric] = avg_metric\n",
    "        \n",
    "    return avg\n",
    "\n",
    "avg_bp = cal_avg(res_bp)\n",
    "avg_hmc = cal_avg(res_hmc)\n",
    "avg_gibbs = cal_avg(res_gibbs)\n",
    "avg_all = [avg_bp, avg_hmc, avg_gibbs]\n",
    "\n",
    "time_bp, time_hmc, time_gibbs = [], [], []\n",
    "for i in range(N):\n",
    "    time_bp.append(res_bp[i]['time'])\n",
    "    time_hmc.append(res_hmc[i]['time'])\n",
    "    time_gibbs.append(res_gibbs[i]['time'])\n",
    "    \n",
    "with open('history_xor.npy', 'wb') as f:\n",
    "    np.save(f, np.array(res_all))\n",
    "    np.save(f, np.array(avg_all))\n",
    "    np.save(f, np.array(time_bp))\n",
    "    np.save(f, np.array(time_hmc))\n",
    "    np.save(f, np.array(time_gibbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aca692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the running times for each method\n",
    "def plot_all(res, method, metric):\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    nrow = 1\n",
    "    ncol = N\n",
    "    \n",
    "    fig, ax = plt.subplots(nrow, ncol, sharex = True)\n",
    "    fig.suptitle(method + \"_\" + metric)\n",
    "    if nrow == 1:\n",
    "        for j in range(ncol):\n",
    "            ax[j].plot(res[j]['history'][metric])\n",
    "            ax[j].set_title(f\"Run {j}\")\n",
    "    else:\n",
    "        for i in range(nrow):\n",
    "            for j in range(ncol):\n",
    "                if i * ncol + j < N:\n",
    "                    ax[i, j].plot(res[i * ncol + j]['history'][metric])\n",
    "                    ax[i, j].set_title(f\"Run {i * ncol + j}\")\n",
    "    plt.savefig(method + \"_\" + metric + '.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "methods = ['bp', 'hmc', 'gibbs']\n",
    "metrics = ['acc', 'loss']\n",
    "for i, method in enumerate(methods):\n",
    "    for metric in metrics:\n",
    "        plot_all(res_all[i], method, metric)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "for metric in metrics:\n",
    "    for i, method in enumerate(methods):\n",
    "        plt.plot(avg_all[i][metric], label = method)\n",
    "    plt.title(metric)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"average_\" + metric + '.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bp.history['accuracy'], label = 'BP')\n",
    "plt.plot(list(range(epochs)), history_hmc['acc'], label = 'HMC')\n",
    "plt.plot(list(range(epochs)), history_gibbs['acc'], label = 'Gibbs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e31bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_bp.history['loss'], label = 'BP')\n",
    "plt.plot(list(range(epochs)), history_hmc['loss'], label = 'HMC')\n",
    "plt.plot(list(range(epochs)), history_gibbs['loss'], label = 'Gibbs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84defaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = np.load(\"sample_xor.npy\", allow_pickle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "748c6dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net': [[<tf.Tensor: shape=(4, 32), dtype=float32, numpy=\n",
       "   array([[ 5.247546  ,  0.7307614 , -0.9986476 ,  1.8493692 , -2.1084967 ,\n",
       "            0.99817675,  2.1708896 ,  3.8547604 ,  2.0662892 ,  0.55476964,\n",
       "            2.5648694 ,  1.8820795 , -2.2667387 ,  2.2725437 ,  0.179213  ,\n",
       "            1.1335764 ,  1.0832776 ,  1.2915303 ,  0.89354396,  0.53566855,\n",
       "           -0.20067498,  0.21958625, -0.23371217,  2.283435  ,  1.7384642 ,\n",
       "            1.4599388 ,  1.9364539 , -1.9246162 ,  2.4115193 ,  0.26946202,\n",
       "            1.9027267 ,  0.93396336],\n",
       "          [-0.08285507,  0.06404629, -1.7181892 ,  2.6197023 , -0.4636103 ,\n",
       "            2.031934  ,  4.2427897 , -1.7785846 , -0.37081188, -2.201135  ,\n",
       "            0.7263847 ,  0.4441526 ,  1.0397297 ,  3.5961483 ,  1.840423  ,\n",
       "            0.10628375, -0.7385302 ,  0.8616044 ,  0.3436515 , -0.72630686,\n",
       "            1.7923987 ,  2.5711749 , -0.15937911,  3.5209155 , -3.7212193 ,\n",
       "            3.300533  ,  1.498778  ,  0.7484917 ,  1.297833  ,  1.6316801 ,\n",
       "            1.5368514 ,  1.7640194 ],\n",
       "          [ 3.6309762 , -0.58831334, -0.22269064, -2.1457314 , -0.91266125,\n",
       "           -0.2464063 ,  2.1826224 ,  0.751665  ,  1.8754942 ,  1.2085414 ,\n",
       "            0.53123593,  2.218343  ,  0.09860885, -1.8551708 ,  0.00752693,\n",
       "           -2.8538508 , -0.8663448 ,  1.8659818 ,  0.4263165 , -2.033121  ,\n",
       "           -1.3593539 ,  0.01790871,  0.7517247 ,  1.3827572 ,  2.2521749 ,\n",
       "           -1.5579114 ,  0.62269807, -0.99395514,  0.8540243 , -0.32566985,\n",
       "            0.84779954,  0.07121956],\n",
       "          [-1.4126929 , -0.15799896, -1.3440917 ,  2.9920814 , -2.2158043 ,\n",
       "            4.2477455 , -0.0525322 , -0.39021033,  2.9901516 ,  0.6527511 ,\n",
       "           -1.313525  ,  3.6280985 , -0.4488537 , -2.6703975 ,  2.6963742 ,\n",
       "           -0.7880479 ,  0.9972759 , -0.6218758 , -1.1259191 ,  0.98777425,\n",
       "           -1.6288292 ,  2.8804822 ,  4.318639  , -2.0661788 ,  1.9881058 ,\n",
       "            0.35162246, -1.9958018 ,  0.363903  , -0.48686892, -0.7508856 ,\n",
       "            0.16227874,  0.32462674]], dtype=float32)>]],\n",
       " 'weight': [[array([[ 0.07527563, -0.37176153, -0.09289355,  0.27553496,  0.3313712 ,\n",
       "           -0.39164564,  0.15677565, -0.37789753,  0.1106617 , -0.0241173 ,\n",
       "           -0.3825873 , -0.12847163, -0.3256014 , -0.2902194 , -0.2662766 ,\n",
       "            0.39008328,  0.14718705, -0.04775401,  0.09774077, -0.26658073,\n",
       "           -0.36928147,  0.03252091,  0.1435913 , -0.20028563,  0.01144385,\n",
       "            0.15980385, -0.16673449,  0.3930814 ,  0.03190425, -0.00263313,\n",
       "            0.11595693, -0.33562312],\n",
       "          [-0.40596154, -0.35551345, -0.40085635, -0.23400249, -0.30438632,\n",
       "           -0.18232429, -0.03830465, -0.24212499,  0.1321273 , -0.27128315,\n",
       "           -0.11593986,  0.15452108,  0.15426347,  0.3090192 ,  0.1248987 ,\n",
       "           -0.2758562 , -0.26469767, -0.10665747, -0.3643543 , -0.3256537 ,\n",
       "           -0.33706188, -0.09195241,  0.36751354, -0.11683878, -0.31230947,\n",
       "            0.24943918,  0.03997332,  0.01400643, -0.103732  ,  0.33416545,\n",
       "            0.09120074, -0.05970512]], dtype=float32),\n",
       "   array([ 1.39341885e-02,  5.06178662e-03, -3.99277313e-03,  8.47125147e-03,\n",
       "          -1.57052856e-02,  2.45344527e-02,  1.57787111e-02,  1.05474237e-02,\n",
       "           1.32972877e-02,  5.48226573e-03,  9.19191819e-03,  1.74513236e-02,\n",
       "          -2.22056918e-03,  5.80739602e-03,  1.54937673e-02, -2.61787279e-03,\n",
       "           7.19513046e-05,  1.05843823e-02,  6.47247722e-03,  8.95232987e-03,\n",
       "           4.20104479e-03,  1.35233961e-02,  2.30032834e-03,  1.57455653e-02,\n",
       "           1.37279276e-02,  3.08919745e-03,  1.07646678e-02, -8.39838199e-03,\n",
       "           1.46085732e-02,  4.24993224e-03,  1.13125732e-02,  1.25427023e-02],\n",
       "         dtype=float32)],\n",
       "  [array([[-0.26609355],\n",
       "          [ 0.01148633],\n",
       "          [-0.37933078],\n",
       "          [-0.29839444],\n",
       "          [ 0.26299426],\n",
       "          [-0.03119391],\n",
       "          [ 0.0912464 ],\n",
       "          [ 0.319387  ],\n",
       "          [-0.2308717 ],\n",
       "          [-0.37313128],\n",
       "          [ 0.29447502],\n",
       "          [ 0.1441736 ],\n",
       "          [-0.1895217 ],\n",
       "          [-0.00495066],\n",
       "          [-0.1720966 ],\n",
       "          [ 0.23511755],\n",
       "          [-0.21301438],\n",
       "          [-0.141348  ],\n",
       "          [-0.3849022 ],\n",
       "          [-0.33685946],\n",
       "          [-0.09254304],\n",
       "          [ 0.03327327],\n",
       "          [-0.24102926],\n",
       "          [ 0.27244294],\n",
       "          [-0.35696784],\n",
       "          [ 0.21826957],\n",
       "          [ 0.23082837],\n",
       "          [ 0.09316399],\n",
       "          [-0.28469023],\n",
       "          [-0.34894547],\n",
       "          [ 0.08828211],\n",
       "          [-0.30931824]], dtype=float32),\n",
       "   array([0.01931992], dtype=float32)]]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c30e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
