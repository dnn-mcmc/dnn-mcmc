{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee31893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]    \n",
    "    return t\n",
    "\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10, lr=1e-3):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y, is_gibbs = False, is_hmc = False):\n",
    "        \n",
    "        # get current state\n",
    "        if is_hmc:\n",
    "            h_current = tf.split(h, self.hidden_layer_sizes, axis = 1)\n",
    "        else:    \n",
    "            h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_current = convert2_zero_one(h_current)\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            logits = layer(pv)\n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(labels = cv, logits = logits)\n",
    "            if not is_gibbs:\n",
    "                ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, burnin, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob(x, v, y, is_hmc = True),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps = int(burnin * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_kernel, is_update_kernel = True):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat(h_current, axis = 1)\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_burnin_steps = 0\n",
    "        num_results = 1\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "    \n",
    "        # Generate new states of chains\n",
    "        #h_state = rerange(samples[0][0])\n",
    "        h_state = samples[0][0]\n",
    "        h_new = tf.split(h_state, self.hidden_layer_sizes, axis = 1) \n",
    "        \n",
    "        # Update the kernel if necesssary\n",
    "        if is_update_kernel:\n",
    "            new_step_size = samples[2].new_step_size.numpy()\n",
    "            ker_new = self.generate_hmc_kernel(x, y, new_step_size)\n",
    "            return(h_new, ker_new)\n",
    "        else:\n",
    "            return h_new\n",
    "    \n",
    "    def update_weights(self, x, h, y, is_gibbs = False):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y, is_gibbs = is_gibbs))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, tf.float32), logits = logits)\n",
    "        \n",
    "        return tf.reduce_sum(loss, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d697ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(size, dat_train, dat_val, epochs, burnin = 500):\n",
    "    '''\n",
    "    HMC training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    target_val = np.concatenate([target for data, target in dat_val.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start HMC\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs = 1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    kernels = [model.generate_hmc_kernel(data, target) for data, target in dat_train]  \n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start HMC Burning\")\n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "        \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.propose_new_state_hamiltonian(data, network[bs], target, kernels[bs]))\n",
    "            burnin_loss += -1 * tf.reduce_sum(model.target_log_prob(data, network[bs], target))\n",
    "    \n",
    "        network, kernels = zip(*res)\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    # Training\n",
    "    print(\"Start HMC Training\")\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target)\n",
    "            network = [model.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel = False) \\\n",
    "                       for (x, y), net, ker in zip(dat_train, network, kernels)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        train_loss /= (bs + 1)\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc)        \n",
    "        \n",
    "        # validate\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_val):\n",
    "            val_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        val_loss /= (bs + 1)\n",
    "        val_losses.append(val_loss)  \n",
    "        \n",
    "        val_preds = [model.get_predictions(data) for data, target in dat_val]\n",
    "        val_acc = accuracy_score(np.concatenate(val_preds), target_val)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f - val_loss: %.4f - val_acc: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"train_acc\": train_accs, \"train_loss\": train_losses,\n",
    "                             \"val_acc\": val_accs, \"val_loss\": val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X, Y = make_moons(200, noise = 0.3)\n",
    "\n",
    "# Split into test and training data\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, random_state=73)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyperparameters of HMC kernel\n",
    "\n",
    "step_sizes = [-3, -2.5, -2, -1, 0]\n",
    "leapfrog = [2, 3, 4, 5]\n",
    "model_size = [32]\n",
    "\n",
    "for size in model_size:\n",
    "    \n",
    "    epochs = 1000\n",
    "    burnin = 500\n",
    "    burnin_loss_hmc, time_hmc, history_hmc = hmc(size, train_ds, val_ds, epochs, burnin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "546fe92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3. , -2.5, -2. , -1.5, -1. , -0.5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range(-6,0)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f5b5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
