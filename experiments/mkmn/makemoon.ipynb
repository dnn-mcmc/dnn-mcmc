{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ba2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d37b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]    \n",
    "    return t\n",
    "\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60269e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10, lr=1e-3):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y, is_gibbs = False, is_hmc = False):\n",
    "        \n",
    "        # get current state\n",
    "        if is_hmc:\n",
    "            h_current = tf.split(h, self.hidden_layer_sizes, axis = 1)\n",
    "        else:    \n",
    "            h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_current = convert2_zero_one(h_current)\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            logits = layer(pv)\n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(labels = cv, logits = logits)\n",
    "            if not is_gibbs:\n",
    "                ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def gibbs_new_state(self, x, h, y):\n",
    "        \n",
    "        '''\n",
    "            generate a new state for the network node by node in Gibbs setting.\n",
    "        '''\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "\n",
    "            # node by node\n",
    "            \n",
    "            nodes = tf.transpose(cv)\n",
    "            prob_parents = tm.sigmoid(in_layer(pv))\n",
    "            \n",
    "            out_layer_weights = out_layer.get_weights()[0]\n",
    "            \n",
    "            next_logits = out_layer(cv)\n",
    "            \n",
    "            new_layer = []\n",
    "            \n",
    "            for j, node in enumerate(nodes):\n",
    "                \n",
    "                # get info for current node (i, j)\n",
    "                \n",
    "                prob_parents_j = prob_parents[:, j]\n",
    "                out_layer_weights_j = out_layer_weights[j]\n",
    "                \n",
    "                # calculate logits and logprob for node is 0 or 1\n",
    "                next_logits_if_node_0 = next_logits[:, :] - node[:, None] * out_layer_weights_j[None, :]\n",
    "                next_logits_if_node_1 = next_logits[:, :] + (1 - node[:, None]) * out_layer_weights_j[None, :]\n",
    "                \n",
    "                #print(next_logits_if_node_0, next_logits_if_node_1)\n",
    "                \n",
    "                logprob_children_if_node_0 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_0), axis = -1)\n",
    "                \n",
    "                logprob_children_if_node_1 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_1), axis = -1)\n",
    "                \n",
    "                # calculate prob for node (i, j)\n",
    "                prob_0 = (1 - prob_parents_j) * tm.exp(logprob_children_if_node_0)\n",
    "                prob_1 = prob_parents_j * tm.exp(logprob_children_if_node_1)\n",
    "                prob_j = prob_1 / (prob_1 + prob_0)\n",
    "            \n",
    "                # sample new state with prob_j for node (i, j)\n",
    "                new_node = tfp.distributions.Bernoulli(probs = prob_j).sample() # MAY BE SLOW\n",
    "                \n",
    "                # update nodes and logits for following calculation\n",
    "                new_node_casted = tf.cast(new_node, dtype = \"float32\")\n",
    "                next_logits = next_logits_if_node_0 * (1 - new_node_casted)[:, None] \\\n",
    "                            + next_logits_if_node_1 * new_node_casted[:, None] \n",
    "                \n",
    "                # keep track of new node values (in prev/curr/next_vals and h_new)\n",
    "                new_layer.append(new_node)\n",
    "           \n",
    "            new_layer = tf.transpose(new_layer)\n",
    "            h_current[i] = new_layer\n",
    "            prev_vals = [x] + h_current[:-1]\n",
    "            curr_vals = h_current\n",
    "            next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        return h_current\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob(x, v, y, is_hmc = True),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_kernel, is_update_kernel = True):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat(h_current, axis = 1)\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_burnin_steps = 0\n",
    "        num_results = 1\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "    \n",
    "        # Generate new states of chains\n",
    "        #h_state = rerange(samples[0][0])\n",
    "        h_state = samples[0][0]\n",
    "        h_new = tf.split(h_state, self.hidden_layer_sizes, axis = 1) \n",
    "        \n",
    "        # Update the kernel if necesssary\n",
    "        if is_update_kernel:\n",
    "            new_step_size = samples[2].new_step_size.numpy()\n",
    "            ker_new = self.generate_hmc_kernel(x, y, new_step_size)\n",
    "            return(h_new, ker_new)\n",
    "        else:\n",
    "            return h_new\n",
    "    \n",
    "    def update_weights(self, x, h, y, is_gibbs = False):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y, is_gibbs = is_gibbs))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, tf.float32), logits = logits)\n",
    "        \n",
    "        return tf.reduce_sum(loss, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca7975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_backprop(size, dat_train, dat_val, epochs):\n",
    "    '''\n",
    "    Standard Backpropogation training\n",
    "    '''\n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    print(\"Start Standard Backprop\")\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(2,)),\n",
    "            layers.Dense(size, activation = \"sigmoid\"),\n",
    "            layers.Dense(1, activation = \"sigmoid\")\n",
    "        ]\n",
    "    )   \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    st = time.time()\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "    history = model.fit(dat_train, batch_size = batch_size, epochs = epochs, validation_data = dat_val)\n",
    "    train_time = time.time() - st\n",
    "    \n",
    "    return train_time, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63cc2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(size, dat_train, dat_val, epochs, burnin = 500):\n",
    "    '''\n",
    "    HMC training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    target_val = np.concatenate([target for data, target in dat_val.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start HMC\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs = 1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    kernels = [model.generate_hmc_kernel(data, target) for data, target in dat_train]\n",
    "    \n",
    "    bs_train = len(network)\n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start HMC Burning\")\n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "        \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.propose_new_state_hamiltonian(data, network[bs], target, kernels[bs]))\n",
    "            burnin_loss += -1 * tf.reduce_mean(model.target_log_prob(data, network[bs], target))\n",
    "    \n",
    "        network, kernels = zip(*res)\n",
    "        burnin_losses.append(burnin_loss / bs_cnt)\n",
    "    \n",
    "    # Training\n",
    "    print(\"Start HMC Training\")\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target)\n",
    "            network = [model.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel = False) \\\n",
    "                       for (x, y), net, ker in zip(dat_train, network, kernels)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        train_loss /= bs_cnt\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc)        \n",
    "        \n",
    "        # validate\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_val):\n",
    "            val_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        val_loss /= bs\n",
    "        val_losses.append(val_loss)  \n",
    "        \n",
    "        val_preds = [model.get_predictions(data) for data, target in dat_val]\n",
    "        val_acc = accuracy_score(np.concatenate(val_preds), target_val)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f - val_loss: %.4f - val_acc: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"train_acc\": train_accs, \"train_loss\": train_losses,\n",
    "                             \"val_acc\": val_accs, \"val_loss\": val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e48a2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(size, dat_train, dat_val, epochs, burnin = 500):\n",
    "    '''\n",
    "    Gibbs Training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    target_val = np.concatenate([target for data, target in dat_val.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start Gibbs\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs=1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start Gibbs Burning\")    \n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "    \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.gibbs_new_state(data, network[bs], target))\n",
    "            burnin_loss += -1 * tf.reduce_mean(model.target_log_prob(data, network[bs], target, is_gibbs = True))\n",
    "            \n",
    "        network = res\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    # Training\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target, is_gibbs = True)\n",
    "            network = [model.gibbs_new_state(x, net, y) for (x, y), net in zip(dat_train, network)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        train_loss /= (bs + 1)\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc)        \n",
    "        \n",
    "        # validate\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_val):\n",
    "            val_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        val_loss /= (bs + 1)\n",
    "        val_losses.append(val_loss)  \n",
    "        \n",
    "        val_preds = [model.get_predictions(data) for data, target in dat_val]\n",
    "        val_acc = accuracy_score(np.concatenate(val_preds), target_val)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f - val_loss: %.4f - val_acc: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"train_acc\": train_accs, \"train_loss\": train_losses,\n",
    "                             \"val_acc\": val_accs, \"val_loss\": val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1712e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X, Y = make_moons(200, noise = 0.3)\n",
    "\n",
    "# Split into test and training data\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, random_state=73)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7fdfd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Standard Backprop\n",
      "Epoch 1/100\n",
      "1/5 [=====>........................] - ETA: 1s - loss: 0.8713 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 16:09:50.275456: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 27ms/step - loss: 0.7540 - accuracy: 0.4625 - val_loss: 0.7057 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5188 - val_loss: 0.7170 - val_accuracy: 0.4250\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6469 - accuracy: 0.5875 - val_loss: 0.6844 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.8188 - val_loss: 0.6556 - val_accuracy: 0.5750\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5741 - accuracy: 0.8188 - val_loss: 0.6412 - val_accuracy: 0.5750\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5430 - accuracy: 0.8062 - val_loss: 0.6342 - val_accuracy: 0.5750\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5102 - accuracy: 0.8188 - val_loss: 0.6307 - val_accuracy: 0.5750\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.8375 - val_loss: 0.6254 - val_accuracy: 0.5750\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.8375 - val_loss: 0.6192 - val_accuracy: 0.5750\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8375 - val_loss: 0.6163 - val_accuracy: 0.6000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8375 - val_loss: 0.6177 - val_accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3888 - accuracy: 0.8375 - val_loss: 0.6221 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8438 - val_loss: 0.6276 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8500 - val_loss: 0.6326 - val_accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8500 - val_loss: 0.6366 - val_accuracy: 0.6000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3507 - accuracy: 0.8500 - val_loss: 0.6398 - val_accuracy: 0.6000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8500 - val_loss: 0.6420 - val_accuracy: 0.6000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.8500 - val_loss: 0.6433 - val_accuracy: 0.6000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3399 - accuracy: 0.8562 - val_loss: 0.6435 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8562 - val_loss: 0.6425 - val_accuracy: 0.6250\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8687 - val_loss: 0.6405 - val_accuracy: 0.6250\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3341 - accuracy: 0.8687 - val_loss: 0.6379 - val_accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8687 - val_loss: 0.6349 - val_accuracy: 0.6250\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3316 - accuracy: 0.8687 - val_loss: 0.6316 - val_accuracy: 0.6250\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8687 - val_loss: 0.6283 - val_accuracy: 0.6250\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3298 - accuracy: 0.8687 - val_loss: 0.6251 - val_accuracy: 0.6250\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8687 - val_loss: 0.6219 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8750 - val_loss: 0.6188 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8750 - val_loss: 0.6160 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8750 - val_loss: 0.6135 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8750 - val_loss: 0.6112 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8750 - val_loss: 0.6092 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8813 - val_loss: 0.6075 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8813 - val_loss: 0.6060 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8813 - val_loss: 0.6047 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8813 - val_loss: 0.6036 - val_accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8813 - val_loss: 0.6027 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8813 - val_loss: 0.6019 - val_accuracy: 0.6500\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8813 - val_loss: 0.6013 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8813 - val_loss: 0.6007 - val_accuracy: 0.6500\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8813 - val_loss: 0.6003 - val_accuracy: 0.6750\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8813 - val_loss: 0.6000 - val_accuracy: 0.7000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5993 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5992 - val_accuracy: 0.7000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5992 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5993 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5996 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5998 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5999 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6000 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6002 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6003 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6004 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6005 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6006 - val_accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6007 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6010 - val_accuracy: 0.7000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6011 - val_accuracy: 0.7000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6012 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6014 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6015 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6016 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6017 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6019 - val_accuracy: 0.7000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6020 - val_accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6021 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6023 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6024 - val_accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6025 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8750 - val_loss: 0.6027 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8750 - val_loss: 0.6028 - val_accuracy: 0.6750\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8750 - val_loss: 0.6029 - val_accuracy: 0.6750\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8750 - val_loss: 0.6031 - val_accuracy: 0.6750\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8750 - val_loss: 0.6032 - val_accuracy: 0.6750\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8750 - val_loss: 0.6034 - val_accuracy: 0.6750\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8750 - val_loss: 0.6035 - val_accuracy: 0.6750\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8750 - val_loss: 0.6037 - val_accuracy: 0.6750\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8750 - val_loss: 0.6038 - val_accuracy: 0.6750\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8750 - val_loss: 0.6039 - val_accuracy: 0.6750\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8750 - val_loss: 0.6041 - val_accuracy: 0.6750\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8750 - val_loss: 0.6042 - val_accuracy: 0.6750\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8750 - val_loss: 0.6044 - val_accuracy: 0.6750\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8750 - val_loss: 0.6045 - val_accuracy: 0.6750\n",
      "Start HMC\n",
      "5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8y/v1j9n38n4ln56z5jhwrpnnyc0000gn/T/ipykernel_46773/473066649.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtime_bp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_bp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mburnin_loss_hmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_hmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_hmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mburnin_loss_gibbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_gibbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_gibbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgibbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# running multiple times\n",
    "N = 5\n",
    "size = 32\n",
    "epochs = 100\n",
    "burnin = 100\n",
    "\n",
    "res_bp, res_hmc, res_gibbs = [], [], []\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    time_bp, history_bp = standard_backprop(size, train_ds, val_ds, epochs)\n",
    "    burnin_loss_hmc, time_hmc, history_hmc = hmc(size, train_ds, val_ds, epochs, burnin)\n",
    "    burnin_loss_gibbs, time_gibbs, history_gibbs = gibbs(size, train_ds, val_ds, epochs, burnin)\n",
    "    \n",
    "    hist_bp = {\"train_acc\": history_bp.history['accuracy'], \"train_loss\": history_bp.history['loss'], \n",
    "               \"val_acc\": history_bp.history['val_accuracy'], \"val_loss\": history_bp.history['val_loss']}\n",
    "    rbp = {'time': time_bp, 'history': hist_bp}\n",
    "    rhmc = {'time': time_hmc, 'burnin': burnin_loss_hmc, 'history': history_hmc}\n",
    "    rgibbs = {'time': time_gibbs, 'burnin': burnin_loss_gibbs, 'history': history_gibbs}\n",
    "    \n",
    "    res_bp.append(rbp)\n",
    "    res_hmc.append(rhmc)\n",
    "    res_gibbs.append(rgibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50a67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bp_tmp = []\n",
    "for i in range(N):\n",
    "    hist_bp = {\"train_acc\": res_bp[i]['history']['train_acc'], \"train_loss\": res_bp[i]['history']['train_loss'], \n",
    "               \"val_acc\": res_bp[i]['history']['val_acc'], \"val_loss\": res_bp[i]['history']['val_loss']}\n",
    "    r_bp = {\"time\": res_bp[i]['time'], \"history\": hist_bp}\n",
    "    res_bp_tmp.append(r_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce4e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the running times for each method\n",
    "def plot_all(res, method, metric):\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    nrow = 3\n",
    "    ncol = 2\n",
    "    \n",
    "    fig, ax = plt.subplots(nrow, ncol, sharex = True)\n",
    "    fig.suptitle(method + \"_\" + metric)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            if i * ncol + j < N:\n",
    "                ax[i, j].plot(res[i * ncol + j]['history'][metric])\n",
    "                ax[i, j].set_title(f\"Run {i * ncol + j}\")\n",
    "    plt.savefig(method + \"_\" + metric + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "res_bp = res_bp_tmp\n",
    "\n",
    "res_all = [res_bp, res_hmc, res_gibbs]\n",
    "methods = ['bp', 'hmc', 'gibbs']\n",
    "metrics = ['train_acc', 'train_loss', 'val_acc', 'val_loss']\n",
    "for i, method in enumerate(methods):\n",
    "    for metric in metrics:\n",
    "        plot_all(res_all[i], method, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ffabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average curve for each method\n",
    "def cal_avg(res):\n",
    "    \n",
    "    metrics = ['train_acc', 'train_loss', 'val_acc', 'val_loss']\n",
    "    avg = {}\n",
    "    for metric in metrics:\n",
    "        arr_metric = np.zeros((N, epochs))\n",
    "        for i in range(N):\n",
    "            arr_metric[i] = np.array(res[i]['history'][metric])\n",
    "        avg_metric = np.mean(arr_metric, axis = 0)\n",
    "        avg[metric] = avg_metric\n",
    "        \n",
    "    return avg\n",
    "\n",
    "avg_bp = cal_avg(res_bp)\n",
    "avg_hmc = cal_avg(res_hmc)\n",
    "avg_gibbs = cal_avg(res_gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0004225",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_all = [avg_bp, avg_hmc, avg_gibbs]\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "for metric in metrics:\n",
    "    for i, method in enumerate(methods):\n",
    "        plt.plot(avg_all[i][metric], label = method)\n",
    "    plt.title(metric)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric.split(\"_\")[1])\n",
    "    plt.legend()\n",
    "    plt.savefig(\"average_\" + metric + '.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62934e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bp, time_hmc, time_gibbs = [], [], []\n",
    "for i in range(N):\n",
    "    time_bp.append(res_bp[i]['time'])\n",
    "    time_hmc.append(res_hmc[i]['time'])\n",
    "    time_gibbs.append(res_gibbs[i]['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9956fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('time.npy', 'wb') as f:\n",
    "    np.save(f, np.array(res_all))\n",
    "    np.save(f, np.array(time_bp))\n",
    "    np.save(f, np.array(time_hmc))\n",
    "    np.save(f, np.array(time_gibbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6812df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
