{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ba2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d37b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]    \n",
    "    return t\n",
    "\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    '''\n",
    "    computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    [l_lim, u_lim].\n",
    "    cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as tf.where evaluates\n",
    "    both options, regardless of the value of the condition.\n",
    "    '''\n",
    "    \n",
    "    cut_lam = tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tm.log(tm.abs(2.0 * tm.atanh(1 - 2.0 * cut_lam))) - tm.log(tm.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tm.log(2.0) + 4.0 / 3.0 * tm.pow(lam - 0.5, 2) + 104.0 / 45.0 * tm.pow(lam - 0.5, 4)\n",
    "    return tf.where(tm.logical_or(tm.less(lam, l_lim), tm.greater(lam, u_lim)), log_norm, taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60269e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10, lr=1e-3):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y, is_gibbs = False, is_hmc = False):\n",
    "        \n",
    "        # get current state\n",
    "        if is_hmc:\n",
    "            h_current = tf.split(h, self.hidden_layer_sizes, axis = 1)\n",
    "        else:    \n",
    "            h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_current = convert2_zero_one(h_current)\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            logits = layer(pv)\n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(labels = cv, logits = logits)\n",
    "            if not is_gibbs:\n",
    "                ce += cont_bern_log_norm(tf.nn.sigmoid(logits))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def gibbs_new_state(self, x, h, y):\n",
    "        \n",
    "        '''\n",
    "            generate a new state for the network node by node in Gibbs setting.\n",
    "        '''\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "\n",
    "            # node by node\n",
    "            \n",
    "            nodes = tf.transpose(cv)\n",
    "            prob_parents = tm.sigmoid(in_layer(pv))\n",
    "            \n",
    "            out_layer_weights = out_layer.get_weights()[0]\n",
    "            \n",
    "            next_logits = out_layer(cv)\n",
    "            \n",
    "            new_layer = []\n",
    "            \n",
    "            for j, node in enumerate(nodes):\n",
    "                \n",
    "                # get info for current node (i, j)\n",
    "                \n",
    "                prob_parents_j = prob_parents[:, j]\n",
    "                out_layer_weights_j = out_layer_weights[j]\n",
    "                \n",
    "                # calculate logits and logprob for node is 0 or 1\n",
    "                next_logits_if_node_0 = next_logits[:, :] - node[:, None] * out_layer_weights_j[None, :]\n",
    "                next_logits_if_node_1 = next_logits[:, :] + (1 - node[:, None]) * out_layer_weights_j[None, :]\n",
    "                \n",
    "                #print(next_logits_if_node_0, next_logits_if_node_1)\n",
    "                \n",
    "                logprob_children_if_node_0 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_0), axis = -1)\n",
    "                \n",
    "                logprob_children_if_node_1 = -1 * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_1), axis = -1)\n",
    "                \n",
    "                # calculate prob for node (i, j)\n",
    "                prob_0 = (1 - prob_parents_j) * tm.exp(logprob_children_if_node_0)\n",
    "                prob_1 = prob_parents_j * tm.exp(logprob_children_if_node_1)\n",
    "                prob_j = prob_1 / (prob_1 + prob_0)\n",
    "            \n",
    "                # sample new state with prob_j for node (i, j)\n",
    "                new_node = tfp.distributions.Bernoulli(probs = prob_j).sample() # MAY BE SLOW\n",
    "                \n",
    "                # update nodes and logits for following calculation\n",
    "                new_node_casted = tf.cast(new_node, dtype = \"float32\")\n",
    "                next_logits = next_logits_if_node_0 * (1 - new_node_casted)[:, None] \\\n",
    "                            + next_logits_if_node_1 * new_node_casted[:, None] \n",
    "                \n",
    "                # keep track of new node values (in prev/curr/next_vals and h_new)\n",
    "                new_layer.append(new_node)\n",
    "           \n",
    "            new_layer = tf.transpose(new_layer)\n",
    "            h_current[i] = new_layer\n",
    "            prev_vals = [x] + h_current[:-1]\n",
    "            curr_vals = h_current\n",
    "            next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        return h_current\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob(x, v, y, is_hmc = True),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y, hmc_kernel, is_update_kernel = True):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat(h_current, axis = 1)\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_burnin_steps = 0\n",
    "        num_results = 1\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "    \n",
    "        # Generate new states of chains\n",
    "        #h_state = rerange(samples[0][0])\n",
    "        h_state = samples[0][0]\n",
    "        h_new = tf.split(h_state, self.hidden_layer_sizes, axis = 1) \n",
    "        \n",
    "        # Update the kernel if necesssary\n",
    "        if is_update_kernel:\n",
    "            new_step_size = samples[2].new_step_size.numpy()\n",
    "            ker_new = self.generate_hmc_kernel(x, y, new_step_size)\n",
    "            return(h_new, ker_new)\n",
    "        else:\n",
    "            return h_new\n",
    "    \n",
    "    def update_weights(self, x, h, y, is_gibbs = False):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y, is_gibbs = is_gibbs))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "            \n",
    "        logits = self.output_layer(x)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, tf.float32), logits = logits)\n",
    "        \n",
    "        return tf.reduce_sum(loss, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca7975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_backprop(size, dat_train, dat_val, epochs):\n",
    "    '''\n",
    "    Standard Backpropogation training\n",
    "    '''\n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    print(\"Start Standard Backprop\")\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(2,)),\n",
    "            layers.Dense(size, activation = \"sigmoid\"),\n",
    "            layers.Dense(1, activation = \"sigmoid\")\n",
    "        ]\n",
    "    )   \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    st = time.time()\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "    history = model.fit(dat_train, batch_size = batch_size, epochs = epochs, validation_data = dat_val)\n",
    "    train_time = time.time() - st\n",
    "    \n",
    "    return train_time, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63cc2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(size, dat_train, dat_val, epochs, burnin = 500):\n",
    "    '''\n",
    "    HMC training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    target_val = np.concatenate([target for data, target in dat_val.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start HMC\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs = 1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    kernels = [model.generate_hmc_kernel(data, target) for data, target in dat_train]  \n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start HMC Burning\")\n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "        \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.propose_new_state_hamiltonian(data, network[bs], target, kernels[bs]))\n",
    "            burnin_loss += -1 * tf.reduce_mean(model.target_log_prob(data, network[bs], target))\n",
    "    \n",
    "        network, kernels = zip(*res)\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    # Training\n",
    "    print(\"Start HMC Training\")\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target)\n",
    "            network = [model.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel = False) \\\n",
    "                       for (x, y), net, ker in zip(dat_train, network, kernels)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        train_loss /= (bs + 1)\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc)        \n",
    "        \n",
    "        # validate\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_val):\n",
    "            val_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        val_loss /= (bs + 1)\n",
    "        val_losses.append(val_loss)  \n",
    "        \n",
    "        val_preds = [model.get_predictions(data) for data, target in dat_val]\n",
    "        val_acc = accuracy_score(np.concatenate(val_preds), target_val)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f - val_loss: %.4f - val_acc: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"train_acc\": train_accs, \"train_loss\": train_losses,\n",
    "                             \"val_acc\": val_accs, \"val_loss\": val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48a2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(size, dat_train, dat_val, epochs, burnin = 500):\n",
    "    '''\n",
    "    Gibbs Training\n",
    "    '''\n",
    "    # Setting\n",
    "    # Get train labels and val labels\n",
    "    target_train = np.concatenate([target for data, target in dat_train.as_numpy_iterator()])\n",
    "    target_val = np.concatenate([target for data, target in dat_val.as_numpy_iterator()])\n",
    "    \n",
    "    print(\"Start Gibbs\")\n",
    "    model = StochasticMLP(hidden_layer_sizes = [size], n_outputs=1, lr = 0.01)\n",
    "    network = [model.call(data) for data, target in dat_train]\n",
    "    \n",
    "    # Burnin\n",
    "    print(\"Start Gibbs Burning\")    \n",
    "    burnin_losses = []\n",
    "    for i in range(burnin):\n",
    "    \n",
    "        if(i % 100 == 0): print(\"Step %d\" % i)\n",
    "\n",
    "        res = []\n",
    "        burnin_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "            res.append(model.gibbs_new_state(data, network[bs], target))\n",
    "            burnin_loss += -1 * tf.reduce_mean(model.target_log_prob(data, network[bs], target, is_gibbs = True))\n",
    "            \n",
    "        network = res\n",
    "        burnin_losses.append(burnin_loss / (bs + 1))\n",
    "    \n",
    "    # Training\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        for bs, (data, target) in enumerate(dat_train):\n",
    "        \n",
    "            model.update_weights(data, network[bs], target, is_gibbs = True)\n",
    "            network = [model.gibbs_new_state(x, net, y) for (x, y), net in zip(dat_train, network)]\n",
    "            \n",
    "        train_loss = 0.0\n",
    "        for data, target in dat_train:\n",
    "            train_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        train_loss /= (bs + 1)\n",
    "        train_losses.append(train_loss)       \n",
    "        \n",
    "        train_preds = [model.get_predictions(data) for data, target in dat_train]\n",
    "        train_acc = accuracy_score(np.concatenate(train_preds), target_train)\n",
    "        train_accs.append(train_acc)        \n",
    "        \n",
    "        # validate\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        for bs, (data, target) in enumerate(dat_val):\n",
    "            val_loss += tf.reduce_mean(model.get_loss(data, target))\n",
    "        val_loss /= (bs + 1)\n",
    "        val_losses.append(val_loss)  \n",
    "        \n",
    "        val_preds = [model.get_predictions(data) for data, target in dat_val]\n",
    "        val_acc = accuracy_score(np.concatenate(val_preds), target_val)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch %d/%d: - %.4fs/step - train_loss: %.4f - train_acc: %.4f - val_loss: %.4f - val_acc: %.4f\" \n",
    "            % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    return burnin_losses, train_time, {\"train_acc\": train_accs, \"train_loss\": train_losses,\n",
    "                             \"val_acc\": val_accs, \"val_loss\": val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1712e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 22:58:47.580464: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "X, Y = make_moons(200, noise = 0.3)\n",
    "\n",
    "# Split into test and training data\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, random_state=73)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fdfd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Standard Backprop\n",
      "Epoch 1/100\n",
      "1/5 [=====>........................] - ETA: 1s - loss: 0.6554 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 22:58:49.730220: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6698 - accuracy: 0.5312 - val_loss: 0.6757 - val_accuracy: 0.5750\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6042 - accuracy: 0.7937 - val_loss: 0.6447 - val_accuracy: 0.6750\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.7625 - val_loss: 0.6347 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5389 - accuracy: 0.8125 - val_loss: 0.6381 - val_accuracy: 0.5750\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.8438 - val_loss: 0.6367 - val_accuracy: 0.5750\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4719 - accuracy: 0.8562 - val_loss: 0.6227 - val_accuracy: 0.5750\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4427 - accuracy: 0.8438 - val_loss: 0.6115 - val_accuracy: 0.6000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.8438 - val_loss: 0.6086 - val_accuracy: 0.5750\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4005 - accuracy: 0.8438 - val_loss: 0.6116 - val_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8500 - val_loss: 0.6172 - val_accuracy: 0.6000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3708 - accuracy: 0.8500 - val_loss: 0.6223 - val_accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8500 - val_loss: 0.6257 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.8500 - val_loss: 0.6283 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8500 - val_loss: 0.6308 - val_accuracy: 0.6250\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3440 - accuracy: 0.8500 - val_loss: 0.6329 - val_accuracy: 0.6250\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3409 - accuracy: 0.8500 - val_loss: 0.6341 - val_accuracy: 0.6250\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8562 - val_loss: 0.6340 - val_accuracy: 0.6250\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8687 - val_loss: 0.6326 - val_accuracy: 0.6250\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8687 - val_loss: 0.6303 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8687 - val_loss: 0.6274 - val_accuracy: 0.6250\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8687 - val_loss: 0.6244 - val_accuracy: 0.6250\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8687 - val_loss: 0.6213 - val_accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.8750 - val_loss: 0.6182 - val_accuracy: 0.6250\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8750 - val_loss: 0.6151 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3298 - accuracy: 0.8750 - val_loss: 0.6122 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3294 - accuracy: 0.8813 - val_loss: 0.6094 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8813 - val_loss: 0.6070 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3287 - accuracy: 0.8813 - val_loss: 0.6048 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8813 - val_loss: 0.6030 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8813 - val_loss: 0.6014 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8813 - val_loss: 0.6000 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3280 - accuracy: 0.8813 - val_loss: 0.5989 - val_accuracy: 0.6750\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3279 - accuracy: 0.8813 - val_loss: 0.5979 - val_accuracy: 0.6750\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3277 - accuracy: 0.8813 - val_loss: 0.5972 - val_accuracy: 0.6750\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8813 - val_loss: 0.5965 - val_accuracy: 0.6750\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8813 - val_loss: 0.5961 - val_accuracy: 0.6750\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3275 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.6750\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8750 - val_loss: 0.5954 - val_accuracy: 0.6750\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8750 - val_loss: 0.5951 - val_accuracy: 0.6750\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8750 - val_loss: 0.5949 - val_accuracy: 0.6750\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8750 - val_loss: 0.5948 - val_accuracy: 0.6750\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8750 - val_loss: 0.5947 - val_accuracy: 0.6750\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8750 - val_loss: 0.5947 - val_accuracy: 0.6750\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8750 - val_loss: 0.5946 - val_accuracy: 0.6750\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8750 - val_loss: 0.5946 - val_accuracy: 0.6750\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8750 - val_loss: 0.5946 - val_accuracy: 0.6750\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8750 - val_loss: 0.5946 - val_accuracy: 0.6750\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8750 - val_loss: 0.5946 - val_accuracy: 0.6750\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8750 - val_loss: 0.5947 - val_accuracy: 0.6750\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8750 - val_loss: 0.5947 - val_accuracy: 0.6750\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8750 - val_loss: 0.5947 - val_accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8750 - val_loss: 0.5948 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8750 - val_loss: 0.5949 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.5949 - val_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.5950 - val_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8750 - val_loss: 0.5951 - val_accuracy: 0.7000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8750 - val_loss: 0.5952 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8750 - val_loss: 0.5953 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8750 - val_loss: 0.5953 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8750 - val_loss: 0.5954 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.5955 - val_accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.5956 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.5958 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5959 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5960 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5961 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5962 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5963 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5964 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.3119 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5966 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5967 - val_accuracy: 0.7000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5969 - val_accuracy: 0.7000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5970 - val_accuracy: 0.7000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5971 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5972 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5973 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5974 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5975 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5977 - val_accuracy: 0.7000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5978 - val_accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5980 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5983 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5985 - val_accuracy: 0.7000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5986 - val_accuracy: 0.7000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5988 - val_accuracy: 0.7000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5989 - val_accuracy: 0.7000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5993 - val_accuracy: 0.7000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.5998 - val_accuracy: 0.7000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.5999 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6001 - val_accuracy: 0.7000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/100: - 0.6617s/step - train_loss: 0.6800 - train_acc: 0.5188 - val_loss: 0.7021 - val_acc: 0.4250\n",
      "Epoch 2/100: - 0.6646s/step - train_loss: 0.6701 - train_acc: 0.6687 - val_loss: 0.6991 - val_acc: 0.3750\n",
      "Epoch 3/100: - 0.6679s/step - train_loss: 0.6644 - train_acc: 0.6312 - val_loss: 0.6977 - val_acc: 0.4500\n",
      "Epoch 4/100: - 0.6693s/step - train_loss: 0.6581 - train_acc: 0.7188 - val_loss: 0.6922 - val_acc: 0.4750\n",
      "Epoch 5/100: - 0.6641s/step - train_loss: 0.6526 - train_acc: 0.7562 - val_loss: 0.6861 - val_acc: 0.5000\n",
      "Epoch 6/100: - 0.6602s/step - train_loss: 0.6477 - train_acc: 0.7750 - val_loss: 0.6815 - val_acc: 0.5250\n",
      "Epoch 7/100: - 0.6610s/step - train_loss: 0.6436 - train_acc: 0.7812 - val_loss: 0.6780 - val_acc: 0.5500\n",
      "Epoch 8/100: - 0.6613s/step - train_loss: 0.6408 - train_acc: 0.7688 - val_loss: 0.6754 - val_acc: 0.5250\n",
      "Epoch 9/100: - 0.6640s/step - train_loss: 0.6388 - train_acc: 0.7937 - val_loss: 0.6728 - val_acc: 0.5250\n",
      "Epoch 10/100: - 0.6642s/step - train_loss: 0.6371 - train_acc: 0.7875 - val_loss: 0.6705 - val_acc: 0.6000\n",
      "Epoch 11/100: - 0.6649s/step - train_loss: 0.6353 - train_acc: 0.7875 - val_loss: 0.6687 - val_acc: 0.5750\n",
      "Epoch 12/100: - 0.6658s/step - train_loss: 0.6326 - train_acc: 0.7875 - val_loss: 0.6672 - val_acc: 0.6000\n",
      "Epoch 13/100: - 0.6663s/step - train_loss: 0.6294 - train_acc: 0.7937 - val_loss: 0.6656 - val_acc: 0.6000\n",
      "Epoch 14/100: - 0.6667s/step - train_loss: 0.6249 - train_acc: 0.8125 - val_loss: 0.6632 - val_acc: 0.6000\n",
      "Epoch 15/100: - 0.6668s/step - train_loss: 0.6188 - train_acc: 0.8000 - val_loss: 0.6603 - val_acc: 0.5750\n",
      "Epoch 16/100: - 0.6652s/step - train_loss: 0.6131 - train_acc: 0.8250 - val_loss: 0.6575 - val_acc: 0.5500\n",
      "Epoch 17/100: - 0.6642s/step - train_loss: 0.6077 - train_acc: 0.8375 - val_loss: 0.6544 - val_acc: 0.5750\n",
      "Epoch 18/100: - 0.6645s/step - train_loss: 0.6018 - train_acc: 0.8313 - val_loss: 0.6506 - val_acc: 0.5750\n",
      "Epoch 19/100: - 0.6653s/step - train_loss: 0.5963 - train_acc: 0.8313 - val_loss: 0.6462 - val_acc: 0.5750\n",
      "Epoch 20/100: - 0.6653s/step - train_loss: 0.5916 - train_acc: 0.8375 - val_loss: 0.6419 - val_acc: 0.5750\n",
      "Epoch 21/100: - 0.6657s/step - train_loss: 0.5877 - train_acc: 0.8438 - val_loss: 0.6385 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: - 0.6659s/step - train_loss: 0.5850 - train_acc: 0.8438 - val_loss: 0.6358 - val_acc: 0.6000\n",
      "Epoch 23/100: - 0.6668s/step - train_loss: 0.5826 - train_acc: 0.8500 - val_loss: 0.6335 - val_acc: 0.6000\n",
      "Epoch 24/100: - 0.6670s/step - train_loss: 0.5794 - train_acc: 0.8562 - val_loss: 0.6305 - val_acc: 0.5750\n",
      "Epoch 25/100: - 0.6675s/step - train_loss: 0.5763 - train_acc: 0.8500 - val_loss: 0.6277 - val_acc: 0.6250\n",
      "Epoch 26/100: - 0.6676s/step - train_loss: 0.5735 - train_acc: 0.8500 - val_loss: 0.6258 - val_acc: 0.6250\n",
      "Epoch 27/100: - 0.6674s/step - train_loss: 0.5701 - train_acc: 0.8562 - val_loss: 0.6240 - val_acc: 0.6000\n",
      "Epoch 28/100: - 0.6674s/step - train_loss: 0.5661 - train_acc: 0.8625 - val_loss: 0.6217 - val_acc: 0.5750\n",
      "Epoch 29/100: - 0.6674s/step - train_loss: 0.5626 - train_acc: 0.8562 - val_loss: 0.6196 - val_acc: 0.6000\n",
      "Epoch 30/100: - 0.6676s/step - train_loss: 0.5599 - train_acc: 0.8562 - val_loss: 0.6172 - val_acc: 0.6000\n",
      "Epoch 31/100: - 0.6678s/step - train_loss: 0.5572 - train_acc: 0.8688 - val_loss: 0.6140 - val_acc: 0.6000\n",
      "Epoch 32/100: - 0.6678s/step - train_loss: 0.5555 - train_acc: 0.8625 - val_loss: 0.6111 - val_acc: 0.6000\n",
      "Epoch 33/100: - 0.6683s/step - train_loss: 0.5544 - train_acc: 0.8500 - val_loss: 0.6094 - val_acc: 0.6250\n",
      "Epoch 34/100: - 0.6685s/step - train_loss: 0.5529 - train_acc: 0.8500 - val_loss: 0.6084 - val_acc: 0.6250\n",
      "Epoch 35/100: - 0.6686s/step - train_loss: 0.5504 - train_acc: 0.8625 - val_loss: 0.6077 - val_acc: 0.6250\n",
      "Epoch 36/100: - 0.6687s/step - train_loss: 0.5473 - train_acc: 0.8688 - val_loss: 0.6068 - val_acc: 0.6000\n",
      "Epoch 37/100: - 0.6686s/step - train_loss: 0.5439 - train_acc: 0.8750 - val_loss: 0.6048 - val_acc: 0.6000\n",
      "Epoch 38/100: - 0.6681s/step - train_loss: 0.5409 - train_acc: 0.8750 - val_loss: 0.6025 - val_acc: 0.6000\n",
      "Epoch 39/100: - 0.6682s/step - train_loss: 0.5377 - train_acc: 0.8750 - val_loss: 0.5994 - val_acc: 0.6000\n",
      "Epoch 40/100: - 0.6683s/step - train_loss: 0.5343 - train_acc: 0.8688 - val_loss: 0.5960 - val_acc: 0.6250\n",
      "Epoch 41/100: - 0.6684s/step - train_loss: 0.5305 - train_acc: 0.8625 - val_loss: 0.5931 - val_acc: 0.6500\n",
      "Epoch 42/100: - 0.6686s/step - train_loss: 0.5269 - train_acc: 0.8688 - val_loss: 0.5911 - val_acc: 0.6250\n",
      "Epoch 43/100: - 0.6686s/step - train_loss: 0.5235 - train_acc: 0.8750 - val_loss: 0.5898 - val_acc: 0.6250\n",
      "Epoch 44/100: - 0.6686s/step - train_loss: 0.5198 - train_acc: 0.8688 - val_loss: 0.5878 - val_acc: 0.6250\n",
      "Epoch 45/100: - 0.6685s/step - train_loss: 0.5161 - train_acc: 0.8750 - val_loss: 0.5859 - val_acc: 0.6250\n",
      "Epoch 46/100: - 0.6686s/step - train_loss: 0.5128 - train_acc: 0.8688 - val_loss: 0.5840 - val_acc: 0.6250\n",
      "Epoch 47/100: - 0.6687s/step - train_loss: 0.5099 - train_acc: 0.8750 - val_loss: 0.5825 - val_acc: 0.6250\n",
      "Epoch 48/100: - 0.6684s/step - train_loss: 0.5078 - train_acc: 0.8688 - val_loss: 0.5811 - val_acc: 0.6250\n",
      "Epoch 49/100: - 0.6678s/step - train_loss: 0.5064 - train_acc: 0.8750 - val_loss: 0.5796 - val_acc: 0.6250\n",
      "Epoch 50/100: - 0.6678s/step - train_loss: 0.5056 - train_acc: 0.8750 - val_loss: 0.5787 - val_acc: 0.6250\n",
      "Epoch 51/100: - 0.6679s/step - train_loss: 0.5051 - train_acc: 0.8750 - val_loss: 0.5783 - val_acc: 0.6250\n",
      "Epoch 52/100: - 0.6680s/step - train_loss: 0.5048 - train_acc: 0.8688 - val_loss: 0.5783 - val_acc: 0.6500\n",
      "Epoch 53/100: - 0.6680s/step - train_loss: 0.5042 - train_acc: 0.8500 - val_loss: 0.5786 - val_acc: 0.6250\n",
      "Epoch 54/100: - 0.6685s/step - train_loss: 0.5027 - train_acc: 0.8625 - val_loss: 0.5790 - val_acc: 0.6500\n",
      "Epoch 55/100: - 0.6686s/step - train_loss: 0.5006 - train_acc: 0.8625 - val_loss: 0.5797 - val_acc: 0.6250\n",
      "Epoch 56/100: - 0.6685s/step - train_loss: 0.4985 - train_acc: 0.8625 - val_loss: 0.5803 - val_acc: 0.6000\n",
      "Epoch 57/100: - 0.6686s/step - train_loss: 0.4969 - train_acc: 0.8625 - val_loss: 0.5809 - val_acc: 0.6000\n",
      "Epoch 58/100: - 0.6687s/step - train_loss: 0.4952 - train_acc: 0.8562 - val_loss: 0.5809 - val_acc: 0.6250\n",
      "Epoch 59/100: - 0.6683s/step - train_loss: 0.4929 - train_acc: 0.8500 - val_loss: 0.5805 - val_acc: 0.6250\n",
      "Epoch 60/100: - 0.6679s/step - train_loss: 0.4910 - train_acc: 0.8500 - val_loss: 0.5804 - val_acc: 0.6250\n",
      "Epoch 61/100: - 0.6680s/step - train_loss: 0.4887 - train_acc: 0.8500 - val_loss: 0.5800 - val_acc: 0.6250\n",
      "Epoch 62/100: - 0.6688s/step - train_loss: 0.4862 - train_acc: 0.8500 - val_loss: 0.5788 - val_acc: 0.6250\n",
      "Epoch 63/100: - 0.6694s/step - train_loss: 0.4841 - train_acc: 0.8500 - val_loss: 0.5774 - val_acc: 0.6250\n",
      "Epoch 64/100: - 0.6703s/step - train_loss: 0.4821 - train_acc: 0.8500 - val_loss: 0.5764 - val_acc: 0.6250\n",
      "Epoch 65/100: - 0.6717s/step - train_loss: 0.4810 - train_acc: 0.8625 - val_loss: 0.5759 - val_acc: 0.6000\n",
      "Epoch 66/100: - 0.6719s/step - train_loss: 0.4802 - train_acc: 0.8625 - val_loss: 0.5757 - val_acc: 0.6000\n",
      "Epoch 67/100: - 0.6718s/step - train_loss: 0.4801 - train_acc: 0.8500 - val_loss: 0.5762 - val_acc: 0.6000\n",
      "Epoch 68/100: - 0.6717s/step - train_loss: 0.4807 - train_acc: 0.8500 - val_loss: 0.5763 - val_acc: 0.6250\n",
      "Epoch 69/100: - 0.6719s/step - train_loss: 0.4810 - train_acc: 0.8688 - val_loss: 0.5755 - val_acc: 0.6000\n",
      "Epoch 70/100: - 0.6715s/step - train_loss: 0.4806 - train_acc: 0.8625 - val_loss: 0.5745 - val_acc: 0.6000\n",
      "Epoch 71/100: - 0.6714s/step - train_loss: 0.4792 - train_acc: 0.8625 - val_loss: 0.5733 - val_acc: 0.5750\n",
      "Epoch 72/100: - 0.6716s/step - train_loss: 0.4773 - train_acc: 0.8625 - val_loss: 0.5725 - val_acc: 0.6000\n",
      "Epoch 73/100: - 0.6716s/step - train_loss: 0.4749 - train_acc: 0.8562 - val_loss: 0.5720 - val_acc: 0.5750\n",
      "Epoch 74/100: - 0.6723s/step - train_loss: 0.4730 - train_acc: 0.8625 - val_loss: 0.5719 - val_acc: 0.6000\n",
      "Epoch 75/100: - 0.6724s/step - train_loss: 0.4723 - train_acc: 0.8688 - val_loss: 0.5726 - val_acc: 0.6000\n",
      "Epoch 76/100: - 0.6722s/step - train_loss: 0.4717 - train_acc: 0.8562 - val_loss: 0.5735 - val_acc: 0.6000\n",
      "Epoch 77/100: - 0.6721s/step - train_loss: 0.4713 - train_acc: 0.8500 - val_loss: 0.5748 - val_acc: 0.6000\n",
      "Epoch 78/100: - 0.6718s/step - train_loss: 0.4721 - train_acc: 0.8375 - val_loss: 0.5764 - val_acc: 0.6000\n",
      "Epoch 79/100: - 0.6716s/step - train_loss: 0.4717 - train_acc: 0.8500 - val_loss: 0.5756 - val_acc: 0.6000\n",
      "Epoch 80/100: - 0.6712s/step - train_loss: 0.4706 - train_acc: 0.8625 - val_loss: 0.5737 - val_acc: 0.6000\n",
      "Epoch 81/100: - 0.6705s/step - train_loss: 0.4712 - train_acc: 0.8562 - val_loss: 0.5739 - val_acc: 0.6000\n",
      "Epoch 82/100: - 0.6704s/step - train_loss: 0.4717 - train_acc: 0.8500 - val_loss: 0.5760 - val_acc: 0.6250\n",
      "Epoch 83/100: - 0.6702s/step - train_loss: 0.4704 - train_acc: 0.8500 - val_loss: 0.5778 - val_acc: 0.6250\n",
      "Epoch 84/100: - 0.6699s/step - train_loss: 0.4681 - train_acc: 0.8562 - val_loss: 0.5787 - val_acc: 0.5750\n",
      "Epoch 85/100: - 0.6699s/step - train_loss: 0.4664 - train_acc: 0.8500 - val_loss: 0.5797 - val_acc: 0.5750\n",
      "Epoch 86/100: - 0.6696s/step - train_loss: 0.4669 - train_acc: 0.8375 - val_loss: 0.5817 - val_acc: 0.5750\n",
      "Epoch 87/100: - 0.6693s/step - train_loss: 0.4684 - train_acc: 0.8375 - val_loss: 0.5827 - val_acc: 0.6000\n",
      "Epoch 88/100: - 0.6690s/step - train_loss: 0.4697 - train_acc: 0.8375 - val_loss: 0.5823 - val_acc: 0.6000\n",
      "Epoch 89/100: - 0.6688s/step - train_loss: 0.4704 - train_acc: 0.8375 - val_loss: 0.5812 - val_acc: 0.5750\n",
      "Epoch 90/100: - 0.6685s/step - train_loss: 0.4706 - train_acc: 0.8562 - val_loss: 0.5802 - val_acc: 0.5750\n",
      "Epoch 91/100: - 0.6683s/step - train_loss: 0.4703 - train_acc: 0.8625 - val_loss: 0.5800 - val_acc: 0.5750\n",
      "Epoch 92/100: - 0.6676s/step - train_loss: 0.4690 - train_acc: 0.8500 - val_loss: 0.5795 - val_acc: 0.5750\n",
      "Epoch 93/100: - 0.6673s/step - train_loss: 0.4677 - train_acc: 0.8500 - val_loss: 0.5788 - val_acc: 0.5750\n",
      "Epoch 94/100: - 0.6671s/step - train_loss: 0.4672 - train_acc: 0.8562 - val_loss: 0.5784 - val_acc: 0.5750\n",
      "Epoch 95/100: - 0.6668s/step - train_loss: 0.4678 - train_acc: 0.8562 - val_loss: 0.5786 - val_acc: 0.5750\n",
      "Epoch 96/100: - 0.6666s/step - train_loss: 0.4681 - train_acc: 0.8562 - val_loss: 0.5787 - val_acc: 0.5750\n",
      "Epoch 97/100: - 0.6664s/step - train_loss: 0.4672 - train_acc: 0.8562 - val_loss: 0.5781 - val_acc: 0.5750\n",
      "Epoch 98/100: - 0.6663s/step - train_loss: 0.4668 - train_acc: 0.8562 - val_loss: 0.5785 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100: - 0.6664s/step - train_loss: 0.4667 - train_acc: 0.8500 - val_loss: 0.5790 - val_acc: 0.6000\n",
      "Epoch 100/100: - 0.6662s/step - train_loss: 0.4656 - train_acc: 0.8562 - val_loss: 0.5777 - val_acc: 0.6000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/100: - 2.2941s/step - train_loss: 0.6870 - train_acc: 0.5750 - val_loss: 0.6660 - val_acc: 0.6250\n",
      "Epoch 2/100: - 2.3114s/step - train_loss: 0.7013 - train_acc: 0.5188 - val_loss: 0.6688 - val_acc: 0.4250\n",
      "Epoch 3/100: - 2.3061s/step - train_loss: 0.6924 - train_acc: 0.5188 - val_loss: 0.6640 - val_acc: 0.4250\n",
      "Epoch 4/100: - 2.2999s/step - train_loss: 0.6828 - train_acc: 0.5750 - val_loss: 0.6660 - val_acc: 0.5750\n",
      "Epoch 5/100: - 2.3009s/step - train_loss: 0.6840 - train_acc: 0.5938 - val_loss: 0.6746 - val_acc: 0.7000\n",
      "Epoch 6/100: - 2.3048s/step - train_loss: 0.6812 - train_acc: 0.7000 - val_loss: 0.6716 - val_acc: 0.8000\n",
      "Epoch 7/100: - 2.3067s/step - train_loss: 0.6792 - train_acc: 0.5188 - val_loss: 0.6661 - val_acc: 0.4750\n",
      "Epoch 8/100: - 2.3060s/step - train_loss: 0.6777 - train_acc: 0.5250 - val_loss: 0.6651 - val_acc: 0.4250\n",
      "Epoch 9/100: - 2.3015s/step - train_loss: 0.6754 - train_acc: 0.5500 - val_loss: 0.6674 - val_acc: 0.5250\n",
      "Epoch 10/100: - 2.3008s/step - train_loss: 0.6744 - train_acc: 0.7688 - val_loss: 0.6708 - val_acc: 0.6500\n",
      "Epoch 11/100: - 2.2939s/step - train_loss: 0.6737 - train_acc: 0.7812 - val_loss: 0.6718 - val_acc: 0.6500\n",
      "Epoch 12/100: - 2.2958s/step - train_loss: 0.6725 - train_acc: 0.6625 - val_loss: 0.6709 - val_acc: 0.5750\n",
      "Epoch 13/100: - 2.2945s/step - train_loss: 0.6714 - train_acc: 0.5813 - val_loss: 0.6709 - val_acc: 0.5250\n",
      "Epoch 14/100: - 2.2881s/step - train_loss: 0.6706 - train_acc: 0.6813 - val_loss: 0.6720 - val_acc: 0.6000\n",
      "Epoch 15/100: - 2.2863s/step - train_loss: 0.6702 - train_acc: 0.7688 - val_loss: 0.6728 - val_acc: 0.6000\n",
      "Epoch 16/100: - 2.2926s/step - train_loss: 0.6697 - train_acc: 0.5813 - val_loss: 0.6719 - val_acc: 0.5250\n",
      "Epoch 17/100: - 2.2931s/step - train_loss: 0.6687 - train_acc: 0.5625 - val_loss: 0.6714 - val_acc: 0.5000\n",
      "Epoch 18/100: - 2.2937s/step - train_loss: 0.6684 - train_acc: 0.7000 - val_loss: 0.6731 - val_acc: 0.5750\n",
      "Epoch 19/100: - 2.2925s/step - train_loss: 0.6687 - train_acc: 0.8250 - val_loss: 0.6750 - val_acc: 0.6250\n",
      "Epoch 20/100: - 2.2866s/step - train_loss: 0.6693 - train_acc: 0.8250 - val_loss: 0.6761 - val_acc: 0.6500\n",
      "Epoch 21/100: - 2.2809s/step - train_loss: 0.6700 - train_acc: 0.6188 - val_loss: 0.6760 - val_acc: 0.5500\n",
      "Epoch 22/100: - 2.2780s/step - train_loss: 0.6715 - train_acc: 0.5563 - val_loss: 0.6771 - val_acc: 0.4750\n",
      "Epoch 23/100: - 2.2788s/step - train_loss: 0.6733 - train_acc: 0.7125 - val_loss: 0.6797 - val_acc: 0.5750\n",
      "Epoch 24/100: - 2.2773s/step - train_loss: 0.6747 - train_acc: 0.7625 - val_loss: 0.6813 - val_acc: 0.6500\n",
      "Epoch 25/100: - 2.2755s/step - train_loss: 0.6746 - train_acc: 0.6937 - val_loss: 0.6802 - val_acc: 0.5500\n",
      "Epoch 26/100: - 2.2731s/step - train_loss: 0.6745 - train_acc: 0.6000 - val_loss: 0.6788 - val_acc: 0.5500\n",
      "Epoch 27/100: - 2.2726s/step - train_loss: 0.6736 - train_acc: 0.6312 - val_loss: 0.6775 - val_acc: 0.5500\n",
      "Epoch 28/100: - 2.2731s/step - train_loss: 0.6726 - train_acc: 0.7312 - val_loss: 0.6767 - val_acc: 0.6000\n",
      "Epoch 29/100: - 2.2728s/step - train_loss: 0.6709 - train_acc: 0.7063 - val_loss: 0.6750 - val_acc: 0.5750\n",
      "Epoch 30/100: - 2.2738s/step - train_loss: 0.6693 - train_acc: 0.6062 - val_loss: 0.6737 - val_acc: 0.5500\n",
      "Epoch 31/100: - 2.2727s/step - train_loss: 0.6674 - train_acc: 0.6875 - val_loss: 0.6727 - val_acc: 0.5500\n",
      "Epoch 32/100: - 2.2734s/step - train_loss: 0.6667 - train_acc: 0.7562 - val_loss: 0.6725 - val_acc: 0.6000\n",
      "Epoch 33/100: - 2.2715s/step - train_loss: 0.6667 - train_acc: 0.6687 - val_loss: 0.6720 - val_acc: 0.5500\n",
      "Epoch 34/100: - 2.2723s/step - train_loss: 0.6662 - train_acc: 0.6875 - val_loss: 0.6717 - val_acc: 0.5500\n",
      "Epoch 35/100: - 2.2712s/step - train_loss: 0.6668 - train_acc: 0.6875 - val_loss: 0.6725 - val_acc: 0.5500\n",
      "Epoch 36/100: - 2.2696s/step - train_loss: 0.6666 - train_acc: 0.7312 - val_loss: 0.6737 - val_acc: 0.6000\n",
      "Epoch 37/100: - 2.2711s/step - train_loss: 0.6660 - train_acc: 0.6625 - val_loss: 0.6738 - val_acc: 0.5500\n",
      "Epoch 38/100: - 2.2704s/step - train_loss: 0.6645 - train_acc: 0.7500 - val_loss: 0.6738 - val_acc: 0.6000\n",
      "Epoch 39/100: - 2.2691s/step - train_loss: 0.6632 - train_acc: 0.8688 - val_loss: 0.6734 - val_acc: 0.6250\n",
      "Epoch 40/100: - 2.2685s/step - train_loss: 0.6623 - train_acc: 0.8688 - val_loss: 0.6719 - val_acc: 0.6750\n",
      "Epoch 41/100: - 2.2694s/step - train_loss: 0.6613 - train_acc: 0.7438 - val_loss: 0.6688 - val_acc: 0.6000\n",
      "Epoch 42/100: - 2.2682s/step - train_loss: 0.6616 - train_acc: 0.7063 - val_loss: 0.6683 - val_acc: 0.5750\n",
      "Epoch 43/100: - 2.2678s/step - train_loss: 0.6616 - train_acc: 0.8250 - val_loss: 0.6689 - val_acc: 0.6250\n",
      "Epoch 44/100: - 2.2657s/step - train_loss: 0.6624 - train_acc: 0.8187 - val_loss: 0.6693 - val_acc: 0.6250\n",
      "Epoch 45/100: - 2.2638s/step - train_loss: 0.6631 - train_acc: 0.6875 - val_loss: 0.6694 - val_acc: 0.5750\n",
      "Epoch 46/100: - 2.2609s/step - train_loss: 0.6631 - train_acc: 0.7125 - val_loss: 0.6699 - val_acc: 0.5750\n",
      "Epoch 47/100: - 2.2596s/step - train_loss: 0.6633 - train_acc: 0.8688 - val_loss: 0.6715 - val_acc: 0.6750\n",
      "Epoch 48/100: - 2.2577s/step - train_loss: 0.6627 - train_acc: 0.8500 - val_loss: 0.6704 - val_acc: 0.6250\n",
      "Epoch 49/100: - 2.2557s/step - train_loss: 0.6612 - train_acc: 0.7875 - val_loss: 0.6681 - val_acc: 0.6000\n",
      "Epoch 50/100: - 2.2537s/step - train_loss: 0.6591 - train_acc: 0.8500 - val_loss: 0.6658 - val_acc: 0.6500\n",
      "Epoch 51/100: - 2.2523s/step - train_loss: 0.6574 - train_acc: 0.8063 - val_loss: 0.6632 - val_acc: 0.6250\n",
      "Epoch 52/100: - 2.2509s/step - train_loss: 0.6573 - train_acc: 0.7500 - val_loss: 0.6616 - val_acc: 0.5750\n",
      "Epoch 53/100: - 2.2487s/step - train_loss: 0.6571 - train_acc: 0.8125 - val_loss: 0.6613 - val_acc: 0.6250\n",
      "Epoch 54/100: - 2.2476s/step - train_loss: 0.6576 - train_acc: 0.8500 - val_loss: 0.6615 - val_acc: 0.7000\n",
      "Epoch 55/100: - 2.2463s/step - train_loss: 0.6577 - train_acc: 0.7500 - val_loss: 0.6615 - val_acc: 0.5750\n",
      "Epoch 56/100: - 2.2472s/step - train_loss: 0.6577 - train_acc: 0.7438 - val_loss: 0.6626 - val_acc: 0.5750\n",
      "Epoch 57/100: - 2.2476s/step - train_loss: 0.6557 - train_acc: 0.8063 - val_loss: 0.6625 - val_acc: 0.6000\n",
      "Epoch 58/100: - 2.2472s/step - train_loss: 0.6539 - train_acc: 0.8812 - val_loss: 0.6630 - val_acc: 0.6750\n",
      "Epoch 59/100: - 2.2463s/step - train_loss: 0.6531 - train_acc: 0.8812 - val_loss: 0.6628 - val_acc: 0.6750\n",
      "Epoch 60/100: - 2.2459s/step - train_loss: 0.6513 - train_acc: 0.8313 - val_loss: 0.6607 - val_acc: 0.6500\n",
      "Epoch 61/100: - 2.2465s/step - train_loss: 0.6493 - train_acc: 0.8313 - val_loss: 0.6595 - val_acc: 0.6500\n",
      "Epoch 62/100: - 2.2463s/step - train_loss: 0.6466 - train_acc: 0.8750 - val_loss: 0.6581 - val_acc: 0.6750\n",
      "Epoch 63/100: - 2.2455s/step - train_loss: 0.6440 - train_acc: 0.8688 - val_loss: 0.6559 - val_acc: 0.6500\n",
      "Epoch 64/100: - 2.2450s/step - train_loss: 0.6439 - train_acc: 0.8625 - val_loss: 0.6553 - val_acc: 0.6750\n",
      "Epoch 65/100: - 2.2445s/step - train_loss: 0.6472 - train_acc: 0.8750 - val_loss: 0.6568 - val_acc: 0.6750\n",
      "Epoch 66/100: - 2.2435s/step - train_loss: 0.6489 - train_acc: 0.8688 - val_loss: 0.6571 - val_acc: 0.7000\n",
      "Epoch 67/100: - 2.2433s/step - train_loss: 0.6488 - train_acc: 0.8250 - val_loss: 0.6571 - val_acc: 0.6250\n",
      "Epoch 68/100: - 2.2429s/step - train_loss: 0.6503 - train_acc: 0.7500 - val_loss: 0.6590 - val_acc: 0.6000\n",
      "Epoch 69/100: - 2.2423s/step - train_loss: 0.6509 - train_acc: 0.8250 - val_loss: 0.6610 - val_acc: 0.6250\n",
      "Epoch 70/100: - 2.2422s/step - train_loss: 0.6503 - train_acc: 0.8250 - val_loss: 0.6623 - val_acc: 0.6500\n",
      "Epoch 71/100: - 2.2417s/step - train_loss: 0.6483 - train_acc: 0.8438 - val_loss: 0.6624 - val_acc: 0.6000\n",
      "Epoch 72/100: - 2.2409s/step - train_loss: 0.6463 - train_acc: 0.8500 - val_loss: 0.6626 - val_acc: 0.6000\n",
      "Epoch 73/100: - 2.2402s/step - train_loss: 0.6444 - train_acc: 0.8438 - val_loss: 0.6625 - val_acc: 0.6250\n",
      "Epoch 74/100: - 2.2402s/step - train_loss: 0.6435 - train_acc: 0.8562 - val_loss: 0.6633 - val_acc: 0.6250\n",
      "Epoch 75/100: - 2.2399s/step - train_loss: 0.6439 - train_acc: 0.8375 - val_loss: 0.6650 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: - 2.2391s/step - train_loss: 0.6441 - train_acc: 0.8500 - val_loss: 0.6633 - val_acc: 0.6000\n",
      "Epoch 77/100: - 2.2388s/step - train_loss: 0.6447 - train_acc: 0.8625 - val_loss: 0.6600 - val_acc: 0.6500\n",
      "Epoch 78/100: - 2.2385s/step - train_loss: 0.6450 - train_acc: 0.8500 - val_loss: 0.6582 - val_acc: 0.6250\n",
      "Epoch 79/100: - 2.2378s/step - train_loss: 0.6430 - train_acc: 0.7750 - val_loss: 0.6567 - val_acc: 0.6000\n",
      "Epoch 80/100: - 2.2374s/step - train_loss: 0.6398 - train_acc: 0.7688 - val_loss: 0.6557 - val_acc: 0.6000\n",
      "Epoch 81/100: - 2.2371s/step - train_loss: 0.6413 - train_acc: 0.8625 - val_loss: 0.6590 - val_acc: 0.6500\n",
      "Epoch 82/100: - 2.2362s/step - train_loss: 0.6435 - train_acc: 0.8625 - val_loss: 0.6599 - val_acc: 0.6500\n",
      "Epoch 83/100: - 2.2360s/step - train_loss: 0.6419 - train_acc: 0.7875 - val_loss: 0.6566 - val_acc: 0.6000\n",
      "Epoch 84/100: - 2.2356s/step - train_loss: 0.6402 - train_acc: 0.8438 - val_loss: 0.6557 - val_acc: 0.6250\n",
      "Epoch 85/100: - 2.2348s/step - train_loss: 0.6408 - train_acc: 0.8812 - val_loss: 0.6567 - val_acc: 0.6500\n",
      "Epoch 86/100: - 2.2337s/step - train_loss: 0.6418 - train_acc: 0.8688 - val_loss: 0.6555 - val_acc: 0.7000\n",
      "Epoch 87/100: - 2.2330s/step - train_loss: 0.6413 - train_acc: 0.8313 - val_loss: 0.6521 - val_acc: 0.6250\n",
      "Epoch 88/100: - 2.2328s/step - train_loss: 0.6398 - train_acc: 0.7750 - val_loss: 0.6512 - val_acc: 0.6000\n",
      "Epoch 89/100: - 2.2318s/step - train_loss: 0.6384 - train_acc: 0.8688 - val_loss: 0.6520 - val_acc: 0.6750\n",
      "Epoch 90/100: - 2.2311s/step - train_loss: 0.6373 - train_acc: 0.8750 - val_loss: 0.6524 - val_acc: 0.7000\n",
      "Epoch 91/100: - 2.2306s/step - train_loss: 0.6350 - train_acc: 0.8688 - val_loss: 0.6504 - val_acc: 0.6750\n",
      "Epoch 92/100: - 2.2297s/step - train_loss: 0.6324 - train_acc: 0.8313 - val_loss: 0.6478 - val_acc: 0.6750\n",
      "Epoch 93/100: - 2.2293s/step - train_loss: 0.6306 - train_acc: 0.8313 - val_loss: 0.6465 - val_acc: 0.6750\n",
      "Epoch 94/100: - 2.2287s/step - train_loss: 0.6309 - train_acc: 0.8313 - val_loss: 0.6471 - val_acc: 0.6500\n",
      "Epoch 95/100: - 2.2281s/step - train_loss: 0.6315 - train_acc: 0.8688 - val_loss: 0.6492 - val_acc: 0.6500\n",
      "Epoch 96/100: - 2.2272s/step - train_loss: 0.6332 - train_acc: 0.8750 - val_loss: 0.6516 - val_acc: 0.6500\n",
      "Epoch 97/100: - 2.2265s/step - train_loss: 0.6333 - train_acc: 0.8625 - val_loss: 0.6513 - val_acc: 0.6250\n",
      "Epoch 98/100: - 2.2260s/step - train_loss: 0.6353 - train_acc: 0.8500 - val_loss: 0.6527 - val_acc: 0.6250\n",
      "Epoch 99/100: - 2.2253s/step - train_loss: 0.6372 - train_acc: 0.8625 - val_loss: 0.6555 - val_acc: 0.6500\n",
      "Epoch 100/100: - 2.2247s/step - train_loss: 0.6366 - train_acc: 0.8688 - val_loss: 0.6550 - val_acc: 0.6500\n",
      "Start Standard Backprop\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.7627 - accuracy: 0.4812 - val_loss: 0.6731 - val_accuracy: 0.6250\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6684 - accuracy: 0.5938 - val_loss: 0.6968 - val_accuracy: 0.4250\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6258 - accuracy: 0.5625 - val_loss: 0.6722 - val_accuracy: 0.5750\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5751 - accuracy: 0.8188 - val_loss: 0.6348 - val_accuracy: 0.6000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5373 - accuracy: 0.8313 - val_loss: 0.6161 - val_accuracy: 0.6250\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.8438 - val_loss: 0.6079 - val_accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4771 - accuracy: 0.8313 - val_loss: 0.6057 - val_accuracy: 0.6000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.8500 - val_loss: 0.6064 - val_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8500 - val_loss: 0.6053 - val_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8500 - val_loss: 0.6041 - val_accuracy: 0.6000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3868 - accuracy: 0.8500 - val_loss: 0.6055 - val_accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3740 - accuracy: 0.8500 - val_loss: 0.6096 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3639 - accuracy: 0.8500 - val_loss: 0.6152 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8500 - val_loss: 0.6207 - val_accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8500 - val_loss: 0.6251 - val_accuracy: 0.6000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8562 - val_loss: 0.6281 - val_accuracy: 0.6250\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8562 - val_loss: 0.6299 - val_accuracy: 0.6250\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8625 - val_loss: 0.6310 - val_accuracy: 0.6250\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8687 - val_loss: 0.6313 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8687 - val_loss: 0.6309 - val_accuracy: 0.6250\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.8687 - val_loss: 0.6298 - val_accuracy: 0.6250\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.8687 - val_loss: 0.6281 - val_accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8687 - val_loss: 0.6260 - val_accuracy: 0.6250\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3304 - accuracy: 0.8687 - val_loss: 0.6236 - val_accuracy: 0.6250\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8687 - val_loss: 0.6212 - val_accuracy: 0.6250\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8750 - val_loss: 0.6188 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8750 - val_loss: 0.6164 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8750 - val_loss: 0.6141 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3277 - accuracy: 0.8750 - val_loss: 0.6120 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8750 - val_loss: 0.6101 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8813 - val_loss: 0.6083 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8813 - val_loss: 0.6067 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8813 - val_loss: 0.6054 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8813 - val_loss: 0.6041 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8813 - val_loss: 0.6031 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8813 - val_loss: 0.6022 - val_accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8813 - val_loss: 0.6015 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8813 - val_loss: 0.6008 - val_accuracy: 0.6500\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8813 - val_loss: 0.6003 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8813 - val_loss: 0.5999 - val_accuracy: 0.6750\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8813 - val_loss: 0.5995 - val_accuracy: 0.6750\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8813 - val_loss: 0.5992 - val_accuracy: 0.6750\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5988 - val_accuracy: 0.7000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5987 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5986 - val_accuracy: 0.7000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5985 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5985 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5985 - val_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5986 - val_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5986 - val_accuracy: 0.7000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5987 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5988 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5988 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5989 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5992 - val_accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5993 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5996 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5998 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5999 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.6000 - val_accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.6001 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6002 - val_accuracy: 0.7000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6004 - val_accuracy: 0.7000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6005 - val_accuracy: 0.7000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6006 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6007 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6008 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6010 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6011 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6012 - val_accuracy: 0.7000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6013 - val_accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6014 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6016 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6017 - val_accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6018 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6019 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6021 - val_accuracy: 0.7000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6022 - val_accuracy: 0.6750\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6023 - val_accuracy: 0.6750\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6025 - val_accuracy: 0.6750\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6026 - val_accuracy: 0.6750\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6027 - val_accuracy: 0.6750\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8750 - val_loss: 0.6029 - val_accuracy: 0.6750\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8750 - val_loss: 0.6030 - val_accuracy: 0.6750\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8750 - val_loss: 0.6031 - val_accuracy: 0.6750\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8750 - val_loss: 0.6033 - val_accuracy: 0.6750\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8750 - val_loss: 0.6034 - val_accuracy: 0.6750\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8750 - val_loss: 0.6036 - val_accuracy: 0.6750\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8750 - val_loss: 0.6037 - val_accuracy: 0.6750\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/100: - 0.6539s/step - train_loss: 0.6600 - train_acc: 0.5188 - val_loss: 0.6708 - val_acc: 0.4250\n",
      "Epoch 2/100: - 0.6541s/step - train_loss: 0.6506 - train_acc: 0.8438 - val_loss: 0.6687 - val_acc: 0.6250\n",
      "Epoch 3/100: - 0.6545s/step - train_loss: 0.6466 - train_acc: 0.8562 - val_loss: 0.6665 - val_acc: 0.5750\n",
      "Epoch 4/100: - 0.6512s/step - train_loss: 0.6429 - train_acc: 0.7688 - val_loss: 0.6617 - val_acc: 0.6500\n",
      "Epoch 5/100: - 0.6526s/step - train_loss: 0.6408 - train_acc: 0.6750 - val_loss: 0.6590 - val_acc: 0.5500\n",
      "Epoch 6/100: - 0.6474s/step - train_loss: 0.6379 - train_acc: 0.7312 - val_loss: 0.6570 - val_acc: 0.5750\n",
      "Epoch 7/100: - 0.6454s/step - train_loss: 0.6346 - train_acc: 0.8125 - val_loss: 0.6550 - val_acc: 0.6250\n",
      "Epoch 8/100: - 0.6419s/step - train_loss: 0.6314 - train_acc: 0.8313 - val_loss: 0.6524 - val_acc: 0.6000\n",
      "Epoch 9/100: - 0.6398s/step - train_loss: 0.6282 - train_acc: 0.7625 - val_loss: 0.6492 - val_acc: 0.6000\n",
      "Epoch 10/100: - 0.6373s/step - train_loss: 0.6254 - train_acc: 0.7438 - val_loss: 0.6468 - val_acc: 0.6000\n",
      "Epoch 11/100: - 0.6360s/step - train_loss: 0.6225 - train_acc: 0.7750 - val_loss: 0.6446 - val_acc: 0.6000\n",
      "Epoch 12/100: - 0.6340s/step - train_loss: 0.6201 - train_acc: 0.8438 - val_loss: 0.6425 - val_acc: 0.6000\n",
      "Epoch 13/100: - 0.6330s/step - train_loss: 0.6181 - train_acc: 0.8500 - val_loss: 0.6401 - val_acc: 0.6250\n",
      "Epoch 14/100: - 0.6315s/step - train_loss: 0.6155 - train_acc: 0.8500 - val_loss: 0.6369 - val_acc: 0.6250\n",
      "Epoch 15/100: - 0.6306s/step - train_loss: 0.6125 - train_acc: 0.8562 - val_loss: 0.6335 - val_acc: 0.6500\n",
      "Epoch 16/100: - 0.6295s/step - train_loss: 0.6088 - train_acc: 0.8625 - val_loss: 0.6297 - val_acc: 0.6500\n",
      "Epoch 17/100: - 0.6288s/step - train_loss: 0.6049 - train_acc: 0.8625 - val_loss: 0.6260 - val_acc: 0.6750\n",
      "Epoch 18/100: - 0.6283s/step - train_loss: 0.6008 - train_acc: 0.8625 - val_loss: 0.6225 - val_acc: 0.6750\n",
      "Epoch 19/100: - 0.6281s/step - train_loss: 0.5954 - train_acc: 0.8750 - val_loss: 0.6187 - val_acc: 0.6750\n",
      "Epoch 20/100: - 0.6276s/step - train_loss: 0.5899 - train_acc: 0.8688 - val_loss: 0.6148 - val_acc: 0.6750\n",
      "Epoch 21/100: - 0.6271s/step - train_loss: 0.5848 - train_acc: 0.8688 - val_loss: 0.6113 - val_acc: 0.6750\n",
      "Epoch 22/100: - 0.6267s/step - train_loss: 0.5808 - train_acc: 0.8688 - val_loss: 0.6084 - val_acc: 0.6750\n",
      "Epoch 23/100: - 0.6264s/step - train_loss: 0.5778 - train_acc: 0.8625 - val_loss: 0.6061 - val_acc: 0.6500\n",
      "Epoch 24/100: - 0.6259s/step - train_loss: 0.5759 - train_acc: 0.8688 - val_loss: 0.6050 - val_acc: 0.6500\n",
      "Epoch 25/100: - 0.6297s/step - train_loss: 0.5736 - train_acc: 0.8625 - val_loss: 0.6039 - val_acc: 0.6750\n",
      "Epoch 26/100: - 0.6292s/step - train_loss: 0.5710 - train_acc: 0.8688 - val_loss: 0.6033 - val_acc: 0.6750\n",
      "Epoch 27/100: - 0.6288s/step - train_loss: 0.5694 - train_acc: 0.8812 - val_loss: 0.6033 - val_acc: 0.6750\n",
      "Epoch 28/100: - 0.6287s/step - train_loss: 0.5681 - train_acc: 0.8812 - val_loss: 0.6030 - val_acc: 0.6750\n",
      "Epoch 29/100: - 0.6293s/step - train_loss: 0.5665 - train_acc: 0.8750 - val_loss: 0.6022 - val_acc: 0.6750\n",
      "Epoch 30/100: - 0.6288s/step - train_loss: 0.5646 - train_acc: 0.8688 - val_loss: 0.6015 - val_acc: 0.6750\n",
      "Epoch 31/100: - 0.6285s/step - train_loss: 0.5630 - train_acc: 0.8688 - val_loss: 0.6015 - val_acc: 0.6750\n",
      "Epoch 32/100: - 0.6281s/step - train_loss: 0.5619 - train_acc: 0.8750 - val_loss: 0.6024 - val_acc: 0.6750\n",
      "Epoch 33/100: - 0.6280s/step - train_loss: 0.5612 - train_acc: 0.8750 - val_loss: 0.6032 - val_acc: 0.6750\n",
      "Epoch 34/100: - 0.6278s/step - train_loss: 0.5606 - train_acc: 0.8688 - val_loss: 0.6033 - val_acc: 0.6500\n",
      "Epoch 35/100: - 0.6275s/step - train_loss: 0.5601 - train_acc: 0.8750 - val_loss: 0.6029 - val_acc: 0.6750\n",
      "Epoch 36/100: - 0.6273s/step - train_loss: 0.5595 - train_acc: 0.8750 - val_loss: 0.6023 - val_acc: 0.6750\n",
      "Epoch 37/100: - 0.6272s/step - train_loss: 0.5589 - train_acc: 0.8812 - val_loss: 0.6021 - val_acc: 0.6750\n",
      "Epoch 38/100: - 0.6274s/step - train_loss: 0.5587 - train_acc: 0.8750 - val_loss: 0.6027 - val_acc: 0.6500\n",
      "Epoch 39/100: - 0.6275s/step - train_loss: 0.5599 - train_acc: 0.8812 - val_loss: 0.6033 - val_acc: 0.6500\n",
      "Epoch 40/100: - 0.6273s/step - train_loss: 0.5603 - train_acc: 0.8812 - val_loss: 0.6031 - val_acc: 0.6750\n",
      "Epoch 41/100: - 0.6275s/step - train_loss: 0.5603 - train_acc: 0.8812 - val_loss: 0.6027 - val_acc: 0.6750\n",
      "Epoch 42/100: - 0.6275s/step - train_loss: 0.5595 - train_acc: 0.8812 - val_loss: 0.6022 - val_acc: 0.6500\n",
      "Epoch 43/100: - 0.6291s/step - train_loss: 0.5579 - train_acc: 0.8750 - val_loss: 0.6006 - val_acc: 0.6500\n",
      "Epoch 44/100: - 0.6307s/step - train_loss: 0.5557 - train_acc: 0.8750 - val_loss: 0.5992 - val_acc: 0.6500\n",
      "Epoch 45/100: - 0.6322s/step - train_loss: 0.5530 - train_acc: 0.8750 - val_loss: 0.5977 - val_acc: 0.6500\n",
      "Epoch 46/100: - 0.6334s/step - train_loss: 0.5502 - train_acc: 0.8812 - val_loss: 0.5960 - val_acc: 0.6500\n",
      "Epoch 47/100: - 0.6337s/step - train_loss: 0.5472 - train_acc: 0.8812 - val_loss: 0.5939 - val_acc: 0.6500\n",
      "Epoch 48/100: - 0.6348s/step - train_loss: 0.5448 - train_acc: 0.8812 - val_loss: 0.5915 - val_acc: 0.6750\n",
      "Epoch 49/100: - 0.6362s/step - train_loss: 0.5425 - train_acc: 0.8750 - val_loss: 0.5890 - val_acc: 0.6500\n",
      "Epoch 50/100: - 0.6364s/step - train_loss: 0.5403 - train_acc: 0.8812 - val_loss: 0.5872 - val_acc: 0.6500\n",
      "Epoch 51/100: - 0.6366s/step - train_loss: 0.5393 - train_acc: 0.8812 - val_loss: 0.5856 - val_acc: 0.6500\n",
      "Epoch 52/100: - 0.6361s/step - train_loss: 0.5399 - train_acc: 0.8750 - val_loss: 0.5849 - val_acc: 0.6750\n",
      "Epoch 53/100: - 0.6362s/step - train_loss: 0.5415 - train_acc: 0.8625 - val_loss: 0.5852 - val_acc: 0.7000\n",
      "Epoch 54/100: - 0.6364s/step - train_loss: 0.5424 - train_acc: 0.8688 - val_loss: 0.5848 - val_acc: 0.7000\n",
      "Epoch 55/100: - 0.6374s/step - train_loss: 0.5423 - train_acc: 0.8750 - val_loss: 0.5841 - val_acc: 0.7000\n",
      "Epoch 56/100: - 0.6383s/step - train_loss: 0.5415 - train_acc: 0.8812 - val_loss: 0.5833 - val_acc: 0.6750\n",
      "Epoch 57/100: - 0.6387s/step - train_loss: 0.5400 - train_acc: 0.8812 - val_loss: 0.5831 - val_acc: 0.6750\n",
      "Epoch 58/100: - 0.6394s/step - train_loss: 0.5374 - train_acc: 0.8750 - val_loss: 0.5830 - val_acc: 0.6750\n",
      "Epoch 59/100: - 0.6405s/step - train_loss: 0.5344 - train_acc: 0.8812 - val_loss: 0.5831 - val_acc: 0.6750\n",
      "Epoch 60/100: - 0.6414s/step - train_loss: 0.5318 - train_acc: 0.8812 - val_loss: 0.5829 - val_acc: 0.6750\n",
      "Epoch 61/100: - 0.6416s/step - train_loss: 0.5294 - train_acc: 0.8812 - val_loss: 0.5824 - val_acc: 0.6500\n",
      "Epoch 62/100: - 0.6419s/step - train_loss: 0.5279 - train_acc: 0.8562 - val_loss: 0.5826 - val_acc: 0.6500\n",
      "Epoch 63/100: - 0.6418s/step - train_loss: 0.5274 - train_acc: 0.8438 - val_loss: 0.5831 - val_acc: 0.6750\n",
      "Epoch 64/100: - 0.6420s/step - train_loss: 0.5259 - train_acc: 0.8438 - val_loss: 0.5820 - val_acc: 0.6750\n",
      "Epoch 65/100: - 0.6421s/step - train_loss: 0.5220 - train_acc: 0.8625 - val_loss: 0.5777 - val_acc: 0.6500\n",
      "Epoch 66/100: - 0.6432s/step - train_loss: 0.5182 - train_acc: 0.8750 - val_loss: 0.5736 - val_acc: 0.6500\n",
      "Epoch 67/100: - 0.6437s/step - train_loss: 0.5155 - train_acc: 0.8750 - val_loss: 0.5720 - val_acc: 0.6750\n",
      "Epoch 68/100: - 0.6440s/step - train_loss: 0.5123 - train_acc: 0.8750 - val_loss: 0.5711 - val_acc: 0.6750\n",
      "Epoch 69/100: - 0.6445s/step - train_loss: 0.5099 - train_acc: 0.8812 - val_loss: 0.5714 - val_acc: 0.6500\n",
      "Epoch 70/100: - 0.6451s/step - train_loss: 0.5082 - train_acc: 0.8688 - val_loss: 0.5717 - val_acc: 0.6500\n",
      "Epoch 71/100: - 0.6454s/step - train_loss: 0.5060 - train_acc: 0.8750 - val_loss: 0.5710 - val_acc: 0.6250\n",
      "Epoch 72/100: - 0.6456s/step - train_loss: 0.5030 - train_acc: 0.8750 - val_loss: 0.5700 - val_acc: 0.6500\n",
      "Epoch 73/100: - 0.6455s/step - train_loss: 0.4991 - train_acc: 0.8688 - val_loss: 0.5691 - val_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100: - 0.6456s/step - train_loss: 0.4956 - train_acc: 0.8750 - val_loss: 0.5692 - val_acc: 0.6250\n",
      "Epoch 75/100: - 0.6464s/step - train_loss: 0.4936 - train_acc: 0.8562 - val_loss: 0.5701 - val_acc: 0.6500\n",
      "Epoch 76/100: - 0.6468s/step - train_loss: 0.4906 - train_acc: 0.8562 - val_loss: 0.5686 - val_acc: 0.6500\n",
      "Epoch 77/100: - 0.6478s/step - train_loss: 0.4863 - train_acc: 0.8625 - val_loss: 0.5644 - val_acc: 0.6250\n",
      "Epoch 78/100: - 0.6480s/step - train_loss: 0.4828 - train_acc: 0.8688 - val_loss: 0.5610 - val_acc: 0.6500\n",
      "Epoch 79/100: - 0.6481s/step - train_loss: 0.4810 - train_acc: 0.8812 - val_loss: 0.5599 - val_acc: 0.6500\n",
      "Epoch 80/100: - 0.6492s/step - train_loss: 0.4796 - train_acc: 0.8750 - val_loss: 0.5602 - val_acc: 0.6250\n",
      "Epoch 81/100: - 0.6497s/step - train_loss: 0.4782 - train_acc: 0.8750 - val_loss: 0.5610 - val_acc: 0.6500\n",
      "Epoch 82/100: - 0.6498s/step - train_loss: 0.4777 - train_acc: 0.8688 - val_loss: 0.5620 - val_acc: 0.6250\n",
      "Epoch 83/100: - 0.6500s/step - train_loss: 0.4780 - train_acc: 0.8688 - val_loss: 0.5624 - val_acc: 0.6250\n",
      "Epoch 84/100: - 0.6498s/step - train_loss: 0.4786 - train_acc: 0.8688 - val_loss: 0.5622 - val_acc: 0.6250\n",
      "Epoch 85/100: - 0.6496s/step - train_loss: 0.4793 - train_acc: 0.8688 - val_loss: 0.5619 - val_acc: 0.6250\n",
      "Epoch 86/100: - 0.6496s/step - train_loss: 0.4793 - train_acc: 0.8688 - val_loss: 0.5610 - val_acc: 0.6250\n",
      "Epoch 87/100: - 0.6496s/step - train_loss: 0.4784 - train_acc: 0.8750 - val_loss: 0.5593 - val_acc: 0.6250\n",
      "Epoch 88/100: - 0.6496s/step - train_loss: 0.4779 - train_acc: 0.8688 - val_loss: 0.5579 - val_acc: 0.6250\n",
      "Epoch 89/100: - 0.6496s/step - train_loss: 0.4771 - train_acc: 0.8750 - val_loss: 0.5572 - val_acc: 0.6250\n",
      "Epoch 90/100: - 0.6496s/step - train_loss: 0.4761 - train_acc: 0.8750 - val_loss: 0.5573 - val_acc: 0.6250\n",
      "Epoch 91/100: - 0.6496s/step - train_loss: 0.4750 - train_acc: 0.8688 - val_loss: 0.5579 - val_acc: 0.6250\n",
      "Epoch 92/100: - 0.6496s/step - train_loss: 0.4735 - train_acc: 0.8625 - val_loss: 0.5585 - val_acc: 0.6250\n",
      "Epoch 93/100: - 0.6502s/step - train_loss: 0.4712 - train_acc: 0.8688 - val_loss: 0.5581 - val_acc: 0.6250\n",
      "Epoch 94/100: - 0.6502s/step - train_loss: 0.4685 - train_acc: 0.8688 - val_loss: 0.5572 - val_acc: 0.6250\n",
      "Epoch 95/100: - 0.6501s/step - train_loss: 0.4658 - train_acc: 0.8688 - val_loss: 0.5563 - val_acc: 0.6250\n",
      "Epoch 96/100: - 0.6498s/step - train_loss: 0.4631 - train_acc: 0.8688 - val_loss: 0.5558 - val_acc: 0.6250\n",
      "Epoch 97/100: - 0.6499s/step - train_loss: 0.4603 - train_acc: 0.8625 - val_loss: 0.5546 - val_acc: 0.6250\n",
      "Epoch 98/100: - 0.6499s/step - train_loss: 0.4579 - train_acc: 0.8688 - val_loss: 0.5528 - val_acc: 0.6250\n",
      "Epoch 99/100: - 0.6499s/step - train_loss: 0.4559 - train_acc: 0.8750 - val_loss: 0.5512 - val_acc: 0.6250\n",
      "Epoch 100/100: - 0.6499s/step - train_loss: 0.4551 - train_acc: 0.8750 - val_loss: 0.5505 - val_acc: 0.6500\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/100: - 2.2760s/step - train_loss: 0.7247 - train_acc: 0.2188 - val_loss: 0.7155 - val_acc: 0.2500\n",
      "Epoch 2/100: - 2.2605s/step - train_loss: 0.7263 - train_acc: 0.5188 - val_loss: 0.7101 - val_acc: 0.4250\n",
      "Epoch 3/100: - 2.3022s/step - train_loss: 0.7131 - train_acc: 0.5188 - val_loss: 0.7028 - val_acc: 0.4250\n",
      "Epoch 4/100: - 2.2942s/step - train_loss: 0.7044 - train_acc: 0.1875 - val_loss: 0.7053 - val_acc: 0.3000\n",
      "Epoch 5/100: - 2.2732s/step - train_loss: 0.7012 - train_acc: 0.4813 - val_loss: 0.7081 - val_acc: 0.5750\n",
      "Epoch 6/100: - 2.2712s/step - train_loss: 0.6924 - train_acc: 0.4813 - val_loss: 0.6970 - val_acc: 0.2000\n",
      "Epoch 7/100: - 2.2646s/step - train_loss: 0.6877 - train_acc: 0.5188 - val_loss: 0.6890 - val_acc: 0.4250\n",
      "Epoch 8/100: - 2.2545s/step - train_loss: 0.6844 - train_acc: 0.5188 - val_loss: 0.6860 - val_acc: 0.4250\n",
      "Epoch 9/100: - 2.2485s/step - train_loss: 0.6810 - train_acc: 0.5188 - val_loss: 0.6868 - val_acc: 0.4250\n",
      "Epoch 10/100: - 2.2430s/step - train_loss: 0.6789 - train_acc: 0.7875 - val_loss: 0.6877 - val_acc: 0.5250\n",
      "Epoch 11/100: - 2.2374s/step - train_loss: 0.6774 - train_acc: 0.7750 - val_loss: 0.6855 - val_acc: 0.5750\n",
      "Epoch 12/100: - 2.2314s/step - train_loss: 0.6770 - train_acc: 0.5312 - val_loss: 0.6829 - val_acc: 0.4250\n",
      "Epoch 13/100: - 2.2296s/step - train_loss: 0.6769 - train_acc: 0.5250 - val_loss: 0.6817 - val_acc: 0.4250\n",
      "Epoch 14/100: - 2.2271s/step - train_loss: 0.6765 - train_acc: 0.5687 - val_loss: 0.6816 - val_acc: 0.4750\n",
      "Epoch 15/100: - 2.2230s/step - train_loss: 0.6766 - train_acc: 0.6438 - val_loss: 0.6812 - val_acc: 0.5500\n",
      "Epoch 16/100: - 2.2238s/step - train_loss: 0.6772 - train_acc: 0.5437 - val_loss: 0.6806 - val_acc: 0.4750\n",
      "Epoch 17/100: - 2.2213s/step - train_loss: 0.6776 - train_acc: 0.5250 - val_loss: 0.6805 - val_acc: 0.4500\n",
      "Epoch 18/100: - 2.2212s/step - train_loss: 0.6767 - train_acc: 0.5563 - val_loss: 0.6799 - val_acc: 0.5000\n",
      "Epoch 19/100: - 2.2333s/step - train_loss: 0.6759 - train_acc: 0.6000 - val_loss: 0.6795 - val_acc: 0.5250\n",
      "Epoch 20/100: - 2.2356s/step - train_loss: 0.6760 - train_acc: 0.6125 - val_loss: 0.6790 - val_acc: 0.5500\n",
      "Epoch 21/100: - 2.2344s/step - train_loss: 0.6757 - train_acc: 0.5813 - val_loss: 0.6780 - val_acc: 0.5250\n",
      "Epoch 22/100: - 2.2330s/step - train_loss: 0.6752 - train_acc: 0.5563 - val_loss: 0.6773 - val_acc: 0.5000\n",
      "Epoch 23/100: - 2.2322s/step - train_loss: 0.6750 - train_acc: 0.5375 - val_loss: 0.6772 - val_acc: 0.5000\n",
      "Epoch 24/100: - 2.2325s/step - train_loss: 0.6757 - train_acc: 0.6375 - val_loss: 0.6784 - val_acc: 0.5500\n",
      "Epoch 25/100: - 2.2303s/step - train_loss: 0.6756 - train_acc: 0.8250 - val_loss: 0.6787 - val_acc: 0.6250\n",
      "Epoch 26/100: - 2.2297s/step - train_loss: 0.6738 - train_acc: 0.8500 - val_loss: 0.6772 - val_acc: 0.7000\n",
      "Epoch 27/100: - 2.2330s/step - train_loss: 0.6735 - train_acc: 0.5813 - val_loss: 0.6758 - val_acc: 0.5250\n",
      "Epoch 28/100: - 2.2322s/step - train_loss: 0.6735 - train_acc: 0.5250 - val_loss: 0.6753 - val_acc: 0.4250\n",
      "Epoch 29/100: - 2.2360s/step - train_loss: 0.6725 - train_acc: 0.5250 - val_loss: 0.6757 - val_acc: 0.4500\n",
      "Epoch 30/100: - 2.2379s/step - train_loss: 0.6713 - train_acc: 0.7125 - val_loss: 0.6765 - val_acc: 0.6000\n",
      "Epoch 31/100: - 2.2380s/step - train_loss: 0.6712 - train_acc: 0.8250 - val_loss: 0.6768 - val_acc: 0.6250\n",
      "Epoch 32/100: - 2.2394s/step - train_loss: 0.6709 - train_acc: 0.8562 - val_loss: 0.6762 - val_acc: 0.6500\n",
      "Epoch 33/100: - 2.2378s/step - train_loss: 0.6709 - train_acc: 0.6937 - val_loss: 0.6747 - val_acc: 0.6000\n",
      "Epoch 34/100: - 2.2358s/step - train_loss: 0.6711 - train_acc: 0.5813 - val_loss: 0.6738 - val_acc: 0.5250\n",
      "Epoch 35/100: - 2.2348s/step - train_loss: 0.6697 - train_acc: 0.6062 - val_loss: 0.6727 - val_acc: 0.5500\n",
      "Epoch 36/100: - 2.2340s/step - train_loss: 0.6683 - train_acc: 0.6562 - val_loss: 0.6718 - val_acc: 0.5500\n",
      "Epoch 37/100: - 2.2325s/step - train_loss: 0.6676 - train_acc: 0.7188 - val_loss: 0.6722 - val_acc: 0.5750\n",
      "Epoch 38/100: - 2.2304s/step - train_loss: 0.6682 - train_acc: 0.8562 - val_loss: 0.6738 - val_acc: 0.6500\n",
      "Epoch 39/100: - 2.2290s/step - train_loss: 0.6684 - train_acc: 0.8375 - val_loss: 0.6740 - val_acc: 0.6500\n",
      "Epoch 40/100: - 2.2282s/step - train_loss: 0.6686 - train_acc: 0.6250 - val_loss: 0.6728 - val_acc: 0.5500\n",
      "Epoch 41/100: - 2.2265s/step - train_loss: 0.6694 - train_acc: 0.6312 - val_loss: 0.6735 - val_acc: 0.5500\n",
      "Epoch 42/100: - 2.2252s/step - train_loss: 0.6708 - train_acc: 0.7750 - val_loss: 0.6750 - val_acc: 0.6000\n",
      "Epoch 43/100: - 2.2242s/step - train_loss: 0.6713 - train_acc: 0.7750 - val_loss: 0.6753 - val_acc: 0.6000\n",
      "Epoch 44/100: - 2.2227s/step - train_loss: 0.6713 - train_acc: 0.6062 - val_loss: 0.6747 - val_acc: 0.5500\n",
      "Epoch 45/100: - 2.2215s/step - train_loss: 0.6711 - train_acc: 0.7125 - val_loss: 0.6754 - val_acc: 0.5750\n",
      "Epoch 46/100: - 2.2205s/step - train_loss: 0.6704 - train_acc: 0.7750 - val_loss: 0.6756 - val_acc: 0.6000\n",
      "Epoch 47/100: - 2.2196s/step - train_loss: 0.6695 - train_acc: 0.7000 - val_loss: 0.6750 - val_acc: 0.5750\n",
      "Epoch 48/100: - 2.2180s/step - train_loss: 0.6690 - train_acc: 0.7125 - val_loss: 0.6750 - val_acc: 0.5750\n",
      "Epoch 49/100: - 2.2173s/step - train_loss: 0.6678 - train_acc: 0.7937 - val_loss: 0.6745 - val_acc: 0.6000\n",
      "Epoch 50/100: - 2.2165s/step - train_loss: 0.6658 - train_acc: 0.8125 - val_loss: 0.6736 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: - 2.2151s/step - train_loss: 0.6646 - train_acc: 0.7438 - val_loss: 0.6727 - val_acc: 0.6000\n",
      "Epoch 52/100: - 2.2146s/step - train_loss: 0.6650 - train_acc: 0.7188 - val_loss: 0.6731 - val_acc: 0.5750\n",
      "Epoch 53/100: - 2.2140s/step - train_loss: 0.6640 - train_acc: 0.7750 - val_loss: 0.6732 - val_acc: 0.6250\n",
      "Epoch 54/100: - 2.2134s/step - train_loss: 0.6635 - train_acc: 0.8125 - val_loss: 0.6739 - val_acc: 0.6250\n",
      "Epoch 55/100: - 2.2146s/step - train_loss: 0.6641 - train_acc: 0.8500 - val_loss: 0.6755 - val_acc: 0.6250\n",
      "Epoch 56/100: - 2.2168s/step - train_loss: 0.6635 - train_acc: 0.8562 - val_loss: 0.6751 - val_acc: 0.6250\n",
      "Epoch 57/100: - 2.2184s/step - train_loss: 0.6630 - train_acc: 0.7625 - val_loss: 0.6735 - val_acc: 0.6500\n",
      "Epoch 58/100: - 2.2177s/step - train_loss: 0.6632 - train_acc: 0.7000 - val_loss: 0.6730 - val_acc: 0.5500\n",
      "Epoch 59/100: - 2.2173s/step - train_loss: 0.6624 - train_acc: 0.7812 - val_loss: 0.6732 - val_acc: 0.6500\n",
      "Epoch 60/100: - 2.2167s/step - train_loss: 0.6607 - train_acc: 0.8688 - val_loss: 0.6726 - val_acc: 0.6250\n",
      "Epoch 61/100: - 2.2156s/step - train_loss: 0.6591 - train_acc: 0.8125 - val_loss: 0.6692 - val_acc: 0.6500\n",
      "Epoch 62/100: - 2.2152s/step - train_loss: 0.6601 - train_acc: 0.6687 - val_loss: 0.6678 - val_acc: 0.5500\n",
      "Epoch 63/100: - 2.2149s/step - train_loss: 0.6614 - train_acc: 0.7125 - val_loss: 0.6687 - val_acc: 0.5500\n",
      "Epoch 64/100: - 2.2140s/step - train_loss: 0.6611 - train_acc: 0.7812 - val_loss: 0.6691 - val_acc: 0.6000\n",
      "Epoch 65/100: - 2.2135s/step - train_loss: 0.6608 - train_acc: 0.8250 - val_loss: 0.6698 - val_acc: 0.6750\n",
      "Epoch 66/100: - 2.2130s/step - train_loss: 0.6606 - train_acc: 0.8625 - val_loss: 0.6700 - val_acc: 0.6500\n",
      "Epoch 67/100: - 2.2126s/step - train_loss: 0.6586 - train_acc: 0.8125 - val_loss: 0.6682 - val_acc: 0.6250\n",
      "Epoch 68/100: - 2.2123s/step - train_loss: 0.6575 - train_acc: 0.7750 - val_loss: 0.6678 - val_acc: 0.6000\n",
      "Epoch 69/100: - 2.2119s/step - train_loss: 0.6591 - train_acc: 0.8562 - val_loss: 0.6695 - val_acc: 0.6500\n",
      "Epoch 70/100: - 2.2115s/step - train_loss: 0.6620 - train_acc: 0.8688 - val_loss: 0.6707 - val_acc: 0.6500\n",
      "Epoch 71/100: - 2.2108s/step - train_loss: 0.6613 - train_acc: 0.7063 - val_loss: 0.6685 - val_acc: 0.5500\n",
      "Epoch 72/100: - 2.2105s/step - train_loss: 0.6591 - train_acc: 0.8000 - val_loss: 0.6674 - val_acc: 0.6000\n",
      "Epoch 73/100: - 2.2102s/step - train_loss: 0.6563 - train_acc: 0.8250 - val_loss: 0.6665 - val_acc: 0.6250\n",
      "Epoch 74/100: - 2.2094s/step - train_loss: 0.6533 - train_acc: 0.8500 - val_loss: 0.6653 - val_acc: 0.6250\n",
      "Epoch 75/100: - 2.2091s/step - train_loss: 0.6510 - train_acc: 0.8625 - val_loss: 0.6645 - val_acc: 0.6250\n",
      "Epoch 76/100: - 2.2088s/step - train_loss: 0.6495 - train_acc: 0.8625 - val_loss: 0.6632 - val_acc: 0.6250\n",
      "Epoch 77/100: - 2.2084s/step - train_loss: 0.6489 - train_acc: 0.7875 - val_loss: 0.6620 - val_acc: 0.6000\n",
      "Epoch 78/100: - 2.2081s/step - train_loss: 0.6489 - train_acc: 0.7875 - val_loss: 0.6630 - val_acc: 0.6250\n",
      "Epoch 79/100: - 2.2083s/step - train_loss: 0.6499 - train_acc: 0.8313 - val_loss: 0.6648 - val_acc: 0.6000\n",
      "Epoch 80/100: - 2.2081s/step - train_loss: 0.6514 - train_acc: 0.8562 - val_loss: 0.6668 - val_acc: 0.6500\n",
      "Epoch 81/100: - 2.2075s/step - train_loss: 0.6509 - train_acc: 0.8438 - val_loss: 0.6660 - val_acc: 0.6250\n",
      "Epoch 82/100: - 2.2077s/step - train_loss: 0.6494 - train_acc: 0.7625 - val_loss: 0.6650 - val_acc: 0.6500\n",
      "Epoch 83/100: - 2.2075s/step - train_loss: 0.6490 - train_acc: 0.7937 - val_loss: 0.6656 - val_acc: 0.6250\n",
      "Epoch 84/100: - 2.2068s/step - train_loss: 0.6510 - train_acc: 0.8500 - val_loss: 0.6680 - val_acc: 0.6250\n",
      "Epoch 85/100: - 2.2066s/step - train_loss: 0.6523 - train_acc: 0.8500 - val_loss: 0.6687 - val_acc: 0.6000\n",
      "Epoch 86/100: - 2.2064s/step - train_loss: 0.6500 - train_acc: 0.8000 - val_loss: 0.6646 - val_acc: 0.6500\n",
      "Epoch 87/100: - 2.2065s/step - train_loss: 0.6489 - train_acc: 0.7125 - val_loss: 0.6629 - val_acc: 0.5750\n",
      "Epoch 88/100: - 2.2073s/step - train_loss: 0.6461 - train_acc: 0.8438 - val_loss: 0.6615 - val_acc: 0.6250\n",
      "Epoch 89/100: - 2.2076s/step - train_loss: 0.6440 - train_acc: 0.8750 - val_loss: 0.6609 - val_acc: 0.6500\n",
      "Epoch 90/100: - 2.2085s/step - train_loss: 0.6420 - train_acc: 0.8688 - val_loss: 0.6582 - val_acc: 0.6500\n",
      "Epoch 91/100: - 2.2091s/step - train_loss: 0.6403 - train_acc: 0.8500 - val_loss: 0.6555 - val_acc: 0.6250\n",
      "Epoch 92/100: - 2.2101s/step - train_loss: 0.6394 - train_acc: 0.8500 - val_loss: 0.6546 - val_acc: 0.6250\n",
      "Epoch 93/100: - 2.2103s/step - train_loss: 0.6396 - train_acc: 0.8750 - val_loss: 0.6554 - val_acc: 0.6750\n",
      "Epoch 94/100: - 2.2109s/step - train_loss: 0.6424 - train_acc: 0.8812 - val_loss: 0.6562 - val_acc: 0.6750\n",
      "Epoch 95/100: - 2.2115s/step - train_loss: 0.6422 - train_acc: 0.8625 - val_loss: 0.6541 - val_acc: 0.6750\n",
      "Epoch 96/100: - 2.2114s/step - train_loss: 0.6422 - train_acc: 0.8187 - val_loss: 0.6535 - val_acc: 0.6250\n",
      "Epoch 97/100: - 2.2109s/step - train_loss: 0.6413 - train_acc: 0.8562 - val_loss: 0.6540 - val_acc: 0.6500\n",
      "Epoch 98/100: - 2.2107s/step - train_loss: 0.6406 - train_acc: 0.8750 - val_loss: 0.6556 - val_acc: 0.6750\n",
      "Epoch 99/100: - 2.2104s/step - train_loss: 0.6381 - train_acc: 0.8625 - val_loss: 0.6540 - val_acc: 0.6250\n",
      "Epoch 100/100: - 2.2099s/step - train_loss: 0.6341 - train_acc: 0.8250 - val_loss: 0.6509 - val_acc: 0.6500\n",
      "Start Standard Backprop\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6798 - accuracy: 0.5688 - val_loss: 0.6986 - val_accuracy: 0.4500\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6221 - accuracy: 0.6625 - val_loss: 0.6744 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5742 - accuracy: 0.8375 - val_loss: 0.6435 - val_accuracy: 0.5750\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5364 - accuracy: 0.8313 - val_loss: 0.6276 - val_accuracy: 0.5750\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.8375 - val_loss: 0.6213 - val_accuracy: 0.5750\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4679 - accuracy: 0.8375 - val_loss: 0.6186 - val_accuracy: 0.5750\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.8375 - val_loss: 0.6152 - val_accuracy: 0.5750\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4137 - accuracy: 0.8438 - val_loss: 0.6126 - val_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8500 - val_loss: 0.6136 - val_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8500 - val_loss: 0.6179 - val_accuracy: 0.6000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8500 - val_loss: 0.6237 - val_accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8500 - val_loss: 0.6290 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8500 - val_loss: 0.6329 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8500 - val_loss: 0.6353 - val_accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3421 - accuracy: 0.8500 - val_loss: 0.6366 - val_accuracy: 0.6250\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8500 - val_loss: 0.6368 - val_accuracy: 0.6250\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8625 - val_loss: 0.6360 - val_accuracy: 0.6250\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8687 - val_loss: 0.6342 - val_accuracy: 0.6250\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8687 - val_loss: 0.6316 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8687 - val_loss: 0.6285 - val_accuracy: 0.6250\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8687 - val_loss: 0.6251 - val_accuracy: 0.6250\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8687 - val_loss: 0.6217 - val_accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3298 - accuracy: 0.8750 - val_loss: 0.6183 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8750 - val_loss: 0.6152 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8750 - val_loss: 0.6123 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3282 - accuracy: 0.8750 - val_loss: 0.6096 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3279 - accuracy: 0.8813 - val_loss: 0.6073 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8813 - val_loss: 0.6052 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8813 - val_loss: 0.6035 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8813 - val_loss: 0.6020 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8813 - val_loss: 0.6008 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8813 - val_loss: 0.5998 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8813 - val_loss: 0.5990 - val_accuracy: 0.6750\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8813 - val_loss: 0.5984 - val_accuracy: 0.6750\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8813 - val_loss: 0.5979 - val_accuracy: 0.6750\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8813 - val_loss: 0.5975 - val_accuracy: 0.6750\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.5972 - val_accuracy: 0.6750\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.5969 - val_accuracy: 0.6750\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8750 - val_loss: 0.5968 - val_accuracy: 0.6750\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8750 - val_loss: 0.5966 - val_accuracy: 0.6750\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.6750\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5966 - val_accuracy: 0.7000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5966 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5967 - val_accuracy: 0.7000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5967 - val_accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5968 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5969 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5970 - val_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5971 - val_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5971 - val_accuracy: 0.7000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5972 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5973 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5974 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5975 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5976 - val_accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5978 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5980 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5982 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5983 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5985 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5986 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5988 - val_accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5989 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5992 - val_accuracy: 0.7000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5996 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5998 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.6000 - val_accuracy: 0.7000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6001 - val_accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6002 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6004 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6005 - val_accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6006 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6007 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6010 - val_accuracy: 0.7000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6011 - val_accuracy: 0.7000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6013 - val_accuracy: 0.7000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6014 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6015 - val_accuracy: 0.7000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6017 - val_accuracy: 0.7000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6018 - val_accuracy: 0.7000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6020 - val_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6021 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6022 - val_accuracy: 0.7000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6024 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6025 - val_accuracy: 0.7000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/100: - 0.6400s/step - train_loss: 0.7137 - train_acc: 0.5188 - val_loss: 0.6981 - val_acc: 0.4250\n",
      "Epoch 2/100: - 0.6433s/step - train_loss: 0.6996 - train_acc: 0.4813 - val_loss: 0.6985 - val_acc: 0.5750\n",
      "Epoch 3/100: - 0.6434s/step - train_loss: 0.6849 - train_acc: 0.4938 - val_loss: 0.6905 - val_acc: 0.5750\n",
      "Epoch 4/100: - 0.6451s/step - train_loss: 0.6711 - train_acc: 0.6750 - val_loss: 0.6791 - val_acc: 0.5500\n",
      "Epoch 5/100: - 0.6453s/step - train_loss: 0.6612 - train_acc: 0.5437 - val_loss: 0.6717 - val_acc: 0.4500\n",
      "Epoch 6/100: - 0.6488s/step - train_loss: 0.6528 - train_acc: 0.6125 - val_loss: 0.6667 - val_acc: 0.5250\n",
      "Epoch 7/100: - 0.6491s/step - train_loss: 0.6448 - train_acc: 0.7500 - val_loss: 0.6629 - val_acc: 0.6250\n",
      "Epoch 8/100: - 0.6484s/step - train_loss: 0.6386 - train_acc: 0.8313 - val_loss: 0.6600 - val_acc: 0.6000\n",
      "Epoch 9/100: - 0.6477s/step - train_loss: 0.6341 - train_acc: 0.8500 - val_loss: 0.6570 - val_acc: 0.6000\n",
      "Epoch 10/100: - 0.6473s/step - train_loss: 0.6309 - train_acc: 0.8187 - val_loss: 0.6543 - val_acc: 0.6000\n",
      "Epoch 11/100: - 0.6465s/step - train_loss: 0.6285 - train_acc: 0.7812 - val_loss: 0.6522 - val_acc: 0.6500\n",
      "Epoch 12/100: - 0.6448s/step - train_loss: 0.6265 - train_acc: 0.7625 - val_loss: 0.6510 - val_acc: 0.6500\n",
      "Epoch 13/100: - 0.6449s/step - train_loss: 0.6242 - train_acc: 0.7750 - val_loss: 0.6503 - val_acc: 0.6500\n",
      "Epoch 14/100: - 0.6452s/step - train_loss: 0.6213 - train_acc: 0.7937 - val_loss: 0.6497 - val_acc: 0.6250\n",
      "Epoch 15/100: - 0.6452s/step - train_loss: 0.6179 - train_acc: 0.7937 - val_loss: 0.6490 - val_acc: 0.6250\n",
      "Epoch 16/100: - 0.6450s/step - train_loss: 0.6134 - train_acc: 0.8187 - val_loss: 0.6481 - val_acc: 0.6000\n",
      "Epoch 17/100: - 0.6451s/step - train_loss: 0.6091 - train_acc: 0.8500 - val_loss: 0.6473 - val_acc: 0.5750\n",
      "Epoch 18/100: - 0.6452s/step - train_loss: 0.6058 - train_acc: 0.8438 - val_loss: 0.6466 - val_acc: 0.5750\n",
      "Epoch 19/100: - 0.6449s/step - train_loss: 0.6032 - train_acc: 0.8375 - val_loss: 0.6459 - val_acc: 0.6000\n",
      "Epoch 20/100: - 0.6451s/step - train_loss: 0.6004 - train_acc: 0.8187 - val_loss: 0.6450 - val_acc: 0.6000\n",
      "Epoch 21/100: - 0.6450s/step - train_loss: 0.5964 - train_acc: 0.8375 - val_loss: 0.6437 - val_acc: 0.5750\n",
      "Epoch 22/100: - 0.6444s/step - train_loss: 0.5920 - train_acc: 0.8313 - val_loss: 0.6419 - val_acc: 0.5750\n",
      "Epoch 23/100: - 0.6433s/step - train_loss: 0.5884 - train_acc: 0.8438 - val_loss: 0.6400 - val_acc: 0.5750\n",
      "Epoch 24/100: - 0.6434s/step - train_loss: 0.5853 - train_acc: 0.8375 - val_loss: 0.6387 - val_acc: 0.5750\n",
      "Epoch 25/100: - 0.6436s/step - train_loss: 0.5817 - train_acc: 0.8375 - val_loss: 0.6372 - val_acc: 0.5750\n",
      "Epoch 26/100: - 0.6444s/step - train_loss: 0.5781 - train_acc: 0.8438 - val_loss: 0.6355 - val_acc: 0.5750\n",
      "Epoch 27/100: - 0.6447s/step - train_loss: 0.5746 - train_acc: 0.8313 - val_loss: 0.6336 - val_acc: 0.5750\n",
      "Epoch 28/100: - 0.6452s/step - train_loss: 0.5710 - train_acc: 0.8313 - val_loss: 0.6316 - val_acc: 0.6000\n",
      "Epoch 29/100: - 0.6458s/step - train_loss: 0.5678 - train_acc: 0.8562 - val_loss: 0.6298 - val_acc: 0.5750\n",
      "Epoch 30/100: - 0.6459s/step - train_loss: 0.5652 - train_acc: 0.8562 - val_loss: 0.6281 - val_acc: 0.5750\n",
      "Epoch 31/100: - 0.6458s/step - train_loss: 0.5628 - train_acc: 0.8562 - val_loss: 0.6270 - val_acc: 0.5750\n",
      "Epoch 32/100: - 0.6459s/step - train_loss: 0.5609 - train_acc: 0.8562 - val_loss: 0.6265 - val_acc: 0.5750\n",
      "Epoch 33/100: - 0.6457s/step - train_loss: 0.5596 - train_acc: 0.8438 - val_loss: 0.6263 - val_acc: 0.5750\n",
      "Epoch 34/100: - 0.6447s/step - train_loss: 0.5584 - train_acc: 0.8250 - val_loss: 0.6263 - val_acc: 0.6000\n",
      "Epoch 35/100: - 0.6447s/step - train_loss: 0.5563 - train_acc: 0.8250 - val_loss: 0.6259 - val_acc: 0.5750\n",
      "Epoch 36/100: - 0.6447s/step - train_loss: 0.5540 - train_acc: 0.8250 - val_loss: 0.6250 - val_acc: 0.5750\n",
      "Epoch 37/100: - 0.6448s/step - train_loss: 0.5508 - train_acc: 0.8313 - val_loss: 0.6232 - val_acc: 0.5750\n",
      "Epoch 38/100: - 0.6448s/step - train_loss: 0.5477 - train_acc: 0.8250 - val_loss: 0.6215 - val_acc: 0.6000\n",
      "Epoch 39/100: - 0.6449s/step - train_loss: 0.5446 - train_acc: 0.8250 - val_loss: 0.6195 - val_acc: 0.6250\n",
      "Epoch 40/100: - 0.6450s/step - train_loss: 0.5414 - train_acc: 0.8313 - val_loss: 0.6176 - val_acc: 0.6250\n",
      "Epoch 41/100: - 0.6449s/step - train_loss: 0.5383 - train_acc: 0.8313 - val_loss: 0.6158 - val_acc: 0.6250\n",
      "Epoch 42/100: - 0.6449s/step - train_loss: 0.5339 - train_acc: 0.8313 - val_loss: 0.6139 - val_acc: 0.6250\n",
      "Epoch 43/100: - 0.6449s/step - train_loss: 0.5292 - train_acc: 0.8438 - val_loss: 0.6124 - val_acc: 0.5750\n",
      "Epoch 44/100: - 0.6449s/step - train_loss: 0.5254 - train_acc: 0.8500 - val_loss: 0.6113 - val_acc: 0.5750\n",
      "Epoch 45/100: - 0.6443s/step - train_loss: 0.5232 - train_acc: 0.8438 - val_loss: 0.6107 - val_acc: 0.5750\n",
      "Epoch 46/100: - 0.6438s/step - train_loss: 0.5213 - train_acc: 0.8438 - val_loss: 0.6097 - val_acc: 0.5750\n",
      "Epoch 47/100: - 0.6438s/step - train_loss: 0.5193 - train_acc: 0.8438 - val_loss: 0.6079 - val_acc: 0.5750\n",
      "Epoch 48/100: - 0.6443s/step - train_loss: 0.5179 - train_acc: 0.8562 - val_loss: 0.6064 - val_acc: 0.5750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: - 0.6452s/step - train_loss: 0.5164 - train_acc: 0.8438 - val_loss: 0.6055 - val_acc: 0.6000\n",
      "Epoch 50/100: - 0.6467s/step - train_loss: 0.5149 - train_acc: 0.8562 - val_loss: 0.6050 - val_acc: 0.5750\n",
      "Epoch 51/100: - 0.6484s/step - train_loss: 0.5137 - train_acc: 0.8562 - val_loss: 0.6051 - val_acc: 0.5750\n",
      "Epoch 52/100: - 0.6493s/step - train_loss: 0.5130 - train_acc: 0.8562 - val_loss: 0.6055 - val_acc: 0.5750\n",
      "Epoch 53/100: - 0.6505s/step - train_loss: 0.5124 - train_acc: 0.8500 - val_loss: 0.6059 - val_acc: 0.5750\n",
      "Epoch 54/100: - 0.6505s/step - train_loss: 0.5119 - train_acc: 0.8375 - val_loss: 0.6056 - val_acc: 0.5750\n",
      "Epoch 55/100: - 0.6504s/step - train_loss: 0.5109 - train_acc: 0.8438 - val_loss: 0.6043 - val_acc: 0.5750\n",
      "Epoch 56/100: - 0.6500s/step - train_loss: 0.5096 - train_acc: 0.8438 - val_loss: 0.6026 - val_acc: 0.5750\n",
      "Epoch 57/100: - 0.6497s/step - train_loss: 0.5086 - train_acc: 0.8562 - val_loss: 0.6017 - val_acc: 0.5750\n",
      "Epoch 58/100: - 0.6496s/step - train_loss: 0.5078 - train_acc: 0.8562 - val_loss: 0.6014 - val_acc: 0.5750\n",
      "Epoch 59/100: - 0.6496s/step - train_loss: 0.5069 - train_acc: 0.8562 - val_loss: 0.6007 - val_acc: 0.5750\n",
      "Epoch 60/100: - 0.6495s/step - train_loss: 0.5049 - train_acc: 0.8500 - val_loss: 0.5996 - val_acc: 0.5750\n",
      "Epoch 61/100: - 0.6496s/step - train_loss: 0.5026 - train_acc: 0.8438 - val_loss: 0.5990 - val_acc: 0.5750\n",
      "Epoch 62/100: - 0.6496s/step - train_loss: 0.4997 - train_acc: 0.8438 - val_loss: 0.5987 - val_acc: 0.5750\n",
      "Epoch 63/100: - 0.6494s/step - train_loss: 0.4966 - train_acc: 0.8438 - val_loss: 0.5986 - val_acc: 0.5750\n",
      "Epoch 64/100: - 0.6494s/step - train_loss: 0.4932 - train_acc: 0.8438 - val_loss: 0.5986 - val_acc: 0.5750\n",
      "Epoch 65/100: - 0.6493s/step - train_loss: 0.4904 - train_acc: 0.8500 - val_loss: 0.5987 - val_acc: 0.5750\n",
      "Epoch 66/100: - 0.6494s/step - train_loss: 0.4879 - train_acc: 0.8562 - val_loss: 0.5987 - val_acc: 0.5750\n",
      "Epoch 67/100: - 0.6490s/step - train_loss: 0.4855 - train_acc: 0.8562 - val_loss: 0.5981 - val_acc: 0.5750\n",
      "Epoch 68/100: - 0.6486s/step - train_loss: 0.4844 - train_acc: 0.8500 - val_loss: 0.5980 - val_acc: 0.5750\n",
      "Epoch 69/100: - 0.6487s/step - train_loss: 0.4842 - train_acc: 0.8375 - val_loss: 0.5986 - val_acc: 0.5750\n",
      "Epoch 70/100: - 0.6485s/step - train_loss: 0.4837 - train_acc: 0.8250 - val_loss: 0.5989 - val_acc: 0.5750\n",
      "Epoch 71/100: - 0.6485s/step - train_loss: 0.4820 - train_acc: 0.8500 - val_loss: 0.5979 - val_acc: 0.5750\n",
      "Epoch 72/100: - 0.6485s/step - train_loss: 0.4802 - train_acc: 0.8562 - val_loss: 0.5966 - val_acc: 0.5750\n",
      "Epoch 73/100: - 0.6487s/step - train_loss: 0.4788 - train_acc: 0.8562 - val_loss: 0.5949 - val_acc: 0.5750\n",
      "Epoch 74/100: - 0.6488s/step - train_loss: 0.4776 - train_acc: 0.8562 - val_loss: 0.5927 - val_acc: 0.5750\n",
      "Epoch 75/100: - 0.6489s/step - train_loss: 0.4771 - train_acc: 0.8562 - val_loss: 0.5903 - val_acc: 0.5750\n",
      "Epoch 76/100: - 0.6489s/step - train_loss: 0.4772 - train_acc: 0.8562 - val_loss: 0.5882 - val_acc: 0.5750\n",
      "Epoch 77/100: - 0.6491s/step - train_loss: 0.4773 - train_acc: 0.8562 - val_loss: 0.5864 - val_acc: 0.5750\n",
      "Epoch 78/100: - 0.6496s/step - train_loss: 0.4773 - train_acc: 0.8562 - val_loss: 0.5853 - val_acc: 0.5750\n",
      "Epoch 79/100: - 0.6496s/step - train_loss: 0.4774 - train_acc: 0.8562 - val_loss: 0.5846 - val_acc: 0.5750\n",
      "Epoch 80/100: - 0.6502s/step - train_loss: 0.4781 - train_acc: 0.8562 - val_loss: 0.5848 - val_acc: 0.6000\n",
      "Epoch 81/100: - 0.6501s/step - train_loss: 0.4783 - train_acc: 0.8500 - val_loss: 0.5850 - val_acc: 0.6000\n",
      "Epoch 82/100: - 0.6502s/step - train_loss: 0.4787 - train_acc: 0.8500 - val_loss: 0.5858 - val_acc: 0.6000\n",
      "Epoch 83/100: - 0.6505s/step - train_loss: 0.4801 - train_acc: 0.8438 - val_loss: 0.5872 - val_acc: 0.6250\n",
      "Epoch 84/100: - 0.6504s/step - train_loss: 0.4810 - train_acc: 0.8375 - val_loss: 0.5880 - val_acc: 0.6250\n",
      "Epoch 85/100: - 0.6504s/step - train_loss: 0.4800 - train_acc: 0.8438 - val_loss: 0.5869 - val_acc: 0.6250\n",
      "Epoch 86/100: - 0.6507s/step - train_loss: 0.4792 - train_acc: 0.8438 - val_loss: 0.5859 - val_acc: 0.6250\n",
      "Epoch 87/100: - 0.6517s/step - train_loss: 0.4796 - train_acc: 0.8438 - val_loss: 0.5856 - val_acc: 0.6250\n",
      "Epoch 88/100: - 0.6531s/step - train_loss: 0.4794 - train_acc: 0.8375 - val_loss: 0.5849 - val_acc: 0.6250\n",
      "Epoch 89/100: - 0.6535s/step - train_loss: 0.4770 - train_acc: 0.8438 - val_loss: 0.5823 - val_acc: 0.6250\n",
      "Epoch 90/100: - 0.6538s/step - train_loss: 0.4726 - train_acc: 0.8500 - val_loss: 0.5777 - val_acc: 0.6250\n",
      "Epoch 91/100: - 0.6542s/step - train_loss: 0.4686 - train_acc: 0.8562 - val_loss: 0.5727 - val_acc: 0.6000\n",
      "Epoch 92/100: - 0.6546s/step - train_loss: 0.4654 - train_acc: 0.8562 - val_loss: 0.5683 - val_acc: 0.6000\n",
      "Epoch 93/100: - 0.6551s/step - train_loss: 0.4636 - train_acc: 0.8625 - val_loss: 0.5647 - val_acc: 0.6000\n",
      "Epoch 94/100: - 0.6556s/step - train_loss: 0.4637 - train_acc: 0.8500 - val_loss: 0.5627 - val_acc: 0.6250\n",
      "Epoch 95/100: - 0.6565s/step - train_loss: 0.4649 - train_acc: 0.8500 - val_loss: 0.5623 - val_acc: 0.6250\n",
      "Epoch 96/100: - 0.6570s/step - train_loss: 0.4656 - train_acc: 0.8500 - val_loss: 0.5623 - val_acc: 0.6250\n",
      "Epoch 97/100: - 0.6575s/step - train_loss: 0.4668 - train_acc: 0.8625 - val_loss: 0.5635 - val_acc: 0.6250\n",
      "Epoch 98/100: - 0.6578s/step - train_loss: 0.4688 - train_acc: 0.8562 - val_loss: 0.5659 - val_acc: 0.6250\n",
      "Epoch 99/100: - 0.6578s/step - train_loss: 0.4720 - train_acc: 0.8438 - val_loss: 0.5701 - val_acc: 0.6250\n",
      "Epoch 100/100: - 0.6578s/step - train_loss: 0.4722 - train_acc: 0.8438 - val_loss: 0.5716 - val_acc: 0.6250\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/100: - 2.2501s/step - train_loss: 0.7049 - train_acc: 0.5125 - val_loss: 0.6775 - val_acc: 0.4250\n",
      "Epoch 2/100: - 2.2490s/step - train_loss: 0.7146 - train_acc: 0.4813 - val_loss: 0.7053 - val_acc: 0.5750\n",
      "Epoch 3/100: - 2.2418s/step - train_loss: 0.7036 - train_acc: 0.4813 - val_loss: 0.6943 - val_acc: 0.5750\n",
      "Epoch 4/100: - 2.2445s/step - train_loss: 0.6930 - train_acc: 0.5125 - val_loss: 0.6754 - val_acc: 0.4250\n",
      "Epoch 5/100: - 2.2475s/step - train_loss: 0.6942 - train_acc: 0.5188 - val_loss: 0.6726 - val_acc: 0.4250\n",
      "Epoch 6/100: - 2.2469s/step - train_loss: 0.6887 - train_acc: 0.5188 - val_loss: 0.6727 - val_acc: 0.4250\n",
      "Epoch 7/100: - 2.2484s/step - train_loss: 0.6851 - train_acc: 0.6937 - val_loss: 0.6783 - val_acc: 0.6750\n",
      "Epoch 8/100: - 2.2491s/step - train_loss: 0.6841 - train_acc: 0.7188 - val_loss: 0.6812 - val_acc: 0.8000\n",
      "Epoch 9/100: - 2.2464s/step - train_loss: 0.6812 - train_acc: 0.6250 - val_loss: 0.6768 - val_acc: 0.5500\n",
      "Epoch 10/100: - 2.2475s/step - train_loss: 0.6788 - train_acc: 0.5250 - val_loss: 0.6735 - val_acc: 0.4250\n",
      "Epoch 11/100: - 2.2482s/step - train_loss: 0.6762 - train_acc: 0.5250 - val_loss: 0.6738 - val_acc: 0.4750\n",
      "Epoch 12/100: - 2.2461s/step - train_loss: 0.6739 - train_acc: 0.6000 - val_loss: 0.6743 - val_acc: 0.5250\n",
      "Epoch 13/100: - 2.2467s/step - train_loss: 0.6718 - train_acc: 0.7688 - val_loss: 0.6741 - val_acc: 0.6000\n",
      "Epoch 14/100: - 2.2472s/step - train_loss: 0.6703 - train_acc: 0.6813 - val_loss: 0.6727 - val_acc: 0.5750\n",
      "Epoch 15/100: - 2.2469s/step - train_loss: 0.6697 - train_acc: 0.6312 - val_loss: 0.6725 - val_acc: 0.5500\n",
      "Epoch 16/100: - 2.2453s/step - train_loss: 0.6700 - train_acc: 0.7500 - val_loss: 0.6736 - val_acc: 0.5750\n",
      "Epoch 17/100: - 2.2461s/step - train_loss: 0.6705 - train_acc: 0.7625 - val_loss: 0.6738 - val_acc: 0.6000\n",
      "Epoch 18/100: - 2.2467s/step - train_loss: 0.6705 - train_acc: 0.6813 - val_loss: 0.6733 - val_acc: 0.5750\n",
      "Epoch 19/100: - 2.2468s/step - train_loss: 0.6706 - train_acc: 0.6562 - val_loss: 0.6735 - val_acc: 0.5500\n",
      "Epoch 20/100: - 2.2472s/step - train_loss: 0.6711 - train_acc: 0.6062 - val_loss: 0.6744 - val_acc: 0.5500\n",
      "Epoch 21/100: - 2.2473s/step - train_loss: 0.6712 - train_acc: 0.6875 - val_loss: 0.6757 - val_acc: 0.5500\n",
      "Epoch 22/100: - 2.2473s/step - train_loss: 0.6711 - train_acc: 0.7125 - val_loss: 0.6768 - val_acc: 0.5500\n",
      "Epoch 23/100: - 2.2475s/step - train_loss: 0.6701 - train_acc: 0.7375 - val_loss: 0.6770 - val_acc: 0.6000\n",
      "Epoch 24/100: - 2.2478s/step - train_loss: 0.6676 - train_acc: 0.6937 - val_loss: 0.6753 - val_acc: 0.5500\n",
      "Epoch 25/100: - 2.2464s/step - train_loss: 0.6661 - train_acc: 0.7312 - val_loss: 0.6750 - val_acc: 0.5750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: - 2.2466s/step - train_loss: 0.6667 - train_acc: 0.8500 - val_loss: 0.6762 - val_acc: 0.6250\n",
      "Epoch 27/100: - 2.2465s/step - train_loss: 0.6685 - train_acc: 0.8625 - val_loss: 0.6769 - val_acc: 0.6250\n",
      "Epoch 28/100: - 2.2443s/step - train_loss: 0.6705 - train_acc: 0.7125 - val_loss: 0.6758 - val_acc: 0.6000\n",
      "Epoch 29/100: - 2.2420s/step - train_loss: 0.6719 - train_acc: 0.5687 - val_loss: 0.6753 - val_acc: 0.5250\n",
      "Epoch 30/100: - 2.2381s/step - train_loss: 0.6720 - train_acc: 0.5750 - val_loss: 0.6749 - val_acc: 0.5250\n",
      "Epoch 31/100: - 2.2341s/step - train_loss: 0.6704 - train_acc: 0.5813 - val_loss: 0.6739 - val_acc: 0.5250\n",
      "Epoch 32/100: - 2.2314s/step - train_loss: 0.6684 - train_acc: 0.6937 - val_loss: 0.6734 - val_acc: 0.5750\n",
      "Epoch 33/100: - 2.2299s/step - train_loss: 0.6672 - train_acc: 0.8500 - val_loss: 0.6736 - val_acc: 0.6250\n",
      "Epoch 34/100: - 2.2274s/step - train_loss: 0.6671 - train_acc: 0.8625 - val_loss: 0.6746 - val_acc: 0.7000\n",
      "Epoch 35/100: - 2.2253s/step - train_loss: 0.6648 - train_acc: 0.8313 - val_loss: 0.6714 - val_acc: 0.6500\n",
      "Epoch 36/100: - 2.2247s/step - train_loss: 0.6632 - train_acc: 0.6875 - val_loss: 0.6688 - val_acc: 0.5500\n",
      "Epoch 37/100: - 2.2227s/step - train_loss: 0.6636 - train_acc: 0.5813 - val_loss: 0.6689 - val_acc: 0.5250\n",
      "Epoch 38/100: - 2.2208s/step - train_loss: 0.6626 - train_acc: 0.8250 - val_loss: 0.6701 - val_acc: 0.6000\n",
      "Epoch 39/100: - 2.2191s/step - train_loss: 0.6618 - train_acc: 0.8625 - val_loss: 0.6705 - val_acc: 0.6500\n",
      "Epoch 40/100: - 2.2175s/step - train_loss: 0.6622 - train_acc: 0.8625 - val_loss: 0.6699 - val_acc: 0.6750\n",
      "Epoch 41/100: - 2.2159s/step - train_loss: 0.6632 - train_acc: 0.7375 - val_loss: 0.6687 - val_acc: 0.5750\n",
      "Epoch 42/100: - 2.2146s/step - train_loss: 0.6635 - train_acc: 0.6062 - val_loss: 0.6684 - val_acc: 0.5500\n",
      "Epoch 43/100: - 2.2135s/step - train_loss: 0.6638 - train_acc: 0.8125 - val_loss: 0.6706 - val_acc: 0.6000\n",
      "Epoch 44/100: - 2.2119s/step - train_loss: 0.6640 - train_acc: 0.8688 - val_loss: 0.6720 - val_acc: 0.6750\n",
      "Epoch 45/100: - 2.2106s/step - train_loss: 0.6632 - train_acc: 0.7562 - val_loss: 0.6694 - val_acc: 0.6000\n",
      "Epoch 46/100: - 2.2095s/step - train_loss: 0.6621 - train_acc: 0.6562 - val_loss: 0.6681 - val_acc: 0.5500\n",
      "Epoch 47/100: - 2.2085s/step - train_loss: 0.6613 - train_acc: 0.8562 - val_loss: 0.6689 - val_acc: 0.6500\n",
      "Epoch 48/100: - 2.2071s/step - train_loss: 0.6614 - train_acc: 0.8625 - val_loss: 0.6693 - val_acc: 0.6750\n",
      "Epoch 49/100: - 2.2059s/step - train_loss: 0.6600 - train_acc: 0.7125 - val_loss: 0.6671 - val_acc: 0.5750\n",
      "Epoch 50/100: - 2.2048s/step - train_loss: 0.6578 - train_acc: 0.7063 - val_loss: 0.6660 - val_acc: 0.5500\n",
      "Epoch 51/100: - 2.2037s/step - train_loss: 0.6560 - train_acc: 0.8313 - val_loss: 0.6667 - val_acc: 0.6500\n",
      "Epoch 52/100: - 2.2026s/step - train_loss: 0.6558 - train_acc: 0.7500 - val_loss: 0.6675 - val_acc: 0.6000\n",
      "Epoch 53/100: - 2.2014s/step - train_loss: 0.6558 - train_acc: 0.6625 - val_loss: 0.6682 - val_acc: 0.5500\n",
      "Epoch 54/100: - 2.2005s/step - train_loss: 0.6557 - train_acc: 0.8500 - val_loss: 0.6707 - val_acc: 0.6000\n",
      "Epoch 55/100: - 2.1996s/step - train_loss: 0.6563 - train_acc: 0.8500 - val_loss: 0.6722 - val_acc: 0.6000\n",
      "Epoch 56/100: - 2.1986s/step - train_loss: 0.6555 - train_acc: 0.8500 - val_loss: 0.6694 - val_acc: 0.6250\n",
      "Epoch 57/100: - 2.1976s/step - train_loss: 0.6558 - train_acc: 0.8438 - val_loss: 0.6675 - val_acc: 0.6000\n",
      "Epoch 58/100: - 2.1968s/step - train_loss: 0.6566 - train_acc: 0.8625 - val_loss: 0.6674 - val_acc: 0.6250\n",
      "Epoch 59/100: - 2.1961s/step - train_loss: 0.6551 - train_acc: 0.8250 - val_loss: 0.6655 - val_acc: 0.6250\n",
      "Epoch 60/100: - 2.1953s/step - train_loss: 0.6526 - train_acc: 0.7937 - val_loss: 0.6638 - val_acc: 0.6000\n",
      "Epoch 61/100: - 2.1950s/step - train_loss: 0.6503 - train_acc: 0.8500 - val_loss: 0.6629 - val_acc: 0.6250\n",
      "Epoch 62/100: - 2.1942s/step - train_loss: 0.6505 - train_acc: 0.8562 - val_loss: 0.6630 - val_acc: 0.6500\n",
      "Epoch 63/100: - 2.1934s/step - train_loss: 0.6511 - train_acc: 0.8562 - val_loss: 0.6633 - val_acc: 0.6500\n",
      "Epoch 64/100: - 2.1927s/step - train_loss: 0.6490 - train_acc: 0.8500 - val_loss: 0.6618 - val_acc: 0.6250\n",
      "Epoch 65/100: - 2.1920s/step - train_loss: 0.6466 - train_acc: 0.7750 - val_loss: 0.6605 - val_acc: 0.6000\n",
      "Epoch 66/100: - 2.1915s/step - train_loss: 0.6465 - train_acc: 0.7625 - val_loss: 0.6617 - val_acc: 0.6000\n",
      "Epoch 67/100: - 2.1908s/step - train_loss: 0.6471 - train_acc: 0.8500 - val_loss: 0.6647 - val_acc: 0.6000\n",
      "Epoch 68/100: - 2.1902s/step - train_loss: 0.6496 - train_acc: 0.8500 - val_loss: 0.6686 - val_acc: 0.6000\n",
      "Epoch 69/100: - 2.1895s/step - train_loss: 0.6521 - train_acc: 0.8562 - val_loss: 0.6692 - val_acc: 0.5750\n",
      "Epoch 70/100: - 2.1890s/step - train_loss: 0.6531 - train_acc: 0.8500 - val_loss: 0.6673 - val_acc: 0.6250\n",
      "Epoch 71/100: - 2.1886s/step - train_loss: 0.6537 - train_acc: 0.8500 - val_loss: 0.6655 - val_acc: 0.6250\n",
      "Epoch 72/100: - 2.1880s/step - train_loss: 0.6536 - train_acc: 0.8688 - val_loss: 0.6630 - val_acc: 0.6500\n",
      "Epoch 73/100: - 2.1874s/step - train_loss: 0.6520 - train_acc: 0.7375 - val_loss: 0.6603 - val_acc: 0.6000\n",
      "Epoch 74/100: - 2.1870s/step - train_loss: 0.6494 - train_acc: 0.7562 - val_loss: 0.6599 - val_acc: 0.6000\n",
      "Epoch 75/100: - 2.1867s/step - train_loss: 0.6465 - train_acc: 0.8562 - val_loss: 0.6603 - val_acc: 0.6500\n",
      "Epoch 76/100: - 2.1863s/step - train_loss: 0.6442 - train_acc: 0.8500 - val_loss: 0.6609 - val_acc: 0.6500\n",
      "Epoch 77/100: - 2.1860s/step - train_loss: 0.6422 - train_acc: 0.8625 - val_loss: 0.6615 - val_acc: 0.6250\n",
      "Epoch 78/100: - 2.1855s/step - train_loss: 0.6415 - train_acc: 0.8562 - val_loss: 0.6617 - val_acc: 0.6250\n",
      "Epoch 79/100: - 2.1850s/step - train_loss: 0.6393 - train_acc: 0.8562 - val_loss: 0.6601 - val_acc: 0.6250\n",
      "Epoch 80/100: - 2.1846s/step - train_loss: 0.6390 - train_acc: 0.8500 - val_loss: 0.6593 - val_acc: 0.6250\n",
      "Epoch 81/100: - 2.1841s/step - train_loss: 0.6393 - train_acc: 0.8500 - val_loss: 0.6583 - val_acc: 0.6250\n",
      "Epoch 82/100: - 2.1837s/step - train_loss: 0.6394 - train_acc: 0.8562 - val_loss: 0.6580 - val_acc: 0.6250\n",
      "Epoch 83/100: - 2.1832s/step - train_loss: 0.6384 - train_acc: 0.8688 - val_loss: 0.6575 - val_acc: 0.6500\n",
      "Epoch 84/100: - 2.1828s/step - train_loss: 0.6377 - train_acc: 0.8625 - val_loss: 0.6557 - val_acc: 0.6500\n",
      "Epoch 85/100: - 2.1824s/step - train_loss: 0.6397 - train_acc: 0.8250 - val_loss: 0.6548 - val_acc: 0.6500\n",
      "Epoch 86/100: - 2.1820s/step - train_loss: 0.6410 - train_acc: 0.7937 - val_loss: 0.6546 - val_acc: 0.6000\n",
      "Epoch 87/100: - 2.1816s/step - train_loss: 0.6397 - train_acc: 0.8688 - val_loss: 0.6544 - val_acc: 0.6750\n",
      "Epoch 88/100: - 2.1812s/step - train_loss: 0.6379 - train_acc: 0.8562 - val_loss: 0.6520 - val_acc: 0.6500\n",
      "Epoch 89/100: - 2.1810s/step - train_loss: 0.6360 - train_acc: 0.8313 - val_loss: 0.6503 - val_acc: 0.6500\n",
      "Epoch 90/100: - 2.1807s/step - train_loss: 0.6328 - train_acc: 0.8500 - val_loss: 0.6494 - val_acc: 0.6250\n",
      "Epoch 91/100: - 2.1804s/step - train_loss: 0.6314 - train_acc: 0.8562 - val_loss: 0.6513 - val_acc: 0.6250\n",
      "Epoch 92/100: - 2.1801s/step - train_loss: 0.6330 - train_acc: 0.8500 - val_loss: 0.6546 - val_acc: 0.6500\n",
      "Epoch 93/100: - 2.1797s/step - train_loss: 0.6358 - train_acc: 0.8250 - val_loss: 0.6571 - val_acc: 0.6750\n",
      "Epoch 94/100: - 2.1793s/step - train_loss: 0.6358 - train_acc: 0.8625 - val_loss: 0.6534 - val_acc: 0.6500\n",
      "Epoch 95/100: - 2.1790s/step - train_loss: 0.6363 - train_acc: 0.7312 - val_loss: 0.6514 - val_acc: 0.6000\n",
      "Epoch 96/100: - 2.1786s/step - train_loss: 0.6338 - train_acc: 0.8125 - val_loss: 0.6510 - val_acc: 0.6000\n",
      "Epoch 97/100: - 2.1791s/step - train_loss: 0.6319 - train_acc: 0.8812 - val_loss: 0.6525 - val_acc: 0.6750\n",
      "Epoch 98/100: - 2.1806s/step - train_loss: 0.6300 - train_acc: 0.8750 - val_loss: 0.6520 - val_acc: 0.6500\n",
      "Epoch 99/100: - 2.1811s/step - train_loss: 0.6290 - train_acc: 0.8500 - val_loss: 0.6505 - val_acc: 0.6250\n",
      "Epoch 100/100: - 2.1819s/step - train_loss: 0.6291 - train_acc: 0.8625 - val_loss: 0.6505 - val_acc: 0.6250\n",
      "Start Standard Backprop\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6230 - accuracy: 0.7563 - val_loss: 0.6659 - val_accuracy: 0.6000\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.8313 - val_loss: 0.6391 - val_accuracy: 0.5750\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.8500 - val_loss: 0.6189 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.8313 - val_loss: 0.6104 - val_accuracy: 0.5750\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.8438 - val_loss: 0.6084 - val_accuracy: 0.6000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.8500 - val_loss: 0.6080 - val_accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4076 - accuracy: 0.8438 - val_loss: 0.6081 - val_accuracy: 0.6000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8500 - val_loss: 0.6111 - val_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3717 - accuracy: 0.8500 - val_loss: 0.6172 - val_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3604 - accuracy: 0.8500 - val_loss: 0.6248 - val_accuracy: 0.6000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8500 - val_loss: 0.6318 - val_accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8500 - val_loss: 0.6369 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3421 - accuracy: 0.8500 - val_loss: 0.6397 - val_accuracy: 0.6250\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.8562 - val_loss: 0.6406 - val_accuracy: 0.6250\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8625 - val_loss: 0.6401 - val_accuracy: 0.6250\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8687 - val_loss: 0.6383 - val_accuracy: 0.6250\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3334 - accuracy: 0.8687 - val_loss: 0.6355 - val_accuracy: 0.6250\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8687 - val_loss: 0.6319 - val_accuracy: 0.6250\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8687 - val_loss: 0.6279 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8687 - val_loss: 0.6237 - val_accuracy: 0.6250\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8750 - val_loss: 0.6197 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8750 - val_loss: 0.6159 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8750 - val_loss: 0.6125 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8750 - val_loss: 0.6095 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8813 - val_loss: 0.6069 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8813 - val_loss: 0.6046 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8813 - val_loss: 0.6027 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8813 - val_loss: 0.6012 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8813 - val_loss: 0.5999 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8813 - val_loss: 0.5989 - val_accuracy: 0.6750\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8813 - val_loss: 0.5981 - val_accuracy: 0.6750\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8813 - val_loss: 0.5975 - val_accuracy: 0.6750\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8813 - val_loss: 0.5970 - val_accuracy: 0.6750\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8750 - val_loss: 0.5967 - val_accuracy: 0.6750\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8750 - val_loss: 0.5964 - val_accuracy: 0.6750\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8750 - val_loss: 0.5962 - val_accuracy: 0.6750\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.5960 - val_accuracy: 0.6750\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.5959 - val_accuracy: 0.6750\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8750 - val_loss: 0.5958 - val_accuracy: 0.6750\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.6750\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.5958 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.5958 - val_accuracy: 0.7000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.5959 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5959 - val_accuracy: 0.7000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5960 - val_accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5961 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5961 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5962 - val_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5963 - val_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5964 - val_accuracy: 0.7000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5964 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5965 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5966 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5967 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5968 - val_accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5969 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5970 - val_accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5971 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5972 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5973 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5974 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5974 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5975 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5976 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5977 - val_accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5978 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5980 - val_accuracy: 0.7000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5982 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5983 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5986 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5987 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5988 - val_accuracy: 0.7000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5989 - val_accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5992 - val_accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5993 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.5996 - val_accuracy: 0.7000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.5998 - val_accuracy: 0.7000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.5999 - val_accuracy: 0.7000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6000 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6001 - val_accuracy: 0.7000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6002 - val_accuracy: 0.7000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6003 - val_accuracy: 0.7000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6005 - val_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6006 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6007 - val_accuracy: 0.7000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6008 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/100: - 0.7138s/step - train_loss: 0.6969 - train_acc: 0.5188 - val_loss: 0.6984 - val_acc: 0.4250\n",
      "Epoch 2/100: - 0.7062s/step - train_loss: 0.6901 - train_acc: 0.5188 - val_loss: 0.6937 - val_acc: 0.4250\n",
      "Epoch 3/100: - 0.7097s/step - train_loss: 0.6834 - train_acc: 0.7125 - val_loss: 0.6938 - val_acc: 0.4250\n",
      "Epoch 4/100: - 0.7104s/step - train_loss: 0.6790 - train_acc: 0.5750 - val_loss: 0.6951 - val_acc: 0.5750\n",
      "Epoch 5/100: - 0.6993s/step - train_loss: 0.6721 - train_acc: 0.6750 - val_loss: 0.6908 - val_acc: 0.6250\n",
      "Epoch 6/100: - 0.6999s/step - train_loss: 0.6642 - train_acc: 0.8063 - val_loss: 0.6840 - val_acc: 0.5000\n",
      "Epoch 7/100: - 0.6986s/step - train_loss: 0.6571 - train_acc: 0.7812 - val_loss: 0.6783 - val_acc: 0.5250\n",
      "Epoch 8/100: - 0.6981s/step - train_loss: 0.6507 - train_acc: 0.8125 - val_loss: 0.6742 - val_acc: 0.6000\n",
      "Epoch 9/100: - 0.6962s/step - train_loss: 0.6456 - train_acc: 0.8000 - val_loss: 0.6717 - val_acc: 0.6000\n",
      "Epoch 10/100: - 0.6946s/step - train_loss: 0.6416 - train_acc: 0.8125 - val_loss: 0.6703 - val_acc: 0.5500\n",
      "Epoch 11/100: - 0.6930s/step - train_loss: 0.6381 - train_acc: 0.8375 - val_loss: 0.6692 - val_acc: 0.5750\n",
      "Epoch 12/100: - 0.6915s/step - train_loss: 0.6356 - train_acc: 0.8313 - val_loss: 0.6681 - val_acc: 0.5750\n",
      "Epoch 13/100: - 0.6904s/step - train_loss: 0.6345 - train_acc: 0.8187 - val_loss: 0.6673 - val_acc: 0.5750\n",
      "Epoch 14/100: - 0.6917s/step - train_loss: 0.6341 - train_acc: 0.8187 - val_loss: 0.6670 - val_acc: 0.5500\n",
      "Epoch 15/100: - 0.6896s/step - train_loss: 0.6343 - train_acc: 0.8187 - val_loss: 0.6673 - val_acc: 0.5500\n",
      "Epoch 16/100: - 0.6870s/step - train_loss: 0.6349 - train_acc: 0.8125 - val_loss: 0.6680 - val_acc: 0.5250\n",
      "Epoch 17/100: - 0.6869s/step - train_loss: 0.6357 - train_acc: 0.8187 - val_loss: 0.6692 - val_acc: 0.5750\n",
      "Epoch 18/100: - 0.6857s/step - train_loss: 0.6365 - train_acc: 0.8250 - val_loss: 0.6705 - val_acc: 0.5750\n",
      "Epoch 19/100: - 0.6848s/step - train_loss: 0.6363 - train_acc: 0.8187 - val_loss: 0.6704 - val_acc: 0.5750\n",
      "Epoch 20/100: - 0.6839s/step - train_loss: 0.6355 - train_acc: 0.8187 - val_loss: 0.6689 - val_acc: 0.5500\n",
      "Epoch 21/100: - 0.6834s/step - train_loss: 0.6349 - train_acc: 0.8125 - val_loss: 0.6677 - val_acc: 0.5250\n",
      "Epoch 22/100: - 0.6830s/step - train_loss: 0.6340 - train_acc: 0.8187 - val_loss: 0.6670 - val_acc: 0.5500\n",
      "Epoch 23/100: - 0.6823s/step - train_loss: 0.6329 - train_acc: 0.8375 - val_loss: 0.6662 - val_acc: 0.5750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: - 0.6814s/step - train_loss: 0.6314 - train_acc: 0.8250 - val_loss: 0.6642 - val_acc: 0.5750\n",
      "Epoch 25/100: - 0.6810s/step - train_loss: 0.6293 - train_acc: 0.8313 - val_loss: 0.6617 - val_acc: 0.5750\n",
      "Epoch 26/100: - 0.6797s/step - train_loss: 0.6264 - train_acc: 0.8313 - val_loss: 0.6592 - val_acc: 0.5750\n",
      "Epoch 27/100: - 0.6786s/step - train_loss: 0.6232 - train_acc: 0.8313 - val_loss: 0.6572 - val_acc: 0.6250\n",
      "Epoch 28/100: - 0.6789s/step - train_loss: 0.6195 - train_acc: 0.8313 - val_loss: 0.6550 - val_acc: 0.6250\n",
      "Epoch 29/100: - 0.6785s/step - train_loss: 0.6158 - train_acc: 0.8375 - val_loss: 0.6526 - val_acc: 0.6000\n",
      "Epoch 30/100: - 0.6781s/step - train_loss: 0.6126 - train_acc: 0.8000 - val_loss: 0.6505 - val_acc: 0.6000\n",
      "Epoch 31/100: - 0.6778s/step - train_loss: 0.6094 - train_acc: 0.8000 - val_loss: 0.6491 - val_acc: 0.6000\n",
      "Epoch 32/100: - 0.6776s/step - train_loss: 0.6065 - train_acc: 0.8187 - val_loss: 0.6482 - val_acc: 0.6000\n",
      "Epoch 33/100: - 0.6772s/step - train_loss: 0.6041 - train_acc: 0.8438 - val_loss: 0.6477 - val_acc: 0.5750\n",
      "Epoch 34/100: - 0.6769s/step - train_loss: 0.6022 - train_acc: 0.8438 - val_loss: 0.6470 - val_acc: 0.5750\n",
      "Epoch 35/100: - 0.6767s/step - train_loss: 0.6007 - train_acc: 0.8438 - val_loss: 0.6454 - val_acc: 0.5750\n",
      "Epoch 36/100: - 0.6771s/step - train_loss: 0.5995 - train_acc: 0.8500 - val_loss: 0.6433 - val_acc: 0.6000\n",
      "Epoch 37/100: - 0.6773s/step - train_loss: 0.5991 - train_acc: 0.8438 - val_loss: 0.6419 - val_acc: 0.5750\n",
      "Epoch 38/100: - 0.6775s/step - train_loss: 0.5992 - train_acc: 0.8500 - val_loss: 0.6416 - val_acc: 0.5750\n",
      "Epoch 39/100: - 0.6796s/step - train_loss: 0.5990 - train_acc: 0.8375 - val_loss: 0.6417 - val_acc: 0.6250\n",
      "Epoch 40/100: - 0.6811s/step - train_loss: 0.5990 - train_acc: 0.8500 - val_loss: 0.6419 - val_acc: 0.6000\n",
      "Epoch 41/100: - 0.6819s/step - train_loss: 0.5986 - train_acc: 0.8375 - val_loss: 0.6413 - val_acc: 0.6250\n",
      "Epoch 42/100: - 0.6827s/step - train_loss: 0.5986 - train_acc: 0.8500 - val_loss: 0.6410 - val_acc: 0.5750\n",
      "Epoch 43/100: - 0.6826s/step - train_loss: 0.5991 - train_acc: 0.8500 - val_loss: 0.6416 - val_acc: 0.6000\n",
      "Epoch 44/100: - 0.6825s/step - train_loss: 0.6000 - train_acc: 0.8500 - val_loss: 0.6428 - val_acc: 0.6000\n",
      "Epoch 45/100: - 0.6824s/step - train_loss: 0.6012 - train_acc: 0.8250 - val_loss: 0.6441 - val_acc: 0.6000\n",
      "Epoch 46/100: - 0.6829s/step - train_loss: 0.6025 - train_acc: 0.8375 - val_loss: 0.6453 - val_acc: 0.6000\n",
      "Epoch 47/100: - 0.6826s/step - train_loss: 0.6037 - train_acc: 0.8438 - val_loss: 0.6465 - val_acc: 0.5750\n",
      "Epoch 48/100: - 0.6819s/step - train_loss: 0.6049 - train_acc: 0.8313 - val_loss: 0.6473 - val_acc: 0.6250\n",
      "Epoch 49/100: - 0.6813s/step - train_loss: 0.6055 - train_acc: 0.8313 - val_loss: 0.6475 - val_acc: 0.6250\n",
      "Epoch 50/100: - 0.6814s/step - train_loss: 0.6054 - train_acc: 0.8438 - val_loss: 0.6470 - val_acc: 0.5750\n",
      "Epoch 51/100: - 0.6821s/step - train_loss: 0.6042 - train_acc: 0.8438 - val_loss: 0.6461 - val_acc: 0.5750\n",
      "Epoch 52/100: - 0.6822s/step - train_loss: 0.6023 - train_acc: 0.8438 - val_loss: 0.6447 - val_acc: 0.5750\n",
      "Epoch 53/100: - 0.6825s/step - train_loss: 0.5998 - train_acc: 0.8500 - val_loss: 0.6428 - val_acc: 0.6000\n",
      "Epoch 54/100: - 0.6826s/step - train_loss: 0.5970 - train_acc: 0.8375 - val_loss: 0.6410 - val_acc: 0.6000\n",
      "Epoch 55/100: - 0.6827s/step - train_loss: 0.5939 - train_acc: 0.8375 - val_loss: 0.6396 - val_acc: 0.6000\n",
      "Epoch 56/100: - 0.6825s/step - train_loss: 0.5906 - train_acc: 0.8438 - val_loss: 0.6386 - val_acc: 0.5750\n",
      "Epoch 57/100: - 0.6824s/step - train_loss: 0.5880 - train_acc: 0.8438 - val_loss: 0.6381 - val_acc: 0.6000\n",
      "Epoch 58/100: - 0.6818s/step - train_loss: 0.5863 - train_acc: 0.8562 - val_loss: 0.6376 - val_acc: 0.5750\n",
      "Epoch 59/100: - 0.6815s/step - train_loss: 0.5848 - train_acc: 0.8562 - val_loss: 0.6362 - val_acc: 0.5750\n",
      "Epoch 60/100: - 0.6814s/step - train_loss: 0.5832 - train_acc: 0.8562 - val_loss: 0.6346 - val_acc: 0.5750\n",
      "Epoch 61/100: - 0.6811s/step - train_loss: 0.5818 - train_acc: 0.8313 - val_loss: 0.6331 - val_acc: 0.6250\n",
      "Epoch 62/100: - 0.6808s/step - train_loss: 0.5801 - train_acc: 0.8438 - val_loss: 0.6321 - val_acc: 0.6000\n",
      "Epoch 63/100: - 0.6804s/step - train_loss: 0.5777 - train_acc: 0.8438 - val_loss: 0.6313 - val_acc: 0.6250\n",
      "Epoch 64/100: - 0.6799s/step - train_loss: 0.5746 - train_acc: 0.8438 - val_loss: 0.6306 - val_acc: 0.6250\n",
      "Epoch 65/100: - 0.6796s/step - train_loss: 0.5715 - train_acc: 0.8562 - val_loss: 0.6299 - val_acc: 0.5750\n",
      "Epoch 66/100: - 0.6793s/step - train_loss: 0.5682 - train_acc: 0.8438 - val_loss: 0.6288 - val_acc: 0.5750\n",
      "Epoch 67/100: - 0.6790s/step - train_loss: 0.5656 - train_acc: 0.8438 - val_loss: 0.6278 - val_acc: 0.6000\n",
      "Epoch 68/100: - 0.6785s/step - train_loss: 0.5639 - train_acc: 0.8375 - val_loss: 0.6268 - val_acc: 0.6250\n",
      "Epoch 69/100: - 0.6778s/step - train_loss: 0.5626 - train_acc: 0.8313 - val_loss: 0.6263 - val_acc: 0.6250\n",
      "Epoch 70/100: - 0.6773s/step - train_loss: 0.5618 - train_acc: 0.8313 - val_loss: 0.6261 - val_acc: 0.6250\n",
      "Epoch 71/100: - 0.6771s/step - train_loss: 0.5612 - train_acc: 0.8313 - val_loss: 0.6263 - val_acc: 0.6250\n",
      "Epoch 72/100: - 0.6772s/step - train_loss: 0.5611 - train_acc: 0.8375 - val_loss: 0.6266 - val_acc: 0.6000\n",
      "Epoch 73/100: - 0.6770s/step - train_loss: 0.5607 - train_acc: 0.8562 - val_loss: 0.6265 - val_acc: 0.5750\n",
      "Epoch 74/100: - 0.6767s/step - train_loss: 0.5598 - train_acc: 0.8562 - val_loss: 0.6256 - val_acc: 0.5750\n",
      "Epoch 75/100: - 0.6764s/step - train_loss: 0.5577 - train_acc: 0.8562 - val_loss: 0.6235 - val_acc: 0.5750\n",
      "Epoch 76/100: - 0.6762s/step - train_loss: 0.5548 - train_acc: 0.8562 - val_loss: 0.6208 - val_acc: 0.5750\n",
      "Epoch 77/100: - 0.6759s/step - train_loss: 0.5509 - train_acc: 0.8562 - val_loss: 0.6179 - val_acc: 0.5750\n",
      "Epoch 78/100: - 0.6757s/step - train_loss: 0.5476 - train_acc: 0.8562 - val_loss: 0.6155 - val_acc: 0.5750\n",
      "Epoch 79/100: - 0.6754s/step - train_loss: 0.5460 - train_acc: 0.8625 - val_loss: 0.6139 - val_acc: 0.5750\n",
      "Epoch 80/100: - 0.6747s/step - train_loss: 0.5460 - train_acc: 0.8625 - val_loss: 0.6134 - val_acc: 0.5750\n",
      "Epoch 81/100: - 0.6743s/step - train_loss: 0.5460 - train_acc: 0.8562 - val_loss: 0.6135 - val_acc: 0.5750\n",
      "Epoch 82/100: - 0.6741s/step - train_loss: 0.5458 - train_acc: 0.8438 - val_loss: 0.6138 - val_acc: 0.6250\n",
      "Epoch 83/100: - 0.6738s/step - train_loss: 0.5453 - train_acc: 0.8438 - val_loss: 0.6145 - val_acc: 0.6250\n",
      "Epoch 84/100: - 0.6736s/step - train_loss: 0.5443 - train_acc: 0.8500 - val_loss: 0.6153 - val_acc: 0.5750\n",
      "Epoch 85/100: - 0.6734s/step - train_loss: 0.5423 - train_acc: 0.8438 - val_loss: 0.6155 - val_acc: 0.6250\n",
      "Epoch 86/100: - 0.6732s/step - train_loss: 0.5408 - train_acc: 0.8438 - val_loss: 0.6159 - val_acc: 0.6250\n",
      "Epoch 87/100: - 0.6729s/step - train_loss: 0.5406 - train_acc: 0.8562 - val_loss: 0.6166 - val_acc: 0.5750\n",
      "Epoch 88/100: - 0.6727s/step - train_loss: 0.5401 - train_acc: 0.8562 - val_loss: 0.6167 - val_acc: 0.5750\n",
      "Epoch 89/100: - 0.6725s/step - train_loss: 0.5393 - train_acc: 0.8500 - val_loss: 0.6160 - val_acc: 0.5750\n",
      "Epoch 90/100: - 0.6723s/step - train_loss: 0.5379 - train_acc: 0.8438 - val_loss: 0.6145 - val_acc: 0.5750\n",
      "Epoch 91/100: - 0.6718s/step - train_loss: 0.5360 - train_acc: 0.8438 - val_loss: 0.6121 - val_acc: 0.5750\n",
      "Epoch 92/100: - 0.6714s/step - train_loss: 0.5339 - train_acc: 0.8375 - val_loss: 0.6095 - val_acc: 0.5750\n",
      "Epoch 93/100: - 0.6712s/step - train_loss: 0.5314 - train_acc: 0.8562 - val_loss: 0.6066 - val_acc: 0.5750\n",
      "Epoch 94/100: - 0.6710s/step - train_loss: 0.5297 - train_acc: 0.8562 - val_loss: 0.6043 - val_acc: 0.5750\n",
      "Epoch 95/100: - 0.6708s/step - train_loss: 0.5285 - train_acc: 0.8562 - val_loss: 0.6024 - val_acc: 0.6000\n",
      "Epoch 96/100: - 0.6707s/step - train_loss: 0.5281 - train_acc: 0.8500 - val_loss: 0.6011 - val_acc: 0.6250\n",
      "Epoch 97/100: - 0.6705s/step - train_loss: 0.5274 - train_acc: 0.8500 - val_loss: 0.6004 - val_acc: 0.6250\n",
      "Epoch 98/100: - 0.6704s/step - train_loss: 0.5263 - train_acc: 0.8500 - val_loss: 0.6000 - val_acc: 0.6250\n",
      "Epoch 99/100: - 0.6715s/step - train_loss: 0.5250 - train_acc: 0.8500 - val_loss: 0.5998 - val_acc: 0.6250\n",
      "Epoch 100/100: - 0.6713s/step - train_loss: 0.5238 - train_acc: 0.8500 - val_loss: 0.5998 - val_acc: 0.6250\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: - 2.1687s/step - train_loss: 0.7818 - train_acc: 0.4000 - val_loss: 0.7125 - val_acc: 0.4250\n",
      "Epoch 2/100: - 2.1880s/step - train_loss: 0.7685 - train_acc: 0.3812 - val_loss: 0.7059 - val_acc: 0.4000\n",
      "Epoch 3/100: - 2.1790s/step - train_loss: 0.7541 - train_acc: 0.2625 - val_loss: 0.7046 - val_acc: 0.5000\n",
      "Epoch 4/100: - 2.1683s/step - train_loss: 0.7418 - train_acc: 0.2437 - val_loss: 0.7012 - val_acc: 0.5000\n",
      "Epoch 5/100: - 2.1593s/step - train_loss: 0.7298 - train_acc: 0.3187 - val_loss: 0.6932 - val_acc: 0.4250\n",
      "Epoch 6/100: - 2.1537s/step - train_loss: 0.7212 - train_acc: 0.4313 - val_loss: 0.6878 - val_acc: 0.4000\n",
      "Epoch 7/100: - 2.1508s/step - train_loss: 0.7122 - train_acc: 0.4688 - val_loss: 0.6857 - val_acc: 0.4250\n",
      "Epoch 8/100: - 2.1485s/step - train_loss: 0.7040 - train_acc: 0.4437 - val_loss: 0.6853 - val_acc: 0.4000\n",
      "Epoch 9/100: - 2.1460s/step - train_loss: 0.6966 - train_acc: 0.4688 - val_loss: 0.6846 - val_acc: 0.4250\n",
      "Epoch 10/100: - 2.1444s/step - train_loss: 0.6897 - train_acc: 0.5125 - val_loss: 0.6820 - val_acc: 0.4250\n",
      "Epoch 11/100: - 2.1427s/step - train_loss: 0.6843 - train_acc: 0.5250 - val_loss: 0.6798 - val_acc: 0.4250\n",
      "Epoch 12/100: - 2.1409s/step - train_loss: 0.6809 - train_acc: 0.5250 - val_loss: 0.6794 - val_acc: 0.4500\n",
      "Epoch 13/100: - 2.1529s/step - train_loss: 0.6795 - train_acc: 0.6937 - val_loss: 0.6808 - val_acc: 0.6000\n",
      "Epoch 14/100: - 2.1632s/step - train_loss: 0.6781 - train_acc: 0.7312 - val_loss: 0.6807 - val_acc: 0.5750\n",
      "Epoch 15/100: - 2.1734s/step - train_loss: 0.6754 - train_acc: 0.6875 - val_loss: 0.6794 - val_acc: 0.5750\n",
      "Epoch 16/100: - 2.1972s/step - train_loss: 0.6727 - train_acc: 0.7000 - val_loss: 0.6786 - val_acc: 0.5500\n",
      "Epoch 17/100: - 2.2160s/step - train_loss: 0.6718 - train_acc: 0.6188 - val_loss: 0.6783 - val_acc: 0.5250\n",
      "Epoch 18/100: - 2.2322s/step - train_loss: 0.6711 - train_acc: 0.6500 - val_loss: 0.6784 - val_acc: 0.5500\n",
      "Epoch 19/100: - 2.2446s/step - train_loss: 0.6696 - train_acc: 0.8125 - val_loss: 0.6786 - val_acc: 0.6000\n",
      "Epoch 20/100: - 2.2611s/step - train_loss: 0.6676 - train_acc: 0.8500 - val_loss: 0.6778 - val_acc: 0.6250\n",
      "Epoch 21/100: - 2.2801s/step - train_loss: 0.6642 - train_acc: 0.8063 - val_loss: 0.6744 - val_acc: 0.6250\n",
      "Epoch 22/100: - 2.2942s/step - train_loss: 0.6630 - train_acc: 0.6500 - val_loss: 0.6718 - val_acc: 0.5500\n",
      "Epoch 23/100: - 2.3013s/step - train_loss: 0.6625 - train_acc: 0.5938 - val_loss: 0.6706 - val_acc: 0.5000\n",
      "Epoch 24/100: - 2.3097s/step - train_loss: 0.6614 - train_acc: 0.7562 - val_loss: 0.6713 - val_acc: 0.6000\n",
      "Epoch 25/100: - 2.3186s/step - train_loss: 0.6603 - train_acc: 0.8562 - val_loss: 0.6718 - val_acc: 0.6250\n",
      "Epoch 26/100: - 2.3227s/step - train_loss: 0.6600 - train_acc: 0.7188 - val_loss: 0.6699 - val_acc: 0.5750\n",
      "Epoch 27/100: - 2.3273s/step - train_loss: 0.6613 - train_acc: 0.6312 - val_loss: 0.6704 - val_acc: 0.5500\n",
      "Epoch 28/100: - 2.3330s/step - train_loss: 0.6614 - train_acc: 0.7500 - val_loss: 0.6718 - val_acc: 0.6000\n",
      "Epoch 29/100: - 2.3373s/step - train_loss: 0.6618 - train_acc: 0.8500 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 30/100: - 2.3433s/step - train_loss: 0.6632 - train_acc: 0.8438 - val_loss: 0.6739 - val_acc: 0.6250\n",
      "Epoch 31/100: - 2.3624s/step - train_loss: 0.6650 - train_acc: 0.8500 - val_loss: 0.6744 - val_acc: 0.6500\n",
      "Epoch 32/100: - 2.3686s/step - train_loss: 0.6663 - train_acc: 0.7125 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 33/100: - 2.3716s/step - train_loss: 0.6663 - train_acc: 0.5437 - val_loss: 0.6728 - val_acc: 0.4750\n",
      "Epoch 34/100: - 2.3743s/step - train_loss: 0.6659 - train_acc: 0.7125 - val_loss: 0.6744 - val_acc: 0.5750\n",
      "Epoch 35/100: - 2.3852s/step - train_loss: 0.6670 - train_acc: 0.8688 - val_loss: 0.6766 - val_acc: 0.6500\n",
      "Epoch 36/100: - 2.3915s/step - train_loss: 0.6664 - train_acc: 0.8438 - val_loss: 0.6755 - val_acc: 0.6500\n",
      "Epoch 37/100: - 2.3963s/step - train_loss: 0.6660 - train_acc: 0.8125 - val_loss: 0.6744 - val_acc: 0.6500\n",
      "Epoch 38/100: - 2.4017s/step - train_loss: 0.6662 - train_acc: 0.8250 - val_loss: 0.6736 - val_acc: 0.6500\n",
      "Epoch 39/100: - 2.4038s/step - train_loss: 0.6654 - train_acc: 0.8250 - val_loss: 0.6722 - val_acc: 0.6000\n",
      "Epoch 40/100: - 2.4049s/step - train_loss: 0.6642 - train_acc: 0.6875 - val_loss: 0.6708 - val_acc: 0.5500\n",
      "Epoch 41/100: - 2.4075s/step - train_loss: 0.6625 - train_acc: 0.8125 - val_loss: 0.6708 - val_acc: 0.6000\n",
      "Epoch 42/100: - 2.4093s/step - train_loss: 0.6608 - train_acc: 0.8625 - val_loss: 0.6700 - val_acc: 0.6500\n",
      "Epoch 43/100: - 2.4099s/step - train_loss: 0.6617 - train_acc: 0.8562 - val_loss: 0.6693 - val_acc: 0.6500\n",
      "Epoch 44/100: - 2.4109s/step - train_loss: 0.6610 - train_acc: 0.7562 - val_loss: 0.6673 - val_acc: 0.6000\n",
      "Epoch 45/100: - 2.4121s/step - train_loss: 0.6588 - train_acc: 0.6687 - val_loss: 0.6661 - val_acc: 0.5500\n",
      "Epoch 46/100: - 2.4126s/step - train_loss: 0.6562 - train_acc: 0.7500 - val_loss: 0.6668 - val_acc: 0.6000\n",
      "Epoch 47/100: - 2.4150s/step - train_loss: 0.6563 - train_acc: 0.8750 - val_loss: 0.6697 - val_acc: 0.6500\n",
      "Epoch 48/100: - 2.4166s/step - train_loss: 0.6569 - train_acc: 0.8688 - val_loss: 0.6705 - val_acc: 0.6250\n",
      "Epoch 49/100: - 2.4188s/step - train_loss: 0.6580 - train_acc: 0.7562 - val_loss: 0.6691 - val_acc: 0.6000\n",
      "Epoch 50/100: - 2.4217s/step - train_loss: 0.6581 - train_acc: 0.7500 - val_loss: 0.6692 - val_acc: 0.6000\n",
      "Epoch 51/100: - 2.4266s/step - train_loss: 0.6555 - train_acc: 0.8375 - val_loss: 0.6687 - val_acc: 0.6000\n",
      "Epoch 52/100: - 2.4300s/step - train_loss: 0.6542 - train_acc: 0.8562 - val_loss: 0.6689 - val_acc: 0.6250\n",
      "Epoch 53/100: - 2.4349s/step - train_loss: 0.6533 - train_acc: 0.8438 - val_loss: 0.6682 - val_acc: 0.6250\n",
      "Epoch 54/100: - 2.4367s/step - train_loss: 0.6515 - train_acc: 0.8500 - val_loss: 0.6663 - val_acc: 0.6000\n",
      "Epoch 55/100: - 2.4387s/step - train_loss: 0.6518 - train_acc: 0.8000 - val_loss: 0.6656 - val_acc: 0.6500\n",
      "Epoch 56/100: - 2.4417s/step - train_loss: 0.6535 - train_acc: 0.8688 - val_loss: 0.6668 - val_acc: 0.6250\n",
      "Epoch 57/100: - 2.4432s/step - train_loss: 0.6540 - train_acc: 0.8625 - val_loss: 0.6663 - val_acc: 0.6250\n",
      "Epoch 58/100: - 2.4457s/step - train_loss: 0.6537 - train_acc: 0.8187 - val_loss: 0.6645 - val_acc: 0.6000\n",
      "Epoch 59/100: - 2.4482s/step - train_loss: 0.6526 - train_acc: 0.7562 - val_loss: 0.6641 - val_acc: 0.6000\n",
      "Epoch 60/100: - 2.4506s/step - train_loss: 0.6521 - train_acc: 0.8125 - val_loss: 0.6646 - val_acc: 0.6250\n",
      "Epoch 61/100: - 2.4528s/step - train_loss: 0.6518 - train_acc: 0.8688 - val_loss: 0.6648 - val_acc: 0.6250\n",
      "Epoch 62/100: - 2.4537s/step - train_loss: 0.6509 - train_acc: 0.8562 - val_loss: 0.6629 - val_acc: 0.6500\n",
      "Epoch 63/100: - 2.4555s/step - train_loss: 0.6519 - train_acc: 0.8000 - val_loss: 0.6630 - val_acc: 0.6000\n",
      "Epoch 64/100: - 2.4572s/step - train_loss: 0.6517 - train_acc: 0.8438 - val_loss: 0.6642 - val_acc: 0.6250\n",
      "Epoch 65/100: - 2.4582s/step - train_loss: 0.6511 - train_acc: 0.8562 - val_loss: 0.6649 - val_acc: 0.6250\n",
      "Epoch 66/100: - 2.4599s/step - train_loss: 0.6491 - train_acc: 0.8375 - val_loss: 0.6625 - val_acc: 0.6000\n",
      "Epoch 67/100: - 2.4596s/step - train_loss: 0.6482 - train_acc: 0.8438 - val_loss: 0.6617 - val_acc: 0.6250\n",
      "Epoch 68/100: - 2.4604s/step - train_loss: 0.6467 - train_acc: 0.8625 - val_loss: 0.6606 - val_acc: 0.6250\n",
      "Epoch 69/100: - 2.4630s/step - train_loss: 0.6448 - train_acc: 0.8625 - val_loss: 0.6587 - val_acc: 0.6500\n",
      "Epoch 70/100: - 2.4640s/step - train_loss: 0.6432 - train_acc: 0.8688 - val_loss: 0.6569 - val_acc: 0.6250\n",
      "Epoch 71/100: - 2.4647s/step - train_loss: 0.6418 - train_acc: 0.8250 - val_loss: 0.6554 - val_acc: 0.6500\n",
      "Epoch 72/100: - 2.4650s/step - train_loss: 0.6415 - train_acc: 0.8187 - val_loss: 0.6563 - val_acc: 0.6500\n",
      "Epoch 73/100: - 2.4677s/step - train_loss: 0.6384 - train_acc: 0.8500 - val_loss: 0.6566 - val_acc: 0.6250\n",
      "Epoch 74/100: - 2.4688s/step - train_loss: 0.6368 - train_acc: 0.8500 - val_loss: 0.6565 - val_acc: 0.6250\n",
      "Epoch 75/100: - 2.4687s/step - train_loss: 0.6377 - train_acc: 0.8562 - val_loss: 0.6571 - val_acc: 0.6250\n",
      "Epoch 76/100: - 2.4682s/step - train_loss: 0.6383 - train_acc: 0.8688 - val_loss: 0.6542 - val_acc: 0.6250\n",
      "Epoch 77/100: - 2.4704s/step - train_loss: 0.6411 - train_acc: 0.8000 - val_loss: 0.6534 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100: - 2.4718s/step - train_loss: 0.6442 - train_acc: 0.8313 - val_loss: 0.6552 - val_acc: 0.6500\n",
      "Epoch 79/100: - 2.4717s/step - train_loss: 0.6460 - train_acc: 0.8688 - val_loss: 0.6577 - val_acc: 0.6750\n",
      "Epoch 80/100: - 2.4719s/step - train_loss: 0.6455 - train_acc: 0.8812 - val_loss: 0.6578 - val_acc: 0.6750\n",
      "Epoch 81/100: - 2.4725s/step - train_loss: 0.6453 - train_acc: 0.8500 - val_loss: 0.6562 - val_acc: 0.6500\n",
      "Epoch 82/100: - 2.4738s/step - train_loss: 0.6444 - train_acc: 0.7125 - val_loss: 0.6550 - val_acc: 0.5750\n",
      "Epoch 83/100: - 2.4746s/step - train_loss: 0.6441 - train_acc: 0.8500 - val_loss: 0.6568 - val_acc: 0.6250\n",
      "Epoch 84/100: - 2.4767s/step - train_loss: 0.6418 - train_acc: 0.8688 - val_loss: 0.6581 - val_acc: 0.6500\n",
      "Epoch 85/100: - 2.4779s/step - train_loss: 0.6372 - train_acc: 0.8688 - val_loss: 0.6548 - val_acc: 0.6500\n",
      "Epoch 86/100: - 2.4792s/step - train_loss: 0.6365 - train_acc: 0.7812 - val_loss: 0.6526 - val_acc: 0.6000\n",
      "Epoch 87/100: - 2.4808s/step - train_loss: 0.6340 - train_acc: 0.8187 - val_loss: 0.6516 - val_acc: 0.6250\n",
      "Epoch 88/100: - 2.4809s/step - train_loss: 0.6329 - train_acc: 0.8688 - val_loss: 0.6532 - val_acc: 0.6500\n",
      "Epoch 89/100: - 2.4809s/step - train_loss: 0.6314 - train_acc: 0.8812 - val_loss: 0.6522 - val_acc: 0.6250\n",
      "Epoch 90/100: - 2.4835s/step - train_loss: 0.6291 - train_acc: 0.8562 - val_loss: 0.6472 - val_acc: 0.6500\n",
      "Epoch 91/100: - 2.4871s/step - train_loss: 0.6277 - train_acc: 0.8562 - val_loss: 0.6451 - val_acc: 0.6500\n",
      "Epoch 92/100: - 2.4883s/step - train_loss: 0.6277 - train_acc: 0.8688 - val_loss: 0.6451 - val_acc: 0.6500\n",
      "Epoch 93/100: - 2.4894s/step - train_loss: 0.6281 - train_acc: 0.8750 - val_loss: 0.6450 - val_acc: 0.7000\n",
      "Epoch 94/100: - 2.4876s/step - train_loss: 0.6303 - train_acc: 0.8812 - val_loss: 0.6442 - val_acc: 0.6750\n",
      "Epoch 95/100: - 2.4861s/step - train_loss: 0.6321 - train_acc: 0.8250 - val_loss: 0.6441 - val_acc: 0.6250\n",
      "Epoch 96/100: - 2.4862s/step - train_loss: 0.6304 - train_acc: 0.8313 - val_loss: 0.6443 - val_acc: 0.6250\n",
      "Epoch 97/100: - 2.4866s/step - train_loss: 0.6309 - train_acc: 0.8688 - val_loss: 0.6459 - val_acc: 0.7250\n",
      "Epoch 98/100: - 2.4871s/step - train_loss: 0.6297 - train_acc: 0.8812 - val_loss: 0.6438 - val_acc: 0.6750\n",
      "Epoch 99/100: - 2.4863s/step - train_loss: 0.6281 - train_acc: 0.8625 - val_loss: 0.6417 - val_acc: 0.6750\n",
      "Epoch 100/100: - 2.4853s/step - train_loss: 0.6251 - train_acc: 0.8625 - val_loss: 0.6400 - val_acc: 0.6750\n",
      "Start Standard Backprop\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6779 - accuracy: 0.5688 - val_loss: 0.7091 - val_accuracy: 0.3750\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6330 - accuracy: 0.6875 - val_loss: 0.6842 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6000 - accuracy: 0.7250 - val_loss: 0.6786 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.7812 - val_loss: 0.6758 - val_accuracy: 0.5500\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7875 - val_loss: 0.6633 - val_accuracy: 0.5750\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.8188 - val_loss: 0.6502 - val_accuracy: 0.5500\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4604 - accuracy: 0.8125 - val_loss: 0.6440 - val_accuracy: 0.5500\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8188 - val_loss: 0.6442 - val_accuracy: 0.5500\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4114 - accuracy: 0.8188 - val_loss: 0.6476 - val_accuracy: 0.5750\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8250 - val_loss: 0.6514 - val_accuracy: 0.5750\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3783 - accuracy: 0.8313 - val_loss: 0.6547 - val_accuracy: 0.5750\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3674 - accuracy: 0.8375 - val_loss: 0.6579 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3592 - accuracy: 0.8375 - val_loss: 0.6608 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8438 - val_loss: 0.6625 - val_accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8500 - val_loss: 0.6625 - val_accuracy: 0.6000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3442 - accuracy: 0.8500 - val_loss: 0.6605 - val_accuracy: 0.6000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3411 - accuracy: 0.8500 - val_loss: 0.6569 - val_accuracy: 0.6000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8500 - val_loss: 0.6522 - val_accuracy: 0.6250\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8562 - val_loss: 0.6469 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8687 - val_loss: 0.6415 - val_accuracy: 0.6250\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8687 - val_loss: 0.6359 - val_accuracy: 0.6250\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8687 - val_loss: 0.6305 - val_accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3306 - accuracy: 0.8687 - val_loss: 0.6254 - val_accuracy: 0.6250\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8687 - val_loss: 0.6207 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8750 - val_loss: 0.6166 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3284 - accuracy: 0.8750 - val_loss: 0.6131 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8750 - val_loss: 0.6100 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8813 - val_loss: 0.6075 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8813 - val_loss: 0.6054 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8813 - val_loss: 0.6037 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8813 - val_loss: 0.6024 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8813 - val_loss: 0.6013 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8813 - val_loss: 0.6005 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8813 - val_loss: 0.5998 - val_accuracy: 0.6750\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8813 - val_loss: 0.5993 - val_accuracy: 0.6750\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8813 - val_loss: 0.5989 - val_accuracy: 0.6750\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8813 - val_loss: 0.5986 - val_accuracy: 0.6750\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.6750\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.5982 - val_accuracy: 0.6750\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.5981 - val_accuracy: 0.6750\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.5980 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 0.5980 - val_accuracy: 0.7000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5980 - val_accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5982 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5982 - val_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5983 - val_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.5984 - val_accuracy: 0.7000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8750 - val_loss: 0.5985 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5986 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.5987 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5988 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8750 - val_loss: 0.5989 - val_accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5992 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 0.5993 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5994 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5996 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8750 - val_loss: 0.5998 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.5999 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 0.6000 - val_accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.6001 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 0.6002 - val_accuracy: 0.7000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6004 - val_accuracy: 0.7000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8750 - val_loss: 0.6005 - val_accuracy: 0.7000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 0.6006 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6007 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8750 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6010 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 0.6011 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6012 - val_accuracy: 0.7000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6014 - val_accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6015 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.6016 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6018 - val_accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.6019 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6021 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 0.6022 - val_accuracy: 0.7000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6023 - val_accuracy: 0.7000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8750 - val_loss: 0.6025 - val_accuracy: 0.7000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6026 - val_accuracy: 0.7000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 0.6028 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8750 - val_loss: 0.6029 - val_accuracy: 0.7000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8750 - val_loss: 0.6031 - val_accuracy: 0.7000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8750 - val_loss: 0.6032 - val_accuracy: 0.7000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8750 - val_loss: 0.6034 - val_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8750 - val_loss: 0.6035 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8750 - val_loss: 0.6037 - val_accuracy: 0.6750\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3228 - accuracy: 0.8750 - val_loss: 0.6038 - val_accuracy: 0.6750\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8750 - val_loss: 0.6040 - val_accuracy: 0.6750\n",
      "Start HMC\n",
      "Start HMC Burning\n",
      "Step 0\n",
      "Start HMC Training\n",
      "Epoch 1/100: - 0.7091s/step - train_loss: 0.6791 - train_acc: 0.5188 - val_loss: 0.6829 - val_acc: 0.4250\n",
      "Epoch 2/100: - 0.7099s/step - train_loss: 0.6704 - train_acc: 0.8000 - val_loss: 0.6825 - val_acc: 0.6000\n",
      "Epoch 3/100: - 0.7349s/step - train_loss: 0.6640 - train_acc: 0.8000 - val_loss: 0.6800 - val_acc: 0.5750\n",
      "Epoch 4/100: - 0.7430s/step - train_loss: 0.6587 - train_acc: 0.7000 - val_loss: 0.6757 - val_acc: 0.5750\n",
      "Epoch 5/100: - 0.7516s/step - train_loss: 0.6547 - train_acc: 0.6500 - val_loss: 0.6725 - val_acc: 0.5000\n",
      "Epoch 6/100: - 0.7492s/step - train_loss: 0.6507 - train_acc: 0.6500 - val_loss: 0.6697 - val_acc: 0.5000\n",
      "Epoch 7/100: - 0.7461s/step - train_loss: 0.6472 - train_acc: 0.6500 - val_loss: 0.6672 - val_acc: 0.5000\n",
      "Epoch 8/100: - 0.7486s/step - train_loss: 0.6433 - train_acc: 0.6937 - val_loss: 0.6649 - val_acc: 0.5500\n",
      "Epoch 9/100: - 0.7500s/step - train_loss: 0.6390 - train_acc: 0.7562 - val_loss: 0.6622 - val_acc: 0.6250\n",
      "Epoch 10/100: - 0.7517s/step - train_loss: 0.6349 - train_acc: 0.7812 - val_loss: 0.6592 - val_acc: 0.6250\n",
      "Epoch 11/100: - 0.7537s/step - train_loss: 0.6322 - train_acc: 0.7438 - val_loss: 0.6566 - val_acc: 0.6000\n",
      "Epoch 12/100: - 0.7537s/step - train_loss: 0.6305 - train_acc: 0.7375 - val_loss: 0.6553 - val_acc: 0.5750\n",
      "Epoch 13/100: - 0.7535s/step - train_loss: 0.6282 - train_acc: 0.7625 - val_loss: 0.6543 - val_acc: 0.6500\n",
      "Epoch 14/100: - 0.7563s/step - train_loss: 0.6257 - train_acc: 0.8063 - val_loss: 0.6536 - val_acc: 0.6000\n",
      "Epoch 15/100: - 0.7568s/step - train_loss: 0.6236 - train_acc: 0.8500 - val_loss: 0.6528 - val_acc: 0.6000\n",
      "Epoch 16/100: - 0.7582s/step - train_loss: 0.6212 - train_acc: 0.8500 - val_loss: 0.6511 - val_acc: 0.6000\n",
      "Epoch 17/100: - 0.7574s/step - train_loss: 0.6184 - train_acc: 0.8500 - val_loss: 0.6488 - val_acc: 0.6000\n",
      "Epoch 18/100: - 0.7575s/step - train_loss: 0.6151 - train_acc: 0.8438 - val_loss: 0.6465 - val_acc: 0.6000\n",
      "Epoch 19/100: - 0.7572s/step - train_loss: 0.6116 - train_acc: 0.8500 - val_loss: 0.6440 - val_acc: 0.6250\n",
      "Epoch 20/100: - 0.7566s/step - train_loss: 0.6090 - train_acc: 0.8500 - val_loss: 0.6420 - val_acc: 0.6250\n",
      "Epoch 21/100: - 0.7573s/step - train_loss: 0.6071 - train_acc: 0.8688 - val_loss: 0.6401 - val_acc: 0.6250\n",
      "Epoch 22/100: - 0.7585s/step - train_loss: 0.6056 - train_acc: 0.8750 - val_loss: 0.6375 - val_acc: 0.6500\n",
      "Epoch 23/100: - 0.7633s/step - train_loss: 0.6041 - train_acc: 0.8562 - val_loss: 0.6341 - val_acc: 0.6250\n",
      "Epoch 24/100: - 0.7654s/step - train_loss: 0.6030 - train_acc: 0.8500 - val_loss: 0.6310 - val_acc: 0.6500\n",
      "Epoch 25/100: - 0.7668s/step - train_loss: 0.6016 - train_acc: 0.8500 - val_loss: 0.6284 - val_acc: 0.6250\n",
      "Epoch 26/100: - 0.7694s/step - train_loss: 0.5998 - train_acc: 0.8500 - val_loss: 0.6262 - val_acc: 0.6250\n",
      "Epoch 27/100: - 0.7696s/step - train_loss: 0.5972 - train_acc: 0.8500 - val_loss: 0.6242 - val_acc: 0.6500\n",
      "Epoch 28/100: - 0.7691s/step - train_loss: 0.5940 - train_acc: 0.8625 - val_loss: 0.6224 - val_acc: 0.6250\n",
      "Epoch 29/100: - 0.7688s/step - train_loss: 0.5905 - train_acc: 0.8562 - val_loss: 0.6208 - val_acc: 0.6500\n",
      "Epoch 30/100: - 0.7692s/step - train_loss: 0.5867 - train_acc: 0.8562 - val_loss: 0.6188 - val_acc: 0.6500\n",
      "Epoch 31/100: - 0.7687s/step - train_loss: 0.5834 - train_acc: 0.8688 - val_loss: 0.6168 - val_acc: 0.6250\n",
      "Epoch 32/100: - 0.7716s/step - train_loss: 0.5809 - train_acc: 0.8625 - val_loss: 0.6156 - val_acc: 0.6250\n",
      "Epoch 33/100: - 0.7719s/step - train_loss: 0.5780 - train_acc: 0.8625 - val_loss: 0.6147 - val_acc: 0.6250\n",
      "Epoch 34/100: - 0.7711s/step - train_loss: 0.5751 - train_acc: 0.8625 - val_loss: 0.6143 - val_acc: 0.6250\n",
      "Epoch 35/100: - 0.7707s/step - train_loss: 0.5721 - train_acc: 0.8625 - val_loss: 0.6136 - val_acc: 0.6250\n",
      "Epoch 36/100: - 0.7721s/step - train_loss: 0.5689 - train_acc: 0.8562 - val_loss: 0.6127 - val_acc: 0.6250\n",
      "Epoch 37/100: - 0.7738s/step - train_loss: 0.5656 - train_acc: 0.8500 - val_loss: 0.6114 - val_acc: 0.6250\n",
      "Epoch 38/100: - 0.7764s/step - train_loss: 0.5618 - train_acc: 0.8625 - val_loss: 0.6093 - val_acc: 0.6500\n",
      "Epoch 39/100: - 0.7772s/step - train_loss: 0.5577 - train_acc: 0.8625 - val_loss: 0.6063 - val_acc: 0.6500\n",
      "Epoch 40/100: - 0.7778s/step - train_loss: 0.5537 - train_acc: 0.8625 - val_loss: 0.6026 - val_acc: 0.6500\n",
      "Epoch 41/100: - 0.7794s/step - train_loss: 0.5506 - train_acc: 0.8562 - val_loss: 0.5994 - val_acc: 0.6250\n",
      "Epoch 42/100: - 0.7812s/step - train_loss: 0.5476 - train_acc: 0.8625 - val_loss: 0.5969 - val_acc: 0.6500\n",
      "Epoch 43/100: - 0.7806s/step - train_loss: 0.5448 - train_acc: 0.8562 - val_loss: 0.5949 - val_acc: 0.6250\n",
      "Epoch 44/100: - 0.7799s/step - train_loss: 0.5418 - train_acc: 0.8625 - val_loss: 0.5934 - val_acc: 0.6250\n",
      "Epoch 45/100: - 0.7808s/step - train_loss: 0.5383 - train_acc: 0.8625 - val_loss: 0.5919 - val_acc: 0.6250\n",
      "Epoch 46/100: - 0.7811s/step - train_loss: 0.5345 - train_acc: 0.8500 - val_loss: 0.5907 - val_acc: 0.6250\n",
      "Epoch 47/100: - 0.7811s/step - train_loss: 0.5311 - train_acc: 0.8688 - val_loss: 0.5898 - val_acc: 0.6500\n",
      "Epoch 48/100: - 0.7812s/step - train_loss: 0.5286 - train_acc: 0.8625 - val_loss: 0.5887 - val_acc: 0.6500\n",
      "Epoch 49/100: - 0.7815s/step - train_loss: 0.5270 - train_acc: 0.8625 - val_loss: 0.5876 - val_acc: 0.6500\n",
      "Epoch 50/100: - 0.7813s/step - train_loss: 0.5255 - train_acc: 0.8625 - val_loss: 0.5866 - val_acc: 0.6500\n",
      "Epoch 51/100: - 0.7810s/step - train_loss: 0.5238 - train_acc: 0.8750 - val_loss: 0.5857 - val_acc: 0.6500\n",
      "Epoch 52/100: - 0.7803s/step - train_loss: 0.5219 - train_acc: 0.8750 - val_loss: 0.5849 - val_acc: 0.6500\n",
      "Epoch 53/100: - 0.7788s/step - train_loss: 0.5202 - train_acc: 0.8750 - val_loss: 0.5841 - val_acc: 0.6500\n",
      "Epoch 54/100: - 0.7784s/step - train_loss: 0.5185 - train_acc: 0.8750 - val_loss: 0.5832 - val_acc: 0.6500\n",
      "Epoch 55/100: - 0.7785s/step - train_loss: 0.5167 - train_acc: 0.8750 - val_loss: 0.5828 - val_acc: 0.6250\n",
      "Epoch 56/100: - 0.7793s/step - train_loss: 0.5154 - train_acc: 0.8688 - val_loss: 0.5827 - val_acc: 0.6250\n",
      "Epoch 57/100: - 0.7804s/step - train_loss: 0.5144 - train_acc: 0.8750 - val_loss: 0.5820 - val_acc: 0.6250\n",
      "Epoch 58/100: - 0.7803s/step - train_loss: 0.5136 - train_acc: 0.8688 - val_loss: 0.5812 - val_acc: 0.6250\n",
      "Epoch 59/100: - 0.7811s/step - train_loss: 0.5130 - train_acc: 0.8750 - val_loss: 0.5807 - val_acc: 0.6500\n",
      "Epoch 60/100: - 0.7814s/step - train_loss: 0.5121 - train_acc: 0.8688 - val_loss: 0.5807 - val_acc: 0.6250\n",
      "Epoch 61/100: - 0.7818s/step - train_loss: 0.5107 - train_acc: 0.8688 - val_loss: 0.5811 - val_acc: 0.6250\n",
      "Epoch 62/100: - 0.7823s/step - train_loss: 0.5089 - train_acc: 0.8688 - val_loss: 0.5812 - val_acc: 0.6250\n",
      "Epoch 63/100: - 0.7812s/step - train_loss: 0.5071 - train_acc: 0.8688 - val_loss: 0.5809 - val_acc: 0.6000\n",
      "Epoch 64/100: - 0.7804s/step - train_loss: 0.5058 - train_acc: 0.8562 - val_loss: 0.5812 - val_acc: 0.6250\n",
      "Epoch 65/100: - 0.7796s/step - train_loss: 0.5037 - train_acc: 0.8500 - val_loss: 0.5819 - val_acc: 0.6250\n",
      "Epoch 66/100: - 0.7787s/step - train_loss: 0.5000 - train_acc: 0.8625 - val_loss: 0.5816 - val_acc: 0.6000\n",
      "Epoch 67/100: - 0.7778s/step - train_loss: 0.4961 - train_acc: 0.8688 - val_loss: 0.5810 - val_acc: 0.6000\n",
      "Epoch 68/100: - 0.7769s/step - train_loss: 0.4933 - train_acc: 0.8688 - val_loss: 0.5803 - val_acc: 0.6000\n",
      "Epoch 69/100: - 0.7761s/step - train_loss: 0.4908 - train_acc: 0.8625 - val_loss: 0.5789 - val_acc: 0.6000\n",
      "Epoch 70/100: - 0.7752s/step - train_loss: 0.4885 - train_acc: 0.8625 - val_loss: 0.5772 - val_acc: 0.6000\n",
      "Epoch 71/100: - 0.7743s/step - train_loss: 0.4866 - train_acc: 0.8562 - val_loss: 0.5754 - val_acc: 0.6250\n",
      "Epoch 72/100: - 0.7730s/step - train_loss: 0.4844 - train_acc: 0.8688 - val_loss: 0.5735 - val_acc: 0.6000\n",
      "Epoch 73/100: - 0.7721s/step - train_loss: 0.4815 - train_acc: 0.8750 - val_loss: 0.5714 - val_acc: 0.6000\n",
      "Epoch 74/100: - 0.7713s/step - train_loss: 0.4791 - train_acc: 0.8625 - val_loss: 0.5698 - val_acc: 0.6250\n",
      "Epoch 75/100: - 0.7706s/step - train_loss: 0.4768 - train_acc: 0.8500 - val_loss: 0.5690 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: - 0.7699s/step - train_loss: 0.4730 - train_acc: 0.8625 - val_loss: 0.5676 - val_acc: 0.6000\n",
      "Epoch 77/100: - 0.7692s/step - train_loss: 0.4689 - train_acc: 0.8750 - val_loss: 0.5658 - val_acc: 0.6000\n",
      "Epoch 78/100: - 0.7685s/step - train_loss: 0.4658 - train_acc: 0.8625 - val_loss: 0.5643 - val_acc: 0.6250\n",
      "Epoch 79/100: - 0.7677s/step - train_loss: 0.4642 - train_acc: 0.8500 - val_loss: 0.5628 - val_acc: 0.6250\n",
      "Epoch 80/100: - 0.7678s/step - train_loss: 0.4632 - train_acc: 0.8500 - val_loss: 0.5605 - val_acc: 0.6250\n",
      "Epoch 81/100: - 0.7683s/step - train_loss: 0.4624 - train_acc: 0.8688 - val_loss: 0.5578 - val_acc: 0.6250\n",
      "Epoch 82/100: - 0.7690s/step - train_loss: 0.4631 - train_acc: 0.8750 - val_loss: 0.5570 - val_acc: 0.6000\n",
      "Epoch 83/100: - 0.7692s/step - train_loss: 0.4644 - train_acc: 0.8688 - val_loss: 0.5572 - val_acc: 0.6250\n",
      "Epoch 84/100: - 0.7692s/step - train_loss: 0.4655 - train_acc: 0.8688 - val_loss: 0.5579 - val_acc: 0.6250\n",
      "Epoch 85/100: - 0.7692s/step - train_loss: 0.4658 - train_acc: 0.8688 - val_loss: 0.5580 - val_acc: 0.6250\n",
      "Epoch 86/100: - 0.7691s/step - train_loss: 0.4656 - train_acc: 0.8688 - val_loss: 0.5576 - val_acc: 0.6250\n",
      "Epoch 87/100: - 0.7690s/step - train_loss: 0.4652 - train_acc: 0.8750 - val_loss: 0.5569 - val_acc: 0.6250\n",
      "Epoch 88/100: - 0.7689s/step - train_loss: 0.4650 - train_acc: 0.8750 - val_loss: 0.5564 - val_acc: 0.6250\n",
      "Epoch 89/100: - 0.7688s/step - train_loss: 0.4643 - train_acc: 0.8750 - val_loss: 0.5559 - val_acc: 0.6250\n",
      "Epoch 90/100: - 0.7688s/step - train_loss: 0.4633 - train_acc: 0.8688 - val_loss: 0.5551 - val_acc: 0.6250\n",
      "Epoch 91/100: - 0.7690s/step - train_loss: 0.4620 - train_acc: 0.8688 - val_loss: 0.5540 - val_acc: 0.6250\n",
      "Epoch 92/100: - 0.7688s/step - train_loss: 0.4606 - train_acc: 0.8688 - val_loss: 0.5533 - val_acc: 0.6250\n",
      "Epoch 93/100: - 0.7692s/step - train_loss: 0.4597 - train_acc: 0.8562 - val_loss: 0.5532 - val_acc: 0.6250\n",
      "Epoch 94/100: - 0.7695s/step - train_loss: 0.4588 - train_acc: 0.8562 - val_loss: 0.5533 - val_acc: 0.6250\n",
      "Epoch 95/100: - 0.7694s/step - train_loss: 0.4579 - train_acc: 0.8562 - val_loss: 0.5534 - val_acc: 0.6250\n",
      "Epoch 96/100: - 0.7693s/step - train_loss: 0.4565 - train_acc: 0.8688 - val_loss: 0.5531 - val_acc: 0.6250\n",
      "Epoch 97/100: - 0.7693s/step - train_loss: 0.4558 - train_acc: 0.8688 - val_loss: 0.5538 - val_acc: 0.6250\n",
      "Epoch 98/100: - 0.7692s/step - train_loss: 0.4556 - train_acc: 0.8688 - val_loss: 0.5551 - val_acc: 0.6250\n",
      "Epoch 99/100: - 0.7691s/step - train_loss: 0.4551 - train_acc: 0.8688 - val_loss: 0.5563 - val_acc: 0.6000\n",
      "Epoch 100/100: - 0.7689s/step - train_loss: 0.4544 - train_acc: 0.8750 - val_loss: 0.5572 - val_acc: 0.6000\n",
      "Start Gibbs\n",
      "Start Gibbs Burning\n",
      "Step 0\n",
      "Epoch 1/100: - 2.4178s/step - train_loss: 0.6542 - train_acc: 0.8688 - val_loss: 0.6624 - val_acc: 0.7250\n",
      "Epoch 2/100: - 2.4587s/step - train_loss: 0.6587 - train_acc: 0.6125 - val_loss: 0.6714 - val_acc: 0.6500\n",
      "Epoch 3/100: - 2.4660s/step - train_loss: 0.6542 - train_acc: 0.8250 - val_loss: 0.6606 - val_acc: 0.6250\n",
      "Epoch 4/100: - 2.4604s/step - train_loss: 0.6564 - train_acc: 0.5625 - val_loss: 0.6593 - val_acc: 0.5250\n",
      "Epoch 5/100: - 2.4668s/step - train_loss: 0.6576 - train_acc: 0.7250 - val_loss: 0.6625 - val_acc: 0.5750\n",
      "Epoch 6/100: - 2.4540s/step - train_loss: 0.6597 - train_acc: 0.8688 - val_loss: 0.6682 - val_acc: 0.6750\n",
      "Epoch 7/100: - 2.4504s/step - train_loss: 0.6622 - train_acc: 0.8688 - val_loss: 0.6697 - val_acc: 0.6750\n",
      "Epoch 8/100: - 2.4512s/step - train_loss: 0.6642 - train_acc: 0.7812 - val_loss: 0.6694 - val_acc: 0.6000\n",
      "Epoch 9/100: - 2.4492s/step - train_loss: 0.6673 - train_acc: 0.6750 - val_loss: 0.6711 - val_acc: 0.5500\n",
      "Epoch 10/100: - 2.4467s/step - train_loss: 0.6693 - train_acc: 0.7312 - val_loss: 0.6739 - val_acc: 0.5750\n",
      "Epoch 11/100: - 2.4478s/step - train_loss: 0.6707 - train_acc: 0.8250 - val_loss: 0.6758 - val_acc: 0.6250\n",
      "Epoch 12/100: - 2.4479s/step - train_loss: 0.6728 - train_acc: 0.7312 - val_loss: 0.6763 - val_acc: 0.5750\n",
      "Epoch 13/100: - 2.4461s/step - train_loss: 0.6754 - train_acc: 0.5813 - val_loss: 0.6765 - val_acc: 0.5250\n",
      "Epoch 14/100: - 2.4430s/step - train_loss: 0.6774 - train_acc: 0.5625 - val_loss: 0.6775 - val_acc: 0.5250\n",
      "Epoch 15/100: - 2.4415s/step - train_loss: 0.6782 - train_acc: 0.5813 - val_loss: 0.6785 - val_acc: 0.5250\n",
      "Epoch 16/100: - 2.4406s/step - train_loss: 0.6785 - train_acc: 0.5813 - val_loss: 0.6789 - val_acc: 0.5250\n",
      "Epoch 17/100: - 2.4413s/step - train_loss: 0.6786 - train_acc: 0.6562 - val_loss: 0.6794 - val_acc: 0.5500\n",
      "Epoch 18/100: - 2.4410s/step - train_loss: 0.6791 - train_acc: 0.5250 - val_loss: 0.6796 - val_acc: 0.4750\n",
      "Epoch 19/100: - 2.4403s/step - train_loss: 0.6796 - train_acc: 0.5250 - val_loss: 0.6799 - val_acc: 0.4250\n",
      "Epoch 20/100: - 2.4417s/step - train_loss: 0.6781 - train_acc: 0.6062 - val_loss: 0.6806 - val_acc: 0.5500\n",
      "Epoch 21/100: - 2.4406s/step - train_loss: 0.6772 - train_acc: 0.8125 - val_loss: 0.6807 - val_acc: 0.6000\n",
      "Epoch 22/100: - 2.4409s/step - train_loss: 0.6762 - train_acc: 0.6562 - val_loss: 0.6792 - val_acc: 0.5500\n",
      "Epoch 23/100: - 2.4403s/step - train_loss: 0.6743 - train_acc: 0.6625 - val_loss: 0.6775 - val_acc: 0.5500\n",
      "Epoch 24/100: - 2.4422s/step - train_loss: 0.6722 - train_acc: 0.7000 - val_loss: 0.6758 - val_acc: 0.5750\n",
      "Epoch 25/100: - 2.4415s/step - train_loss: 0.6709 - train_acc: 0.7625 - val_loss: 0.6755 - val_acc: 0.6000\n",
      "Epoch 26/100: - 2.4413s/step - train_loss: 0.6683 - train_acc: 0.8250 - val_loss: 0.6748 - val_acc: 0.6250\n",
      "Epoch 27/100: - 2.4406s/step - train_loss: 0.6661 - train_acc: 0.7500 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 28/100: - 2.4411s/step - train_loss: 0.6669 - train_acc: 0.6438 - val_loss: 0.6729 - val_acc: 0.5500\n",
      "Epoch 29/100: - 2.4422s/step - train_loss: 0.6678 - train_acc: 0.6500 - val_loss: 0.6736 - val_acc: 0.5500\n",
      "Epoch 30/100: - 2.4417s/step - train_loss: 0.6690 - train_acc: 0.7500 - val_loss: 0.6751 - val_acc: 0.6000\n",
      "Epoch 31/100: - 2.4397s/step - train_loss: 0.6707 - train_acc: 0.7063 - val_loss: 0.6756 - val_acc: 0.5750\n",
      "Epoch 32/100: - 2.4407s/step - train_loss: 0.6715 - train_acc: 0.6000 - val_loss: 0.6756 - val_acc: 0.5250\n",
      "Epoch 33/100: - 2.4387s/step - train_loss: 0.6724 - train_acc: 0.6000 - val_loss: 0.6758 - val_acc: 0.5250\n",
      "Epoch 34/100: - 2.4358s/step - train_loss: 0.6729 - train_acc: 0.8250 - val_loss: 0.6768 - val_acc: 0.6250\n",
      "Epoch 35/100: - 2.4350s/step - train_loss: 0.6722 - train_acc: 0.7063 - val_loss: 0.6761 - val_acc: 0.5750\n",
      "Epoch 36/100: - 2.4366s/step - train_loss: 0.6700 - train_acc: 0.6062 - val_loss: 0.6745 - val_acc: 0.5500\n",
      "Epoch 37/100: - 2.4364s/step - train_loss: 0.6677 - train_acc: 0.6875 - val_loss: 0.6738 - val_acc: 0.5500\n",
      "Epoch 38/100: - 2.4363s/step - train_loss: 0.6660 - train_acc: 0.8063 - val_loss: 0.6735 - val_acc: 0.6000\n",
      "Epoch 39/100: - 2.4363s/step - train_loss: 0.6648 - train_acc: 0.8250 - val_loss: 0.6727 - val_acc: 0.6250\n",
      "Epoch 40/100: - 2.4360s/step - train_loss: 0.6635 - train_acc: 0.7188 - val_loss: 0.6711 - val_acc: 0.6000\n",
      "Epoch 41/100: - 2.4356s/step - train_loss: 0.6618 - train_acc: 0.7438 - val_loss: 0.6699 - val_acc: 0.6000\n",
      "Epoch 42/100: - 2.4354s/step - train_loss: 0.6607 - train_acc: 0.8500 - val_loss: 0.6698 - val_acc: 0.6500\n",
      "Epoch 43/100: - 2.4352s/step - train_loss: 0.6621 - train_acc: 0.8562 - val_loss: 0.6707 - val_acc: 0.6500\n",
      "Epoch 44/100: - 2.4345s/step - train_loss: 0.6621 - train_acc: 0.7125 - val_loss: 0.6691 - val_acc: 0.6000\n",
      "Epoch 45/100: - 2.4344s/step - train_loss: 0.6613 - train_acc: 0.6062 - val_loss: 0.6673 - val_acc: 0.5500\n",
      "Epoch 46/100: - 2.4339s/step - train_loss: 0.6596 - train_acc: 0.6687 - val_loss: 0.6668 - val_acc: 0.5500\n",
      "Epoch 47/100: - 2.4337s/step - train_loss: 0.6581 - train_acc: 0.8625 - val_loss: 0.6686 - val_acc: 0.6250\n",
      "Epoch 48/100: - 2.4337s/step - train_loss: 0.6589 - train_acc: 0.8500 - val_loss: 0.6707 - val_acc: 0.6500\n",
      "Epoch 49/100: - 2.4343s/step - train_loss: 0.6597 - train_acc: 0.8688 - val_loss: 0.6697 - val_acc: 0.6750\n",
      "Epoch 50/100: - 2.4343s/step - train_loss: 0.6616 - train_acc: 0.7000 - val_loss: 0.6681 - val_acc: 0.5750\n",
      "Epoch 51/100: - 2.4339s/step - train_loss: 0.6629 - train_acc: 0.6000 - val_loss: 0.6675 - val_acc: 0.5250\n",
      "Epoch 52/100: - 2.4336s/step - train_loss: 0.6631 - train_acc: 0.8250 - val_loss: 0.6686 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: - 2.4336s/step - train_loss: 0.6635 - train_acc: 0.8438 - val_loss: 0.6690 - val_acc: 0.6250\n",
      "Epoch 54/100: - 2.4331s/step - train_loss: 0.6631 - train_acc: 0.6375 - val_loss: 0.6682 - val_acc: 0.5500\n",
      "Epoch 55/100: - 2.4333s/step - train_loss: 0.6637 - train_acc: 0.5563 - val_loss: 0.6696 - val_acc: 0.5000\n",
      "Epoch 56/100: - 2.4333s/step - train_loss: 0.6635 - train_acc: 0.8125 - val_loss: 0.6724 - val_acc: 0.6500\n",
      "Epoch 57/100: - 2.4334s/step - train_loss: 0.6638 - train_acc: 0.8500 - val_loss: 0.6736 - val_acc: 0.6250\n",
      "Epoch 58/100: - 2.4348s/step - train_loss: 0.6634 - train_acc: 0.8125 - val_loss: 0.6721 - val_acc: 0.6500\n",
      "Epoch 59/100: - 2.4347s/step - train_loss: 0.6619 - train_acc: 0.7562 - val_loss: 0.6703 - val_acc: 0.6000\n",
      "Epoch 60/100: - 2.4346s/step - train_loss: 0.6610 - train_acc: 0.8250 - val_loss: 0.6698 - val_acc: 0.6500\n",
      "Epoch 61/100: - 2.4353s/step - train_loss: 0.6604 - train_acc: 0.8125 - val_loss: 0.6685 - val_acc: 0.6000\n",
      "Epoch 62/100: - 2.4416s/step - train_loss: 0.6591 - train_acc: 0.7125 - val_loss: 0.6672 - val_acc: 0.6000\n",
      "Epoch 63/100: - 2.4451s/step - train_loss: 0.6570 - train_acc: 0.8625 - val_loss: 0.6670 - val_acc: 0.6250\n",
      "Epoch 64/100: - 2.4477s/step - train_loss: 0.6559 - train_acc: 0.8750 - val_loss: 0.6663 - val_acc: 0.6750\n",
      "Epoch 65/100: - 2.4466s/step - train_loss: 0.6555 - train_acc: 0.8313 - val_loss: 0.6632 - val_acc: 0.6250\n",
      "Epoch 66/100: - 2.4458s/step - train_loss: 0.6550 - train_acc: 0.7688 - val_loss: 0.6614 - val_acc: 0.6000\n",
      "Epoch 67/100: - 2.4439s/step - train_loss: 0.6542 - train_acc: 0.8125 - val_loss: 0.6607 - val_acc: 0.6250\n",
      "Epoch 68/100: - 2.4424s/step - train_loss: 0.6527 - train_acc: 0.8375 - val_loss: 0.6612 - val_acc: 0.6250\n",
      "Epoch 69/100: - 2.4412s/step - train_loss: 0.6498 - train_acc: 0.8313 - val_loss: 0.6597 - val_acc: 0.6500\n",
      "Epoch 70/100: - 2.4397s/step - train_loss: 0.6488 - train_acc: 0.8313 - val_loss: 0.6596 - val_acc: 0.6500\n",
      "Epoch 71/100: - 2.4386s/step - train_loss: 0.6488 - train_acc: 0.8313 - val_loss: 0.6599 - val_acc: 0.6500\n",
      "Epoch 72/100: - 2.4374s/step - train_loss: 0.6486 - train_acc: 0.8500 - val_loss: 0.6601 - val_acc: 0.6500\n",
      "Epoch 73/100: - 2.4364s/step - train_loss: 0.6474 - train_acc: 0.8688 - val_loss: 0.6598 - val_acc: 0.6500\n",
      "Epoch 74/100: - 2.4397s/step - train_loss: 0.6461 - train_acc: 0.8688 - val_loss: 0.6593 - val_acc: 0.6500\n",
      "Epoch 75/100: - 2.4398s/step - train_loss: 0.6440 - train_acc: 0.8500 - val_loss: 0.6574 - val_acc: 0.6500\n",
      "Epoch 76/100: - 2.4390s/step - train_loss: 0.6414 - train_acc: 0.8562 - val_loss: 0.6553 - val_acc: 0.6500\n",
      "Epoch 77/100: - 2.4391s/step - train_loss: 0.6393 - train_acc: 0.8750 - val_loss: 0.6549 - val_acc: 0.6750\n",
      "Epoch 78/100: - 2.4391s/step - train_loss: 0.6378 - train_acc: 0.8688 - val_loss: 0.6537 - val_acc: 0.6250\n",
      "Epoch 79/100: - 2.4384s/step - train_loss: 0.6390 - train_acc: 0.8625 - val_loss: 0.6541 - val_acc: 0.6500\n",
      "Epoch 80/100: - 2.4393s/step - train_loss: 0.6407 - train_acc: 0.8125 - val_loss: 0.6553 - val_acc: 0.6250\n",
      "Epoch 81/100: - 2.4406s/step - train_loss: 0.6423 - train_acc: 0.8313 - val_loss: 0.6579 - val_acc: 0.6500\n",
      "Epoch 82/100: - 2.4428s/step - train_loss: 0.6419 - train_acc: 0.8688 - val_loss: 0.6596 - val_acc: 0.6500\n",
      "Epoch 83/100: - 2.4446s/step - train_loss: 0.6409 - train_acc: 0.8750 - val_loss: 0.6578 - val_acc: 0.6750\n",
      "Epoch 84/100: - 2.4482s/step - train_loss: 0.6409 - train_acc: 0.8250 - val_loss: 0.6539 - val_acc: 0.6500\n",
      "Epoch 85/100: - 2.4537s/step - train_loss: 0.6388 - train_acc: 0.8250 - val_loss: 0.6522 - val_acc: 0.6000\n",
      "Epoch 86/100: - 2.4575s/step - train_loss: 0.6373 - train_acc: 0.8750 - val_loss: 0.6532 - val_acc: 0.6750\n",
      "Epoch 87/100: - 2.4592s/step - train_loss: 0.6382 - train_acc: 0.8750 - val_loss: 0.6541 - val_acc: 0.6750\n",
      "Epoch 88/100: - 2.4616s/step - train_loss: 0.6376 - train_acc: 0.8500 - val_loss: 0.6528 - val_acc: 0.6250\n",
      "Epoch 89/100: - 2.4647s/step - train_loss: 0.6367 - train_acc: 0.8625 - val_loss: 0.6519 - val_acc: 0.6500\n",
      "Epoch 90/100: - 2.4695s/step - train_loss: 0.6367 - train_acc: 0.8688 - val_loss: 0.6524 - val_acc: 0.6750\n",
      "Epoch 91/100: - 2.4725s/step - train_loss: 0.6366 - train_acc: 0.8688 - val_loss: 0.6521 - val_acc: 0.6500\n",
      "Epoch 92/100: - 2.4749s/step - train_loss: 0.6368 - train_acc: 0.8562 - val_loss: 0.6515 - val_acc: 0.6500\n",
      "Epoch 93/100: - 2.4776s/step - train_loss: 0.6368 - train_acc: 0.8500 - val_loss: 0.6513 - val_acc: 0.6250\n",
      "Epoch 94/100: - 2.4785s/step - train_loss: 0.6369 - train_acc: 0.8750 - val_loss: 0.6526 - val_acc: 0.6750\n",
      "Epoch 95/100: - 2.4776s/step - train_loss: 0.6376 - train_acc: 0.8688 - val_loss: 0.6531 - val_acc: 0.7000\n",
      "Epoch 96/100: - 2.4777s/step - train_loss: 0.6357 - train_acc: 0.8688 - val_loss: 0.6502 - val_acc: 0.6750\n",
      "Epoch 97/100: - 2.4800s/step - train_loss: 0.6361 - train_acc: 0.8250 - val_loss: 0.6495 - val_acc: 0.6250\n",
      "Epoch 98/100: - 2.4808s/step - train_loss: 0.6366 - train_acc: 0.8250 - val_loss: 0.6507 - val_acc: 0.6250\n",
      "Epoch 99/100: - 2.4823s/step - train_loss: 0.6376 - train_acc: 0.8500 - val_loss: 0.6527 - val_acc: 0.6500\n",
      "Epoch 100/100: - 2.4824s/step - train_loss: 0.6385 - train_acc: 0.8812 - val_loss: 0.6546 - val_acc: 0.6500\n"
     ]
    }
   ],
   "source": [
    "# running multiple times\n",
    "N = 5\n",
    "size = 32\n",
    "epochs = 100\n",
    "burnin = 100\n",
    "\n",
    "res_bp, res_hmc, res_gibbs = [], [], []\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    time_bp, history_bp = standard_backprop(size, train_ds, val_ds, epochs)\n",
    "    burnin_loss_hmc, time_hmc, history_hmc = hmc(size, train_ds, val_ds, epochs, burnin)\n",
    "    burnin_loss_gibbs, time_gibbs, history_gibbs = gibbs(size, train_ds, val_ds, epochs, burnin)\n",
    "    \n",
    "    hist_bp = {\"train_acc\": history_bp.history['accuracy'], \"train_loss\": history_bp.history['loss'], \n",
    "               \"val_acc\": history_bp.history['val_accuracy'], \"val_loss\": history_bp.history['val_loss']}\n",
    "    rbp = {'time': time_bp, 'history': hist_bp}\n",
    "    rhmc = {'time': time_hmc, 'burnin': burnin_loss_hmc, 'history': history_hmc}\n",
    "    rgibbs = {'time': time_gibbs, 'burnin': burnin_loss_gibbs, 'history': history_gibbs}\n",
    "    \n",
    "    res_bp.append(rbp)\n",
    "    res_hmc.append(rhmc)\n",
    "    res_gibbs.append(rgibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50a67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bp_tmp = []\n",
    "for i in range(N):\n",
    "    hist_bp = {\"train_acc\": res_bp[i]['history']['train_acc'], \"train_loss\": res_bp[i]['history']['train_loss'], \n",
    "               \"val_acc\": res_bp[i]['history']['val_acc'], \"val_loss\": res_bp[i]['history']['val_loss']}\n",
    "    r_bp = {\"time\": res_bp[i]['time'], \"history\": hist_bp}\n",
    "    res_bp_tmp.append(r_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce4e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the running times for each method\n",
    "def plot_all(res, method, metric):\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    nrow = 3\n",
    "    ncol = 2\n",
    "    \n",
    "    fig, ax = plt.subplots(nrow, ncol, sharex = True)\n",
    "    fig.suptitle(method + \"_\" + metric)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            if i * ncol + j < N:\n",
    "                ax[i, j].plot(res[i * ncol + j]['history'][metric])\n",
    "                ax[i, j].set_title(f\"Run {i * ncol + j}\")\n",
    "    plt.savefig(method + \"_\" + metric + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "res_bp = res_bp_tmp\n",
    "\n",
    "res_all = [res_bp, res_hmc, res_gibbs]\n",
    "methods = ['bp', 'hmc', 'gibbs']\n",
    "metrics = ['train_acc', 'train_loss', 'val_acc', 'val_loss']\n",
    "for i, method in enumerate(methods):\n",
    "    for metric in metrics:\n",
    "        plot_all(res_all[i], method, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ffabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average curve for each method\n",
    "def cal_avg(res):\n",
    "    \n",
    "    metrics = ['train_acc', 'train_loss', 'val_acc', 'val_loss']\n",
    "    avg = {}\n",
    "    for metric in metrics:\n",
    "        arr_metric = np.zeros((N, epochs))\n",
    "        for i in range(N):\n",
    "            arr_metric[i] = np.array(res[i]['history'][metric])\n",
    "        avg_metric = np.mean(arr_metric, axis = 0)\n",
    "        avg[metric] = avg_metric\n",
    "        \n",
    "    return avg\n",
    "\n",
    "avg_bp = cal_avg(res_bp)\n",
    "avg_hmc = cal_avg(res_hmc)\n",
    "avg_gibbs = cal_avg(res_gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0004225",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_all = [avg_bp, avg_hmc, avg_gibbs]\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "for metric in metrics:\n",
    "    for i, method in enumerate(methods):\n",
    "        plt.plot(avg_all[i][metric], label = method)\n",
    "    plt.title(metric)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric.split(\"_\")[1])\n",
    "    plt.legend()\n",
    "    plt.savefig(\"average_\" + metric + '.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62934e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bp, time_hmc, time_gibbs = [], [], []\n",
    "for i in range(N):\n",
    "    time_bp.append(res_bp[i]['time'])\n",
    "    time_hmc.append(res_hmc[i]['time'])\n",
    "    time_gibbs.append(res_gibbs[i]['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9956fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('time.npy', 'wb') as f:\n",
    "    np.save(f, np.array(res_all))\n",
    "    np.save(f, np.array(time_bp))\n",
    "    np.save(f, np.array(time_hmc))\n",
    "    np.save(f, np.array(time_gibbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6812df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
