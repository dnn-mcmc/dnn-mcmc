{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b3779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.math as tm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69dbf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 13:55:57.226609: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "X, Y = make_moons(100, noise = 0.3)\n",
    "\n",
    "# Split into test and training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=73)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73baa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h]\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            logits = layer(pv)\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=logits)\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def gibbs_new_state(self, x, h, y):\n",
    "        \n",
    "        '''\n",
    "            generate a new state for the network node by node in Gibbs setting.\n",
    "        '''\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "\n",
    "            # node by node\n",
    "            \n",
    "            nodes = tf.transpose(cv)\n",
    "            prob_parents = tm.sigmoid(in_layer(pv))\n",
    "            \n",
    "            out_layer_weights = out_layer.get_weights()[0]\n",
    "            \n",
    "            next_logits = out_layer(cv)\n",
    "            \n",
    "            new_layer = []\n",
    "            \n",
    "            for j, node in enumerate(nodes):\n",
    "                \n",
    "                # get info for current node (i, j)\n",
    "                \n",
    "                prob_parents_j = prob_parents[:, j]\n",
    "                out_layer_weights_j = out_layer_weights[j]\n",
    "                \n",
    "                # calculate logits and logprob for node is 0 or 1\n",
    "                next_logits_if_node_0 = next_logits[:, :] - node[:, None] * out_layer_weights_j[None, :]\n",
    "                next_logits_if_node_1 = next_logits[:, :] + (1 - node[:, None]) * out_layer_weights_j[None, :]\n",
    "                \n",
    "                #print(next_logits_if_node_0, next_logits_if_node_1)\n",
    "                \n",
    "                logprob_children_if_node_0 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_0), axis = -1)\n",
    "                \n",
    "                logprob_children_if_node_1 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.cast(nv, dtype = tf.float32), logits=next_logits_if_node_1), axis = -1)\n",
    "                \n",
    "                # calculate prob for node (i, j)\n",
    "                prob_0 = (1 - prob_parents_j) * tm.exp(logprob_children_if_node_0)\n",
    "                prob_1 = prob_parents_j * tm.exp(logprob_children_if_node_1)\n",
    "                prob_j = prob_1 / (prob_1 + prob_0)\n",
    "            \n",
    "                # sample new state with prob_j for node (i, j)\n",
    "                new_node = tfp.distributions.Bernoulli(probs = prob_j).sample() # MAY BE SLOW\n",
    "                \n",
    "                # update nodes and logits for following calculation\n",
    "                new_node_casted = tf.cast(new_node, dtype = \"float32\")\n",
    "                next_logits = next_logits_if_node_0 * (1 - new_node_casted)[:, None] \\\n",
    "                            + next_logits_if_node_1 * new_node_casted[:, None] \n",
    "                \n",
    "                # keep track of new node values (in prev/curr/next_vals and h_new)\n",
    "                new_layer.append(new_node)\n",
    "           \n",
    "            new_layer = tf.transpose(new_layer)\n",
    "            h_current[i] = new_layer\n",
    "            prev_vals = [x] + h_current[:-1]\n",
    "            curr_vals = h_current\n",
    "            next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        return h_current\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    \n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tm.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tm.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tm.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f95e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layer_sizes = [32], n_outputs=1)\n",
    "network = [model.call(x) for x, y in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba131fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Step 10\n",
      "Step 20\n",
      "Step 30\n",
      "Step 40\n"
     ]
    }
   ],
   "source": [
    "# Gibbs Burnin\n",
    "burnin = 50\n",
    "\n",
    "for i in range(burnin):\n",
    "    \n",
    "    if(i % 10 == 0): print(\"Step %d\" % i)\n",
    "    network = [model.gibbs_new_state(images, net, labels) for (images, labels), net in zip(train_ds, network)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83493df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: - 0.8365s/step - loss: 67.2969 - accuracy: 0.5500\n",
      "Epoch 2/50: - 0.8042s/step - loss: 66.4549 - accuracy: 0.5500\n",
      "Epoch 3/50: - 0.7865s/step - loss: 66.1563 - accuracy: 0.5500\n",
      "Epoch 4/50: - 0.7777s/step - loss: 64.2668 - accuracy: 0.5500\n",
      "Epoch 5/50: - 0.7729s/step - loss: 64.9815 - accuracy: 0.5500\n",
      "Epoch 6/50: - 0.7720s/step - loss: 64.7212 - accuracy: 0.4750\n",
      "Epoch 7/50: - 0.7785s/step - loss: 62.7816 - accuracy: 0.7875\n",
      "Epoch 8/50: - 0.7750s/step - loss: 62.7981 - accuracy: 0.5000\n",
      "Epoch 9/50: - 0.7721s/step - loss: 61.4250 - accuracy: 0.5500\n",
      "Epoch 10/50: - 0.7697s/step - loss: 62.3493 - accuracy: 0.7750\n",
      "Epoch 11/50: - 0.7682s/step - loss: 61.5713 - accuracy: 0.5500\n",
      "Epoch 12/50: - 0.7671s/step - loss: 61.2463 - accuracy: 0.6875\n",
      "Epoch 13/50: - 0.7660s/step - loss: 61.5908 - accuracy: 0.8000\n",
      "Epoch 14/50: - 0.7652s/step - loss: 60.6586 - accuracy: 0.7125\n",
      "Epoch 15/50: - 0.7645s/step - loss: 61.6575 - accuracy: 0.6375\n",
      "Epoch 16/50: - 0.7641s/step - loss: 59.5514 - accuracy: 0.8000\n",
      "Epoch 17/50: - 0.7634s/step - loss: 60.4481 - accuracy: 0.8250\n",
      "Epoch 18/50: - 0.7623s/step - loss: 59.5224 - accuracy: 0.7000\n",
      "Epoch 19/50: - 0.7612s/step - loss: 59.8897 - accuracy: 0.7750\n",
      "Epoch 20/50: - 0.7603s/step - loss: 59.6850 - accuracy: 0.6500\n",
      "Epoch 21/50: - 0.7596s/step - loss: 58.2325 - accuracy: 0.5500\n",
      "Epoch 22/50: - 0.7589s/step - loss: 59.5809 - accuracy: 0.8000\n",
      "Epoch 23/50: - 0.7584s/step - loss: 57.5108 - accuracy: 0.5500\n",
      "Epoch 24/50: - 0.7578s/step - loss: 56.8363 - accuracy: 0.5500\n",
      "Epoch 25/50: - 0.7574s/step - loss: 55.5521 - accuracy: 0.5500\n",
      "Epoch 26/50: - 0.7568s/step - loss: 54.6265 - accuracy: 0.8250\n",
      "Epoch 27/50: - 0.7563s/step - loss: 53.3790 - accuracy: 0.5500\n",
      "Epoch 28/50: - 0.7561s/step - loss: 53.8783 - accuracy: 0.6875\n",
      "Epoch 29/50: - 0.7558s/step - loss: 53.1344 - accuracy: 0.5500\n",
      "Epoch 30/50: - 0.7555s/step - loss: 51.6834 - accuracy: 0.8625\n",
      "Epoch 31/50: - 0.7560s/step - loss: 53.8643 - accuracy: 0.8125\n",
      "Epoch 32/50: - 0.7555s/step - loss: 50.8936 - accuracy: 0.5500\n",
      "Epoch 33/50: - 0.7553s/step - loss: 51.7195 - accuracy: 0.7250\n",
      "Epoch 34/50: - 0.7550s/step - loss: 50.6899 - accuracy: 0.5750\n",
      "Epoch 35/50: - 0.7546s/step - loss: 52.6782 - accuracy: 0.6750\n",
      "Epoch 36/50: - 0.7544s/step - loss: 52.0634 - accuracy: 0.8375\n",
      "Epoch 37/50: - 0.7541s/step - loss: 50.6799 - accuracy: 0.6500\n",
      "Epoch 38/50: - 0.7548s/step - loss: 49.7322 - accuracy: 0.8000\n",
      "Epoch 39/50: - 0.7546s/step - loss: 50.6793 - accuracy: 0.7750\n",
      "Epoch 40/50: - 0.7547s/step - loss: 51.2250 - accuracy: 0.7750\n",
      "Epoch 41/50: - 0.7545s/step - loss: 51.5831 - accuracy: 0.5500\n",
      "Epoch 42/50: - 0.7544s/step - loss: 47.3830 - accuracy: 0.8750\n",
      "Epoch 43/50: - 0.7543s/step - loss: 48.9931 - accuracy: 0.8125\n",
      "Epoch 44/50: - 0.7541s/step - loss: 49.2334 - accuracy: 0.6375\n",
      "Epoch 45/50: - 0.7539s/step - loss: 47.4905 - accuracy: 0.5500\n",
      "Epoch 46/50: - 0.7539s/step - loss: 48.4049 - accuracy: 0.7500\n",
      "Epoch 47/50: - 0.7537s/step - loss: 47.7391 - accuracy: 0.5750\n",
      "Epoch 48/50: - 0.7535s/step - loss: 47.5968 - accuracy: 0.7875\n",
      "Epoch 49/50: - 0.7536s/step - loss: 47.6492 - accuracy: 0.6625\n",
      "Epoch 50/50: - 0.7534s/step - loss: 49.9058 - accuracy: 0.5500\n",
      "Time of HMC:  37.67085599899292\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 50\n",
    "loss_ls = []\n",
    "acc_ls = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (x, y) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(x, network[bs], y, 0.1)\n",
    "        network = [model.gibbs_new_state(x, net, y) for (x, y), net in zip(train_ds, network)]\n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(x, network[bs], y))\n",
    "    \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    train_acc = accuracy_score(np.concatenate(preds), y_train)\n",
    "    loss_ls.append(loss)\n",
    "    acc_ls.append(train_acc)\n",
    "    \n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))\n",
    "\n",
    "print(\"Time of Gibbs: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b054d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print plot and save data\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(list(range(epochs)), acc_ls, label = 'HMC')\n",
    "ax.legend()\n",
    "fig.savefig('make_moon_gibbs_acc_1000.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b2c5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('make_moon_gibbs_loss_1000.npy', 'wb') as f:\n",
    "    np.save(f, np.array(acc_ls))\n",
    "    np.save(f, np.array(loss_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259c530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
