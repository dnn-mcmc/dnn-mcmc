{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 training images.\n",
      "There are 10000 test images.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "(x_dev, y_dev), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('There are', len(x_dev), 'training images.')\n",
    "print('There are', len(x_test), 'test images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_dev, y_dev)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)     \n",
    "            \n",
    "        final_logits = self.output_layer(x)\n",
    "        \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        log_prob = 0.\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits( # only works for discretize version\n",
    "                labels=cv, logits=layer(pv))\n",
    "            log_prob += tf.reduce_sum(ce, axis = -1)\n",
    "            \n",
    "        log_prob += tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.int32), logits=self.output_layer(h_current[-1]))\n",
    "            \n",
    "        return log_prob\n",
    "    \n",
    "    def target_log_prob2(self, x, h_current, y):\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "        #h_current_new = [tf.split(h_current[i], [self.hidden_layer_sizes[0]]) for i in range(32)]\n",
    "        h_current_new = tf.split(h_current[0], self.hidden_layer_sizes, axis = 1)\n",
    "        h_previous = [x] + h_current_new[:-1]\n",
    "        log_prob = 0.\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current_new, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits( # only works for discretize version\n",
    "                labels=cv, logits=layer(pv))\n",
    "            log_prob += tf.reduce_sum(ce, axis = -1)\n",
    "            \n",
    "        log_prob += tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.int32), logits=self.output_layer(h_current_new[-1]))\n",
    "        \n",
    "        #print(log_prob)\n",
    "        \n",
    "        return log_prob\n",
    "    \n",
    "    def run_chain(self, x, h, y):\n",
    "        \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = tf.concat([h_current[0], h_current[1]], axis=1)\n",
    "        \n",
    "        #print(np.asarray(h_current).shape)\n",
    "        #print(h)\n",
    "        \n",
    "        def tlp(*args):\n",
    "            return self.target_log_prob2(x, args, y)\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = tlp,\n",
    "            num_leapfrog_steps = 10,\n",
    "            step_size = pow(1000, -1/4)),\n",
    "            num_adaptation_steps=int(1000 * 0.8))\n",
    "        \n",
    "        # Run the chain (with burn-in).\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "          num_results=5,\n",
    "          num_burnin_steps=1, # set to 1000\n",
    "          current_state=h_current,\n",
    "          kernel=adaptive_hmc,\n",
    "          trace_fn=None,\n",
    "          return_final_kernel_results=True\n",
    "          )\n",
    "        #print(samples)\n",
    "        #sample_mean = tf.reduce_mean(samples)\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layer_sizes = [100, 50], n_outputs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f90e62ed3c8>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/internal/prefer_static.py\", line 254, in cond\n",
      "    return true_fn()  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\", line 462, in <lambda>\n",
      "    num_steps_traced + 1),  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\", line 453, in trace_one_step\n",
      "    tf.nest.flatten(trace_fn(state), expand_composites=True))]  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\", line 451, in <listcomp>\n",
      "    return [ta.write(num_steps_traced, x) for ta, x in  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f90e630b4e0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/internal/prefer_static.py\", line 254, in cond\n",
      "    return true_fn()  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\", line 462, in <lambda>\n",
      "    num_steps_traced + 1),  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\", line 453, in trace_one_step\n",
      "    tf.nest.flatten(trace_fn(state), expand_composites=True))]  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_probability/python/mcmc/internal/util.py\", line 451, in <listcomp>\n",
      "    return [ta.write(num_steps_traced, x) for ta, x in  File \"/Users/hmy/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "hmc = [model.run_chain(images, net, labels) for (images, labels), net in zip(train_ds, network)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
