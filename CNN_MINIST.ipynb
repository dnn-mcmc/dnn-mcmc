{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12665 training images.\n",
      "There are 2115 test images.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select binary data\n",
    "label_sub = [0,1]\n",
    "x_train_sub = [x for x, y in zip(x_train, y_train) if y in label_sub]\n",
    "y_train_sub = [y for y in y_train if y in label_sub]\n",
    "x_test_sub = [x for x, y in zip(x_test, y_test) if y in label_sub]\n",
    "y_test_sub = [y for y in y_test if y in label_sub]\n",
    "\n",
    "\n",
    "print('There are', len(x_train_sub), 'training images.')\n",
    "print('There are', len(x_test_sub), 'test images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a597f6390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 555 # You may select anything up to 60,000\n",
    "print(y_train[image_index]) # The label is 8\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 115s 2ms/sample - loss: 0.2063 - accuracy: 0.9386\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 96s 2ms/sample - loss: 0.0813 - accuracy: 0.9758\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.0573 - accuracy: 0.9816\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.0443 - accuracy: 0.9857\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.0339 - accuracy: 0.9893\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.0294 - accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.0254 - accuracy: 0.9915\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0208 - accuracy: 0.9929\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 105s 2ms/sample - loss: 0.0206 - accuracy: 0.9930\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 100s 2ms/sample - loss: 0.0189 - accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4d4f41d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the Model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
