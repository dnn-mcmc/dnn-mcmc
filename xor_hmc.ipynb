{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0, 0],\n",
    "           [0, 1],\n",
    "           [1, 0],\n",
    "           [1, 1]])\n",
    "y_train = np.array([[0],\n",
    "           [1],\n",
    "           [1],\n",
    "           [0]])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = [tf.math.sigmoid(i) for i in x]\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerange(x, r = 6.0):\n",
    "    \n",
    "    out_of_range = tf.cast(tf.math.greater(tf.math.abs(x), r), tf.float32)\n",
    "    sign = tf.math.sign(x)\n",
    "    \n",
    "    return x * (1 - out_of_range) + sign * r * out_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        #x = Flatten()(x)\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "\n",
    "    def target_log_prob2(self, x, h, y):\n",
    "\n",
    "        h_current = convert2_zero_one(tf.split(h, self.hidden_layer_sizes, axis = 1))\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        nlog_prob = 0.\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = h_current[0]\n",
    "\n",
    "        # initialize the HMC transition kernel\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = pow(1000, -1/4)),\n",
    "            num_adaptation_steps=int(100*0.8))\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 100\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = adaptive_hmc,\n",
    "            trace_fn = None)\n",
    "\n",
    "        h_new = tf.split(samples[0], self.hidden_layer_sizes, axis = 1)\n",
    "\n",
    "        return(h_new)\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tf.math.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tf.math.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    # use the method in MH\n",
    "    def get_cpt(self, x, h, y, id=[0, 1]):\n",
    "        \n",
    "        h_current = convert2_zero_one(tf.split(h, self.hidden_layer_sizes, axis = 1))\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        in_layers = self.fc_layers\n",
    "        out_layers = self.fc_layers[1:] + [self.output_layer]\n",
    "        \n",
    "        prev_vals = [x] + h_current[:-1]\n",
    "        curr_vals = h_current\n",
    "        next_vals = h_current[1:] + [y]\n",
    "        \n",
    "        for i, (in_layer, out_layer, pv, cv, nv) in enumerate(\n",
    "            zip(in_layers, out_layers, prev_vals, curr_vals, next_vals)):\n",
    "            \n",
    "            prob_parents = tf.math.sigmoid(in_layer(pv))\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layer_sizes = [2], n_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weight\n",
    "w_0 = np.array([[1, -1], [1, -1]], dtype = \"float32\")\n",
    "b_0 = np.array([-0.5, 1], dtype = \"float32\")\n",
    "l_0 = [w_0, b_0]\n",
    "\n",
    "w_1 = np.array([[1], [1]], dtype = \"float32\")\n",
    "b_1 = np.array([-1], dtype = \"float32\")\n",
    "l_1 = [w_1, b_1]\n",
    "\n",
    "model.fc_layers[0].set_weights(l_0)\n",
    "model.output_layer.set_weights(l_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "  array([[1, 1],\n",
       "         [0, 0],\n",
       "         [1, 1],\n",
       "         [1, 0]], dtype=int32)>]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [model.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "burnin = 1000\n",
    "step_sizes = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    network_new = []\n",
    "    kernels_new = []\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network, kernels):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        net_current = tf.concat(net_current, axis = 1)\n",
    "        \n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2].new_step_size.numpy()\n",
    "        step_sizes.append(new_step_size)\n",
    "        \n",
    "        new_state = rerange(samples[0][0])\n",
    "        net_new = tf.split(new_state, [2], axis = 1)   \n",
    "        network_new.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new.append(ker_new)\n",
    "            \n",
    "    network = network_new\n",
    "    kernels = kernels_new\n",
    "    \n",
    "    #print(network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes)), step_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
      "array([[-6.       ,  6.       ],\n",
      "       [ 6.       ,  6.       ],\n",
      "       [ 3.8416862,  6.       ],\n",
      "       [ 3.4457765, -6.       ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: - 0.0551s/step - loss: 0.5838 - accuracy: 0.7500\n",
      "Epoch 2/500: - 0.0750s/step - loss: 0.5820 - accuracy: 0.7500\n",
      "Epoch 3/500: - 0.0631s/step - loss: 0.5820 - accuracy: 0.7500\n",
      "Epoch 4/500: - 0.0576s/step - loss: 0.5819 - accuracy: 0.7500\n",
      "Epoch 5/500: - 0.0535s/step - loss: 0.5819 - accuracy: 0.7500\n",
      "Epoch 6/500: - 0.0517s/step - loss: 0.5818 - accuracy: 0.7500\n",
      "Epoch 7/500: - 0.0494s/step - loss: 0.5818 - accuracy: 0.7500\n",
      "Epoch 8/500: - 0.0476s/step - loss: 0.5817 - accuracy: 0.7500\n",
      "Epoch 9/500: - 0.0463s/step - loss: 0.5817 - accuracy: 0.7500\n",
      "Epoch 10/500: - 0.0452s/step - loss: 0.5816 - accuracy: 0.7500\n",
      "Epoch 11/500: - 0.0452s/step - loss: 0.5816 - accuracy: 0.7500\n",
      "Epoch 12/500: - 0.0454s/step - loss: 0.5815 - accuracy: 0.7500\n",
      "Epoch 13/500: - 0.0446s/step - loss: 0.5815 - accuracy: 0.7500\n",
      "Epoch 14/500: - 0.0439s/step - loss: 0.5815 - accuracy: 0.7500\n",
      "Epoch 15/500: - 0.0434s/step - loss: 0.5814 - accuracy: 0.7500\n",
      "Epoch 16/500: - 0.0439s/step - loss: 0.5814 - accuracy: 0.7500\n",
      "Epoch 17/500: - 0.0448s/step - loss: 0.5813 - accuracy: 0.7500\n",
      "Epoch 18/500: - 0.0450s/step - loss: 0.5813 - accuracy: 0.7500\n",
      "Epoch 19/500: - 0.0448s/step - loss: 0.5819 - accuracy: 0.7500\n",
      "Epoch 20/500: - 0.0443s/step - loss: 0.5812 - accuracy: 0.7500\n",
      "Epoch 21/500: - 0.0440s/step - loss: 0.5811 - accuracy: 0.7500\n",
      "Epoch 22/500: - 0.0438s/step - loss: 0.5811 - accuracy: 0.7500\n",
      "Epoch 23/500: - 0.0436s/step - loss: 0.5810 - accuracy: 0.7500\n",
      "Epoch 24/500: - 0.0432s/step - loss: 1.3093 - accuracy: 0.7500\n",
      "Epoch 25/500: - 0.0430s/step - loss: 0.5813 - accuracy: 0.7500\n",
      "Epoch 26/500: - 0.0427s/step - loss: 0.5813 - accuracy: 0.7500\n",
      "Epoch 27/500: - 0.0426s/step - loss: 0.5812 - accuracy: 0.7500\n",
      "Epoch 28/500: - 0.0427s/step - loss: 0.5811 - accuracy: 0.7500\n",
      "Epoch 29/500: - 0.0424s/step - loss: 1.7898 - accuracy: 0.7500\n",
      "Epoch 30/500: - 0.0421s/step - loss: 1.7679 - accuracy: 0.7500\n",
      "Epoch 31/500: - 0.0421s/step - loss: 1.7552 - accuracy: 0.7500\n",
      "Epoch 32/500: - 0.0418s/step - loss: 1.7426 - accuracy: 0.7500\n",
      "Epoch 33/500: - 0.0418s/step - loss: 1.7300 - accuracy: 0.7500\n",
      "Epoch 34/500: - 0.0421s/step - loss: 1.7175 - accuracy: 0.7500\n",
      "Epoch 35/500: - 0.0420s/step - loss: 1.7050 - accuracy: 0.7500\n",
      "Epoch 36/500: - 0.0418s/step - loss: 1.6926 - accuracy: 0.7500\n",
      "Epoch 37/500: - 0.0417s/step - loss: 1.6802 - accuracy: 0.7500\n",
      "Epoch 38/500: - 0.0419s/step - loss: 1.6679 - accuracy: 0.7500\n",
      "Epoch 39/500: - 0.0426s/step - loss: 1.6556 - accuracy: 0.7500\n",
      "Epoch 40/500: - 0.0433s/step - loss: 0.5852 - accuracy: 0.7500\n",
      "Epoch 41/500: - 0.0437s/step - loss: 0.5849 - accuracy: 0.7500\n",
      "Epoch 42/500: - 0.0442s/step - loss: 0.5847 - accuracy: 0.7500\n",
      "Epoch 43/500: - 0.0444s/step - loss: 0.5845 - accuracy: 0.7500\n",
      "Epoch 44/500: - 0.0442s/step - loss: 0.5843 - accuracy: 0.7500\n",
      "Epoch 45/500: - 0.0440s/step - loss: 0.5841 - accuracy: 0.7500\n",
      "Epoch 46/500: - 0.0438s/step - loss: 0.5840 - accuracy: 0.7500\n",
      "Epoch 47/500: - 0.0436s/step - loss: 0.5838 - accuracy: 0.7500\n",
      "Epoch 48/500: - 0.0436s/step - loss: 0.5845 - accuracy: 0.7500\n",
      "Epoch 49/500: - 0.0436s/step - loss: 0.5844 - accuracy: 0.7500\n",
      "Epoch 50/500: - 0.0434s/step - loss: 0.5834 - accuracy: 0.7500\n",
      "Epoch 51/500: - 0.0432s/step - loss: 0.5833 - accuracy: 0.7500\n",
      "Epoch 52/500: - 0.0430s/step - loss: 0.5832 - accuracy: 0.7500\n",
      "Epoch 53/500: - 0.0428s/step - loss: 0.5831 - accuracy: 0.7500\n",
      "Epoch 54/500: - 0.0427s/step - loss: 0.5830 - accuracy: 0.7500\n",
      "Epoch 55/500: - 0.0428s/step - loss: 0.5843 - accuracy: 0.7500\n",
      "Epoch 56/500: - 0.0426s/step - loss: 0.5828 - accuracy: 0.7500\n",
      "Epoch 57/500: - 0.0425s/step - loss: 0.5827 - accuracy: 0.7500\n",
      "Epoch 58/500: - 0.0424s/step - loss: 0.6155 - accuracy: 0.7500\n",
      "Epoch 59/500: - 0.0423s/step - loss: 0.6154 - accuracy: 0.7500\n",
      "Epoch 60/500: - 0.0423s/step - loss: 0.5824 - accuracy: 0.7500\n",
      "Epoch 61/500: - 0.0426s/step - loss: 0.5824 - accuracy: 0.7500\n",
      "Epoch 62/500: - 0.0429s/step - loss: 0.5823 - accuracy: 0.7500\n",
      "Epoch 63/500: - 0.0431s/step - loss: 0.5822 - accuracy: 0.7500\n",
      "Epoch 64/500: - 0.0435s/step - loss: 0.5821 - accuracy: 0.7500\n",
      "Epoch 65/500: - 0.0435s/step - loss: 0.5821 - accuracy: 0.7500\n",
      "Epoch 66/500: - 0.0434s/step - loss: 0.5820 - accuracy: 0.7500\n",
      "Epoch 67/500: - 0.0432s/step - loss: 0.5819 - accuracy: 0.7500\n",
      "Epoch 68/500: - 0.0431s/step - loss: 0.5819 - accuracy: 0.7500\n",
      "Epoch 69/500: - 0.0430s/step - loss: 0.5818 - accuracy: 0.7500\n",
      "Epoch 70/500: - 0.0430s/step - loss: 0.5818 - accuracy: 0.7500\n",
      "Epoch 71/500: - 0.0431s/step - loss: 0.5817 - accuracy: 0.7500\n",
      "Epoch 72/500: - 0.0430s/step - loss: 0.5816 - accuracy: 0.7500\n",
      "Epoch 73/500: - 0.0429s/step - loss: 0.5816 - accuracy: 0.7500\n",
      "Epoch 74/500: - 0.0428s/step - loss: 0.5815 - accuracy: 0.7500\n",
      "Epoch 75/500: - 0.0427s/step - loss: 0.5815 - accuracy: 0.7500\n",
      "Epoch 76/500: - 0.0427s/step - loss: 0.5814 - accuracy: 0.7500\n",
      "Epoch 77/500: - 0.0430s/step - loss: 0.5814 - accuracy: 0.7500\n",
      "Epoch 78/500: - 0.0432s/step - loss: 0.5813 - accuracy: 0.7500\n",
      "Epoch 79/500: - 0.0435s/step - loss: 0.5812 - accuracy: 0.7500\n",
      "Epoch 80/500: - 0.0436s/step - loss: 0.5812 - accuracy: 0.7500\n",
      "Epoch 81/500: - 0.0437s/step - loss: 0.5811 - accuracy: 0.7500\n",
      "Epoch 82/500: - 0.0436s/step - loss: 0.5825 - accuracy: 0.7500\n",
      "Epoch 83/500: - 0.0436s/step - loss: 0.5824 - accuracy: 0.7500\n",
      "Epoch 84/500: - 0.0435s/step - loss: 0.5824 - accuracy: 0.7500\n",
      "Epoch 85/500: - 0.0434s/step - loss: 0.5823 - accuracy: 0.7500\n",
      "Epoch 86/500: - 0.0434s/step - loss: 0.5823 - accuracy: 0.7500\n",
      "Epoch 87/500: - 0.0435s/step - loss: 0.5822 - accuracy: 0.7500\n",
      "Epoch 88/500: - 0.0434s/step - loss: 0.5808 - accuracy: 0.7500\n",
      "Epoch 89/500: - 0.0433s/step - loss: 0.5807 - accuracy: 0.7500\n",
      "Epoch 90/500: - 0.0433s/step - loss: 0.5807 - accuracy: 0.7500\n",
      "Epoch 91/500: - 0.0433s/step - loss: 0.5806 - accuracy: 0.7500\n",
      "Epoch 92/500: - 0.0436s/step - loss: 0.5805 - accuracy: 0.7500\n",
      "Epoch 93/500: - 0.0438s/step - loss: 0.5805 - accuracy: 0.7500\n",
      "Epoch 94/500: - 0.0438s/step - loss: 0.5804 - accuracy: 0.7500\n",
      "Epoch 95/500: - 0.0437s/step - loss: 0.5804 - accuracy: 0.7500\n",
      "Epoch 96/500: - 0.0436s/step - loss: 0.5803 - accuracy: 0.7500\n",
      "Epoch 97/500: - 0.0436s/step - loss: 0.5810 - accuracy: 0.7500\n",
      "Epoch 98/500: - 0.0436s/step - loss: 0.5802 - accuracy: 0.7500\n",
      "Epoch 99/500: - 0.0435s/step - loss: 0.5802 - accuracy: 0.7500\n",
      "Epoch 100/500: - 0.0434s/step - loss: 0.5810 - accuracy: 0.7500\n",
      "Epoch 101/500: - 0.0433s/step - loss: 0.5810 - accuracy: 0.7500\n",
      "Epoch 102/500: - 0.0433s/step - loss: 0.5809 - accuracy: 0.7500\n",
      "Epoch 103/500: - 0.0433s/step - loss: 0.5800 - accuracy: 0.7500\n",
      "Epoch 104/500: - 0.0434s/step - loss: 0.5799 - accuracy: 0.7500\n",
      "Epoch 105/500: - 0.0433s/step - loss: 0.5799 - accuracy: 0.7500\n",
      "Epoch 106/500: - 0.0432s/step - loss: 0.5798 - accuracy: 0.7500\n",
      "Epoch 107/500: - 0.0431s/step - loss: 0.5798 - accuracy: 0.7500\n",
      "Epoch 108/500: - 0.0430s/step - loss: 0.5797 - accuracy: 0.7500\n",
      "Epoch 109/500: - 0.0430s/step - loss: 0.5797 - accuracy: 0.7500\n",
      "Epoch 110/500: - 0.0433s/step - loss: 0.5796 - accuracy: 0.7500\n",
      "Epoch 111/500: - 0.0433s/step - loss: 0.5796 - accuracy: 0.7500\n",
      "Epoch 112/500: - 0.0432s/step - loss: 0.5795 - accuracy: 0.7500\n",
      "Epoch 113/500: - 0.0431s/step - loss: 0.5795 - accuracy: 0.7500\n",
      "Epoch 114/500: - 0.0431s/step - loss: 0.5794 - accuracy: 0.7500\n",
      "Epoch 115/500: - 0.0431s/step - loss: 0.5794 - accuracy: 0.7500\n",
      "Epoch 116/500: - 0.0430s/step - loss: 0.5793 - accuracy: 0.7500\n",
      "Epoch 117/500: - 0.0429s/step - loss: 0.5793 - accuracy: 0.7500\n",
      "Epoch 118/500: - 0.0428s/step - loss: 0.5792 - accuracy: 0.7500\n",
      "Epoch 119/500: - 0.0428s/step - loss: 0.5792 - accuracy: 0.7500\n",
      "Epoch 120/500: - 0.0427s/step - loss: 0.5791 - accuracy: 0.7500\n",
      "Epoch 121/500: - 0.0428s/step - loss: 0.5791 - accuracy: 0.7500\n",
      "Epoch 122/500: - 0.0427s/step - loss: 0.5790 - accuracy: 0.7500\n",
      "Epoch 123/500: - 0.0426s/step - loss: 0.5790 - accuracy: 0.7500\n",
      "Epoch 124/500: - 0.0426s/step - loss: 0.5789 - accuracy: 0.7500\n",
      "Epoch 125/500: - 0.0425s/step - loss: 0.5789 - accuracy: 0.7500\n",
      "Epoch 126/500: - 0.0425s/step - loss: 0.5788 - accuracy: 0.7500\n",
      "Epoch 127/500: - 0.0425s/step - loss: 0.5788 - accuracy: 0.7500\n",
      "Epoch 128/500: - 0.0424s/step - loss: 0.5787 - accuracy: 0.7500\n",
      "Epoch 129/500: - 0.0424s/step - loss: 0.5787 - accuracy: 0.7500\n",
      "Epoch 130/500: - 0.0424s/step - loss: 0.5786 - accuracy: 0.7500\n",
      "Epoch 131/500: - 0.0423s/step - loss: 0.5786 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500: - 0.0423s/step - loss: 0.5785 - accuracy: 0.7500\n",
      "Epoch 133/500: - 0.0423s/step - loss: 0.5800 - accuracy: 0.7500\n",
      "Epoch 134/500: - 0.0423s/step - loss: 0.5799 - accuracy: 0.7500\n",
      "Epoch 135/500: - 0.0422s/step - loss: 0.5784 - accuracy: 0.7500\n",
      "Epoch 136/500: - 0.0422s/step - loss: 0.5784 - accuracy: 0.7500\n",
      "Epoch 137/500: - 0.0422s/step - loss: 0.5783 - accuracy: 0.7500\n",
      "Epoch 138/500: - 0.0423s/step - loss: 0.5783 - accuracy: 0.7500\n",
      "Epoch 139/500: - 0.0422s/step - loss: 0.5782 - accuracy: 0.7500\n",
      "Epoch 140/500: - 0.0422s/step - loss: 0.5782 - accuracy: 0.7500\n",
      "Epoch 141/500: - 0.0421s/step - loss: 0.5783 - accuracy: 0.7500\n",
      "Epoch 142/500: - 0.0421s/step - loss: 1.6290 - accuracy: 0.7500\n",
      "Epoch 143/500: - 0.0420s/step - loss: 0.5784 - accuracy: 0.7500\n",
      "Epoch 144/500: - 0.0420s/step - loss: 0.5783 - accuracy: 0.7500\n",
      "Epoch 145/500: - 0.0420s/step - loss: 0.5783 - accuracy: 0.7500\n",
      "Epoch 146/500: - 0.0419s/step - loss: 0.5782 - accuracy: 0.7500\n",
      "Epoch 147/500: - 0.0418s/step - loss: 0.5782 - accuracy: 0.7500\n",
      "Epoch 148/500: - 0.0418s/step - loss: 0.5781 - accuracy: 0.7500\n",
      "Epoch 149/500: - 0.0417s/step - loss: 0.5781 - accuracy: 0.7500\n",
      "Epoch 150/500: - 0.0417s/step - loss: 0.5780 - accuracy: 0.7500\n",
      "Epoch 151/500: - 0.0417s/step - loss: 0.5780 - accuracy: 0.7500\n",
      "Epoch 152/500: - 0.0416s/step - loss: 0.5794 - accuracy: 0.7500\n",
      "Epoch 153/500: - 0.0416s/step - loss: 0.5794 - accuracy: 0.7500\n",
      "Epoch 154/500: - 0.0415s/step - loss: 0.5778 - accuracy: 0.7500\n",
      "Epoch 155/500: - 0.0415s/step - loss: 0.5778 - accuracy: 0.7500\n",
      "Epoch 156/500: - 0.0416s/step - loss: 0.5777 - accuracy: 0.7500\n",
      "Epoch 157/500: - 0.0416s/step - loss: 0.5777 - accuracy: 0.7500\n",
      "Epoch 158/500: - 0.0415s/step - loss: 0.5776 - accuracy: 0.7500\n",
      "Epoch 159/500: - 0.0415s/step - loss: 0.5776 - accuracy: 0.7500\n",
      "Epoch 160/500: - 0.0414s/step - loss: 0.5775 - accuracy: 0.7500\n",
      "Epoch 161/500: - 0.0414s/step - loss: 0.5775 - accuracy: 0.7500\n",
      "Epoch 162/500: - 0.0414s/step - loss: 0.5774 - accuracy: 0.7500\n",
      "Epoch 163/500: - 0.0414s/step - loss: 0.5774 - accuracy: 0.7500\n",
      "Epoch 164/500: - 0.0414s/step - loss: 0.5783 - accuracy: 0.7500\n",
      "Epoch 165/500: - 0.0413s/step - loss: 0.5782 - accuracy: 0.7500\n",
      "Epoch 166/500: - 0.0413s/step - loss: 0.5782 - accuracy: 0.7500\n",
      "Epoch 167/500: - 0.0413s/step - loss: 0.5772 - accuracy: 0.7500\n",
      "Epoch 168/500: - 0.0413s/step - loss: 0.5772 - accuracy: 0.7500\n",
      "Epoch 169/500: - 0.0412s/step - loss: 0.5771 - accuracy: 0.7500\n",
      "Epoch 170/500: - 0.0412s/step - loss: 0.5771 - accuracy: 0.7500\n",
      "Epoch 171/500: - 0.0412s/step - loss: 0.5770 - accuracy: 0.7500\n",
      "Epoch 172/500: - 0.0412s/step - loss: 0.5770 - accuracy: 0.7500\n",
      "Epoch 173/500: - 0.0411s/step - loss: 0.5769 - accuracy: 0.7500\n",
      "Epoch 174/500: - 0.0411s/step - loss: 0.5769 - accuracy: 0.7500\n",
      "Epoch 175/500: - 0.0411s/step - loss: 0.5768 - accuracy: 0.7500\n",
      "Epoch 176/500: - 0.0411s/step - loss: 0.5768 - accuracy: 0.7500\n",
      "Epoch 177/500: - 0.0410s/step - loss: 0.5767 - accuracy: 0.7500\n",
      "Epoch 178/500: - 0.0410s/step - loss: 0.5767 - accuracy: 0.7500\n",
      "Epoch 179/500: - 0.0409s/step - loss: 0.5766 - accuracy: 0.7500\n",
      "Epoch 180/500: - 0.0409s/step - loss: 0.5766 - accuracy: 0.7500\n",
      "Epoch 181/500: - 0.0409s/step - loss: 0.5766 - accuracy: 0.7500\n",
      "Epoch 182/500: - 0.0409s/step - loss: 0.5765 - accuracy: 0.7500\n",
      "Epoch 183/500: - 0.0408s/step - loss: 0.5765 - accuracy: 0.7500\n",
      "Epoch 184/500: - 0.0408s/step - loss: 0.5764 - accuracy: 0.7500\n",
      "Epoch 185/500: - 0.0408s/step - loss: 0.5764 - accuracy: 0.7500\n",
      "Epoch 186/500: - 0.0408s/step - loss: 0.5763 - accuracy: 0.7500\n",
      "Epoch 187/500: - 0.0408s/step - loss: 0.5763 - accuracy: 0.7500\n",
      "Epoch 188/500: - 0.0408s/step - loss: 0.5762 - accuracy: 0.7500\n",
      "Epoch 189/500: - 0.0407s/step - loss: 0.5762 - accuracy: 0.7500\n",
      "Epoch 190/500: - 0.0407s/step - loss: 0.5761 - accuracy: 0.7500\n",
      "Epoch 191/500: - 0.0408s/step - loss: 0.5761 - accuracy: 0.7500\n",
      "Epoch 192/500: - 0.0410s/step - loss: 0.5761 - accuracy: 0.7500\n",
      "Epoch 193/500: - 0.0411s/step - loss: 0.5760 - accuracy: 0.7500\n",
      "Epoch 194/500: - 0.0411s/step - loss: 0.5760 - accuracy: 0.7500\n",
      "Epoch 195/500: - 0.0411s/step - loss: 0.5759 - accuracy: 0.7500\n",
      "Epoch 196/500: - 0.0411s/step - loss: 0.5759 - accuracy: 0.7500\n",
      "Epoch 197/500: - 0.0411s/step - loss: 0.5758 - accuracy: 0.7500\n",
      "Epoch 198/500: - 0.0411s/step - loss: 0.5758 - accuracy: 0.7500\n",
      "Epoch 199/500: - 0.0411s/step - loss: 0.5757 - accuracy: 0.7500\n",
      "Epoch 200/500: - 0.0411s/step - loss: 0.5757 - accuracy: 0.7500\n",
      "Epoch 201/500: - 0.0410s/step - loss: 0.5757 - accuracy: 0.7500\n",
      "Epoch 202/500: - 0.0410s/step - loss: 0.5756 - accuracy: 0.7500\n",
      "Epoch 203/500: - 0.0409s/step - loss: 0.5756 - accuracy: 0.7500\n",
      "Epoch 204/500: - 0.0410s/step - loss: 0.5755 - accuracy: 0.7500\n",
      "Epoch 205/500: - 0.0409s/step - loss: 0.5755 - accuracy: 0.7500\n",
      "Epoch 206/500: - 0.0409s/step - loss: 0.5770 - accuracy: 0.7500\n",
      "Epoch 207/500: - 0.0409s/step - loss: 0.5770 - accuracy: 0.7500\n",
      "Epoch 208/500: - 0.0409s/step - loss: 0.5769 - accuracy: 0.7500\n",
      "Epoch 209/500: - 0.0410s/step - loss: 0.5769 - accuracy: 0.7500\n",
      "Epoch 210/500: - 0.0409s/step - loss: 0.5753 - accuracy: 0.7500\n",
      "Epoch 211/500: - 0.0409s/step - loss: 0.5759 - accuracy: 0.7500\n",
      "Epoch 212/500: - 0.0409s/step - loss: 0.5758 - accuracy: 0.7500\n",
      "Epoch 213/500: - 0.0409s/step - loss: 0.5751 - accuracy: 0.7500\n",
      "Epoch 214/500: - 0.0409s/step - loss: 0.5751 - accuracy: 0.7500\n",
      "Epoch 215/500: - 0.0409s/step - loss: 0.5766 - accuracy: 0.7500\n",
      "Epoch 216/500: - 0.0409s/step - loss: 0.5765 - accuracy: 0.7500\n",
      "Epoch 217/500: - 0.0408s/step - loss: 0.5750 - accuracy: 0.7500\n",
      "Epoch 218/500: - 0.0408s/step - loss: 0.5749 - accuracy: 0.7500\n",
      "Epoch 219/500: - 0.0408s/step - loss: 0.5749 - accuracy: 0.7500\n",
      "Epoch 220/500: - 0.0408s/step - loss: 0.5748 - accuracy: 0.7500\n",
      "Epoch 221/500: - 0.0408s/step - loss: 0.5748 - accuracy: 0.7500\n",
      "Epoch 222/500: - 0.0407s/step - loss: 0.5748 - accuracy: 0.7500\n",
      "Epoch 223/500: - 0.0407s/step - loss: 0.5747 - accuracy: 0.7500\n",
      "Epoch 224/500: - 0.0407s/step - loss: 0.5747 - accuracy: 0.7500\n",
      "Epoch 225/500: - 0.0406s/step - loss: 0.5746 - accuracy: 0.7500\n",
      "Epoch 226/500: - 0.0406s/step - loss: 0.5746 - accuracy: 0.7500\n",
      "Epoch 227/500: - 0.0406s/step - loss: 0.5759 - accuracy: 0.7500\n",
      "Epoch 228/500: - 0.0406s/step - loss: 0.5758 - accuracy: 0.7500\n",
      "Epoch 229/500: - 0.0406s/step - loss: 0.5758 - accuracy: 0.7500\n",
      "Epoch 230/500: - 0.0405s/step - loss: 0.5744 - accuracy: 0.7500\n",
      "Epoch 231/500: - 0.0405s/step - loss: 0.5744 - accuracy: 0.7500\n",
      "Epoch 232/500: - 0.0405s/step - loss: 0.5743 - accuracy: 0.7500\n",
      "Epoch 233/500: - 0.0405s/step - loss: 0.5743 - accuracy: 0.7500\n",
      "Epoch 234/500: - 0.0405s/step - loss: 0.5743 - accuracy: 0.7500\n",
      "Epoch 235/500: - 0.0405s/step - loss: 0.5742 - accuracy: 0.7500\n",
      "Epoch 236/500: - 0.0405s/step - loss: 0.5742 - accuracy: 0.7500\n",
      "Epoch 237/500: - 0.0404s/step - loss: 0.5741 - accuracy: 0.7500\n",
      "Epoch 238/500: - 0.0405s/step - loss: 0.5741 - accuracy: 0.7500\n",
      "Epoch 239/500: - 0.0404s/step - loss: 0.5740 - accuracy: 0.7500\n",
      "Epoch 240/500: - 0.0404s/step - loss: 0.5740 - accuracy: 0.7500\n",
      "Epoch 241/500: - 0.0404s/step - loss: 0.5745 - accuracy: 0.7500\n",
      "Epoch 242/500: - 0.0404s/step - loss: 0.5744 - accuracy: 0.7500\n",
      "Epoch 243/500: - 0.0404s/step - loss: 0.5744 - accuracy: 0.7500\n",
      "Epoch 244/500: - 0.0404s/step - loss: 0.5743 - accuracy: 0.7500\n",
      "Epoch 245/500: - 0.0404s/step - loss: 0.5743 - accuracy: 0.7500\n",
      "Epoch 246/500: - 0.0403s/step - loss: 0.5738 - accuracy: 0.7500\n",
      "Epoch 247/500: - 0.0403s/step - loss: 0.5737 - accuracy: 0.7500\n",
      "Epoch 248/500: - 0.0403s/step - loss: 0.5737 - accuracy: 0.7500\n",
      "Epoch 249/500: - 0.0403s/step - loss: 0.5736 - accuracy: 0.7500\n",
      "Epoch 250/500: - 0.0403s/step - loss: 0.5736 - accuracy: 0.7500\n",
      "Epoch 251/500: - 0.0403s/step - loss: 0.5736 - accuracy: 0.7500\n",
      "Epoch 252/500: - 0.0402s/step - loss: 0.5735 - accuracy: 0.7500\n",
      "Epoch 253/500: - 0.0402s/step - loss: 0.5735 - accuracy: 0.7500\n",
      "Epoch 254/500: - 0.0402s/step - loss: 0.5734 - accuracy: 0.7500\n",
      "Epoch 255/500: - 0.0402s/step - loss: 0.5734 - accuracy: 0.7500\n",
      "Epoch 256/500: - 0.0402s/step - loss: 0.5733 - accuracy: 0.7500\n",
      "Epoch 257/500: - 0.0402s/step - loss: 0.5733 - accuracy: 0.7500\n",
      "Epoch 258/500: - 0.0401s/step - loss: 0.5733 - accuracy: 0.7500\n",
      "Epoch 259/500: - 0.0401s/step - loss: 0.5732 - accuracy: 0.7500\n",
      "Epoch 260/500: - 0.0402s/step - loss: 0.5732 - accuracy: 0.7500\n",
      "Epoch 261/500: - 0.0402s/step - loss: 0.5731 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262/500: - 0.0402s/step - loss: 0.5731 - accuracy: 0.7500\n",
      "Epoch 263/500: - 0.0402s/step - loss: 0.5731 - accuracy: 0.7500\n",
      "Epoch 264/500: - 0.0401s/step - loss: 0.5730 - accuracy: 0.7500\n",
      "Epoch 265/500: - 0.0401s/step - loss: 0.5730 - accuracy: 0.7500\n",
      "Epoch 266/500: - 0.0401s/step - loss: 0.5729 - accuracy: 0.7500\n",
      "Epoch 267/500: - 0.0401s/step - loss: 0.5729 - accuracy: 0.7500\n",
      "Epoch 268/500: - 0.0401s/step - loss: 0.5729 - accuracy: 0.7500\n",
      "Epoch 269/500: - 0.0401s/step - loss: 0.5728 - accuracy: 0.7500\n",
      "Epoch 270/500: - 0.0401s/step - loss: 0.5728 - accuracy: 0.7500\n",
      "Epoch 271/500: - 0.0401s/step - loss: 0.5727 - accuracy: 0.7500\n",
      "Epoch 272/500: - 0.0401s/step - loss: 0.5727 - accuracy: 0.7500\n",
      "Epoch 273/500: - 0.0401s/step - loss: 0.5727 - accuracy: 0.7500\n",
      "Epoch 274/500: - 0.0401s/step - loss: 0.5726 - accuracy: 0.7500\n",
      "Epoch 275/500: - 0.0401s/step - loss: 0.5726 - accuracy: 0.7500\n",
      "Epoch 276/500: - 0.0400s/step - loss: 0.5726 - accuracy: 0.7500\n",
      "Epoch 277/500: - 0.0400s/step - loss: 0.5725 - accuracy: 0.7500\n",
      "Epoch 278/500: - 0.0400s/step - loss: 0.5725 - accuracy: 0.7500\n",
      "Epoch 279/500: - 0.0400s/step - loss: 0.5758 - accuracy: 0.7500\n",
      "Epoch 280/500: - 0.0400s/step - loss: 0.5758 - accuracy: 0.7500\n",
      "Epoch 281/500: - 0.0400s/step - loss: 0.5757 - accuracy: 0.7500\n",
      "Epoch 282/500: - 0.0400s/step - loss: 0.5723 - accuracy: 0.7500\n",
      "Epoch 283/500: - 0.0400s/step - loss: 0.5723 - accuracy: 0.7500\n",
      "Epoch 284/500: - 0.0399s/step - loss: 0.5722 - accuracy: 0.7500\n",
      "Epoch 285/500: - 0.0399s/step - loss: 0.5722 - accuracy: 0.7500\n",
      "Epoch 286/500: - 0.0399s/step - loss: 0.5722 - accuracy: 0.7500\n",
      "Epoch 287/500: - 0.0399s/step - loss: 0.5721 - accuracy: 0.7500\n",
      "Epoch 288/500: - 0.0400s/step - loss: 0.5721 - accuracy: 0.7500\n",
      "Epoch 289/500: - 0.0399s/step - loss: 0.5720 - accuracy: 0.7500\n",
      "Epoch 290/500: - 0.0399s/step - loss: 0.5720 - accuracy: 0.7500\n",
      "Epoch 291/500: - 0.0399s/step - loss: 0.5720 - accuracy: 0.7500\n",
      "Epoch 292/500: - 0.0399s/step - loss: 0.5719 - accuracy: 0.7500\n",
      "Epoch 293/500: - 0.0399s/step - loss: 0.5719 - accuracy: 0.7500\n",
      "Epoch 294/500: - 0.0399s/step - loss: 0.5719 - accuracy: 0.7500\n",
      "Epoch 295/500: - 0.0399s/step - loss: 0.5718 - accuracy: 0.7500\n",
      "Epoch 296/500: - 0.0399s/step - loss: 0.5718 - accuracy: 0.7500\n",
      "Epoch 297/500: - 0.0399s/step - loss: 0.5717 - accuracy: 0.7500\n",
      "Epoch 298/500: - 0.0399s/step - loss: 0.5717 - accuracy: 0.7500\n",
      "Epoch 299/500: - 0.0399s/step - loss: 0.5717 - accuracy: 0.7500\n",
      "Epoch 300/500: - 0.0399s/step - loss: 0.6181 - accuracy: 0.7500\n",
      "Epoch 301/500: - 0.0399s/step - loss: 0.6180 - accuracy: 0.7500\n",
      "Epoch 302/500: - 0.0399s/step - loss: 0.6180 - accuracy: 0.7500\n",
      "Epoch 303/500: - 0.0398s/step - loss: 0.6179 - accuracy: 0.7500\n",
      "Epoch 304/500: - 0.0398s/step - loss: 0.5715 - accuracy: 0.7500\n",
      "Epoch 305/500: - 0.0398s/step - loss: 0.5715 - accuracy: 0.7500\n",
      "Epoch 306/500: - 0.0398s/step - loss: 0.5714 - accuracy: 0.7500\n",
      "Epoch 307/500: - 0.0398s/step - loss: 0.5978 - accuracy: 0.7500\n",
      "Epoch 308/500: - 0.0398s/step - loss: 0.5978 - accuracy: 0.7500\n",
      "Epoch 309/500: - 0.0398s/step - loss: 0.5977 - accuracy: 0.7500\n",
      "Epoch 310/500: - 0.0398s/step - loss: 0.5713 - accuracy: 0.7500\n",
      "Epoch 311/500: - 0.0398s/step - loss: 0.5713 - accuracy: 0.7500\n",
      "Epoch 312/500: - 0.0397s/step - loss: 0.5712 - accuracy: 0.7500\n",
      "Epoch 313/500: - 0.0397s/step - loss: 0.5712 - accuracy: 0.7500\n",
      "Epoch 314/500: - 0.0397s/step - loss: 0.5711 - accuracy: 0.7500\n",
      "Epoch 315/500: - 0.0397s/step - loss: 0.5711 - accuracy: 0.7500\n",
      "Epoch 316/500: - 0.0397s/step - loss: 0.5711 - accuracy: 0.7500\n",
      "Epoch 317/500: - 0.0397s/step - loss: 0.5710 - accuracy: 0.7500\n",
      "Epoch 318/500: - 0.0397s/step - loss: 0.5710 - accuracy: 0.7500\n",
      "Epoch 319/500: - 0.0397s/step - loss: 0.5710 - accuracy: 0.7500\n",
      "Epoch 320/500: - 0.0397s/step - loss: 0.5709 - accuracy: 0.7500\n",
      "Epoch 321/500: - 0.0397s/step - loss: 0.5709 - accuracy: 0.7500\n",
      "Epoch 322/500: - 0.0397s/step - loss: 0.5708 - accuracy: 0.7500\n",
      "Epoch 323/500: - 0.0397s/step - loss: 0.5708 - accuracy: 0.7500\n",
      "Epoch 324/500: - 0.0397s/step - loss: 0.5708 - accuracy: 0.7500\n",
      "Epoch 325/500: - 0.0397s/step - loss: 0.5707 - accuracy: 0.7500\n",
      "Epoch 326/500: - 0.0397s/step - loss: 0.5707 - accuracy: 0.7500\n",
      "Epoch 327/500: - 0.0397s/step - loss: 0.5707 - accuracy: 0.7500\n",
      "Epoch 328/500: - 0.0397s/step - loss: 0.5706 - accuracy: 0.7500\n",
      "Epoch 329/500: - 0.0397s/step - loss: 0.5706 - accuracy: 0.7500\n",
      "Epoch 330/500: - 0.0396s/step - loss: 0.5706 - accuracy: 0.7500\n",
      "Epoch 331/500: - 0.0396s/step - loss: 0.5705 - accuracy: 0.7500\n",
      "Epoch 332/500: - 0.0396s/step - loss: 0.5705 - accuracy: 0.7500\n",
      "Epoch 333/500: - 0.0396s/step - loss: 0.5704 - accuracy: 0.7500\n",
      "Epoch 334/500: - 0.0396s/step - loss: 0.5704 - accuracy: 0.7500\n",
      "Epoch 335/500: - 0.0396s/step - loss: 0.5704 - accuracy: 0.7500\n",
      "Epoch 336/500: - 0.0396s/step - loss: 0.5703 - accuracy: 0.7500\n",
      "Epoch 337/500: - 0.0396s/step - loss: 0.5703 - accuracy: 0.7500\n",
      "Epoch 338/500: - 0.0395s/step - loss: 0.5703 - accuracy: 0.7500\n",
      "Epoch 339/500: - 0.0395s/step - loss: 0.5702 - accuracy: 0.7500\n",
      "Epoch 340/500: - 0.0395s/step - loss: 0.5702 - accuracy: 0.7500\n",
      "Epoch 341/500: - 0.0395s/step - loss: 0.5702 - accuracy: 0.7500\n",
      "Epoch 342/500: - 0.0395s/step - loss: 0.5701 - accuracy: 0.7500\n",
      "Epoch 343/500: - 0.0395s/step - loss: 0.5701 - accuracy: 0.7500\n",
      "Epoch 344/500: - 0.0395s/step - loss: 0.5700 - accuracy: 0.7500\n",
      "Epoch 345/500: - 0.0395s/step - loss: 0.5700 - accuracy: 0.7500\n",
      "Epoch 346/500: - 0.0395s/step - loss: 0.5700 - accuracy: 0.7500\n",
      "Epoch 347/500: - 0.0395s/step - loss: 0.5699 - accuracy: 0.7500\n",
      "Epoch 348/500: - 0.0395s/step - loss: 0.5699 - accuracy: 0.7500\n",
      "Epoch 349/500: - 0.0395s/step - loss: 0.5699 - accuracy: 0.7500\n",
      "Epoch 350/500: - 0.0395s/step - loss: 0.5698 - accuracy: 0.7500\n",
      "Epoch 351/500: - 0.0395s/step - loss: 0.5698 - accuracy: 0.7500\n",
      "Epoch 352/500: - 0.0395s/step - loss: 0.5698 - accuracy: 0.7500\n",
      "Epoch 353/500: - 0.0395s/step - loss: 0.5697 - accuracy: 0.7500\n",
      "Epoch 354/500: - 0.0394s/step - loss: 0.5697 - accuracy: 0.7500\n",
      "Epoch 355/500: - 0.0394s/step - loss: 0.5697 - accuracy: 0.7500\n",
      "Epoch 356/500: - 0.0394s/step - loss: 0.5696 - accuracy: 0.7500\n",
      "Epoch 357/500: - 0.0394s/step - loss: 0.5696 - accuracy: 0.7500\n",
      "Epoch 358/500: - 0.0394s/step - loss: 0.5696 - accuracy: 0.7500\n",
      "Epoch 359/500: - 0.0394s/step - loss: 0.5695 - accuracy: 0.7500\n",
      "Epoch 360/500: - 0.0394s/step - loss: 0.5695 - accuracy: 0.7500\n",
      "Epoch 361/500: - 0.0394s/step - loss: 0.5694 - accuracy: 0.7500\n",
      "Epoch 362/500: - 0.0394s/step - loss: 0.5694 - accuracy: 0.7500\n",
      "Epoch 363/500: - 0.0394s/step - loss: 0.5694 - accuracy: 0.7500\n",
      "Epoch 364/500: - 0.0394s/step - loss: 0.5693 - accuracy: 0.7500\n",
      "Epoch 365/500: - 0.0394s/step - loss: 0.5693 - accuracy: 0.7500\n",
      "Epoch 366/500: - 0.0393s/step - loss: 0.5693 - accuracy: 0.7500\n",
      "Epoch 367/500: - 0.0393s/step - loss: 0.5692 - accuracy: 0.7500\n",
      "Epoch 368/500: - 0.0393s/step - loss: 0.5705 - accuracy: 0.7500\n",
      "Epoch 369/500: - 0.0393s/step - loss: 0.6278 - accuracy: 0.7500\n",
      "Epoch 370/500: - 0.0394s/step - loss: 0.6278 - accuracy: 0.7500\n",
      "Epoch 371/500: - 0.0393s/step - loss: 0.5691 - accuracy: 0.7500\n",
      "Epoch 372/500: - 0.0393s/step - loss: 0.5691 - accuracy: 0.7500\n",
      "Epoch 373/500: - 0.0393s/step - loss: 0.5706 - accuracy: 0.7500\n",
      "Epoch 374/500: - 0.0393s/step - loss: 0.5690 - accuracy: 0.7500\n",
      "Epoch 375/500: - 0.0393s/step - loss: 0.5690 - accuracy: 0.7500\n",
      "Epoch 376/500: - 0.0393s/step - loss: 0.5689 - accuracy: 0.7500\n",
      "Epoch 377/500: - 0.0393s/step - loss: 0.5689 - accuracy: 0.7500\n",
      "Epoch 378/500: - 0.0393s/step - loss: 0.5689 - accuracy: 0.7500\n",
      "Epoch 379/500: - 0.0393s/step - loss: 0.5688 - accuracy: 0.7500\n",
      "Epoch 380/500: - 0.0393s/step - loss: 0.5688 - accuracy: 0.7500\n",
      "Epoch 381/500: - 0.0393s/step - loss: 0.5688 - accuracy: 0.7500\n",
      "Epoch 382/500: - 0.0393s/step - loss: 0.5687 - accuracy: 0.7500\n",
      "Epoch 383/500: - 0.0393s/step - loss: 0.5687 - accuracy: 0.7500\n",
      "Epoch 384/500: - 0.0393s/step - loss: 0.5687 - accuracy: 0.7500\n",
      "Epoch 385/500: - 0.0393s/step - loss: 0.5686 - accuracy: 0.7500\n",
      "Epoch 386/500: - 0.0392s/step - loss: 0.5686 - accuracy: 0.7500\n",
      "Epoch 387/500: - 0.0392s/step - loss: 0.5686 - accuracy: 0.7500\n",
      "Epoch 388/500: - 0.0393s/step - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 389/500: - 0.0393s/step - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 390/500: - 0.0393s/step - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 391/500: - 0.0392s/step - loss: 0.5684 - accuracy: 0.7500\n",
      "Epoch 392/500: - 0.0392s/step - loss: 1.6733 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/500: - 0.0392s/step - loss: 1.6605 - accuracy: 0.7500\n",
      "Epoch 394/500: - 0.0392s/step - loss: 0.5716 - accuracy: 0.7500\n",
      "Epoch 395/500: - 0.0392s/step - loss: 0.5698 - accuracy: 0.7500\n",
      "Epoch 396/500: - 0.0392s/step - loss: 0.5698 - accuracy: 0.7500\n",
      "Epoch 397/500: - 0.0392s/step - loss: 0.5697 - accuracy: 0.7500\n",
      "Epoch 398/500: - 0.0392s/step - loss: 0.5697 - accuracy: 0.7500\n",
      "Epoch 399/500: - 0.0392s/step - loss: 0.5687 - accuracy: 0.7500\n",
      "Epoch 400/500: - 0.0392s/step - loss: 0.5686 - accuracy: 0.7500\n",
      "Epoch 401/500: - 0.0392s/step - loss: 0.5686 - accuracy: 0.7500\n",
      "Epoch 402/500: - 0.0392s/step - loss: 0.5686 - accuracy: 0.7500\n",
      "Epoch 403/500: - 0.0392s/step - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 404/500: - 0.0392s/step - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 405/500: - 0.0392s/step - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 406/500: - 0.0392s/step - loss: 0.5684 - accuracy: 0.7500\n",
      "Epoch 407/500: - 0.0392s/step - loss: 0.5684 - accuracy: 0.7500\n",
      "Epoch 408/500: - 0.0392s/step - loss: 0.5683 - accuracy: 0.7500\n",
      "Epoch 409/500: - 0.0392s/step - loss: 0.5683 - accuracy: 0.7500\n",
      "Epoch 410/500: - 0.0392s/step - loss: 0.5683 - accuracy: 0.7500\n",
      "Epoch 411/500: - 0.0392s/step - loss: 0.5682 - accuracy: 0.7500\n",
      "Epoch 412/500: - 0.0392s/step - loss: 0.5682 - accuracy: 0.7500\n",
      "Epoch 413/500: - 0.0392s/step - loss: 0.5682 - accuracy: 0.7500\n",
      "Epoch 414/500: - 0.0392s/step - loss: 0.5681 - accuracy: 0.7500\n",
      "Epoch 415/500: - 0.0391s/step - loss: 0.5681 - accuracy: 0.7500\n",
      "Epoch 416/500: - 0.0391s/step - loss: 0.5681 - accuracy: 0.7500\n",
      "Epoch 417/500: - 0.0391s/step - loss: 0.5680 - accuracy: 0.7500\n",
      "Epoch 418/500: - 0.0391s/step - loss: 0.5680 - accuracy: 0.7500\n",
      "Epoch 419/500: - 0.0391s/step - loss: 0.5680 - accuracy: 0.7500\n",
      "Epoch 420/500: - 0.0391s/step - loss: 0.5679 - accuracy: 0.7500\n",
      "Epoch 421/500: - 0.0391s/step - loss: 0.5679 - accuracy: 0.7500\n",
      "Epoch 422/500: - 0.0391s/step - loss: 0.5683 - accuracy: 0.7500\n",
      "Epoch 423/500: - 0.0391s/step - loss: 0.5678 - accuracy: 0.7500\n",
      "Epoch 424/500: - 0.0391s/step - loss: 0.5678 - accuracy: 0.7500\n",
      "Epoch 425/500: - 0.0391s/step - loss: 0.5678 - accuracy: 0.7500\n",
      "Epoch 426/500: - 0.0391s/step - loss: 0.5677 - accuracy: 0.7500\n",
      "Epoch 427/500: - 0.0391s/step - loss: 0.5677 - accuracy: 0.7500\n",
      "Epoch 428/500: - 0.0391s/step - loss: 0.5677 - accuracy: 0.7500\n",
      "Epoch 429/500: - 0.0392s/step - loss: 0.5676 - accuracy: 0.7500\n",
      "Epoch 430/500: - 0.0391s/step - loss: 0.5676 - accuracy: 0.7500\n",
      "Epoch 431/500: - 0.0391s/step - loss: 0.5676 - accuracy: 0.7500\n",
      "Epoch 432/500: - 0.0391s/step - loss: 0.5675 - accuracy: 0.7500\n",
      "Epoch 433/500: - 0.0391s/step - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 434/500: - 0.0391s/step - loss: 0.5684 - accuracy: 0.7500\n",
      "Epoch 435/500: - 0.0391s/step - loss: 0.5684 - accuracy: 0.7500\n",
      "Epoch 436/500: - 0.0391s/step - loss: 0.5684 - accuracy: 0.7500\n",
      "Epoch 437/500: - 0.0391s/step - loss: 0.5683 - accuracy: 0.7500\n",
      "Epoch 438/500: - 0.0391s/step - loss: 0.5683 - accuracy: 0.7500\n",
      "Epoch 439/500: - 0.0391s/step - loss: 0.5683 - accuracy: 0.7500\n",
      "Epoch 440/500: - 0.0391s/step - loss: 0.5682 - accuracy: 0.7500\n",
      "Epoch 441/500: - 0.0391s/step - loss: 0.5682 - accuracy: 0.7500\n",
      "Epoch 442/500: - 0.0391s/step - loss: 0.5672 - accuracy: 0.7500\n",
      "Epoch 443/500: - 0.0391s/step - loss: 0.5672 - accuracy: 0.7500\n",
      "Epoch 444/500: - 0.0390s/step - loss: 0.5671 - accuracy: 0.7500\n",
      "Epoch 445/500: - 0.0390s/step - loss: 0.5671 - accuracy: 0.7500\n",
      "Epoch 446/500: - 0.0390s/step - loss: 0.5671 - accuracy: 0.7500\n",
      "Epoch 447/500: - 0.0390s/step - loss: 0.5670 - accuracy: 0.7500\n",
      "Epoch 448/500: - 0.0390s/step - loss: 0.5670 - accuracy: 0.7500\n",
      "Epoch 449/500: - 0.0390s/step - loss: 0.5670 - accuracy: 0.7500\n",
      "Epoch 450/500: - 0.0390s/step - loss: 0.5669 - accuracy: 0.7500\n",
      "Epoch 451/500: - 0.0390s/step - loss: 0.5669 - accuracy: 0.7500\n",
      "Epoch 452/500: - 0.0390s/step - loss: 0.5669 - accuracy: 0.7500\n",
      "Epoch 453/500: - 0.0390s/step - loss: 0.5668 - accuracy: 0.7500\n",
      "Epoch 454/500: - 0.0390s/step - loss: 0.5668 - accuracy: 0.7500\n",
      "Epoch 455/500: - 0.0390s/step - loss: 0.5668 - accuracy: 0.7500\n",
      "Epoch 456/500: - 0.0390s/step - loss: 0.5667 - accuracy: 0.7500\n",
      "Epoch 457/500: - 0.0390s/step - loss: 0.5667 - accuracy: 0.7500\n",
      "Epoch 458/500: - 0.0390s/step - loss: 0.5674 - accuracy: 0.7500\n",
      "Epoch 459/500: - 0.0390s/step - loss: 0.5674 - accuracy: 0.7500\n",
      "Epoch 460/500: - 0.0390s/step - loss: 0.5666 - accuracy: 0.7500\n",
      "Epoch 461/500: - 0.0390s/step - loss: 0.5666 - accuracy: 0.7500\n",
      "Epoch 462/500: - 0.0390s/step - loss: 0.5666 - accuracy: 0.7500\n",
      "Epoch 463/500: - 0.0390s/step - loss: 0.5665 - accuracy: 0.7500\n",
      "Epoch 464/500: - 0.0390s/step - loss: 0.5665 - accuracy: 0.7500\n",
      "Epoch 465/500: - 0.0390s/step - loss: 0.5665 - accuracy: 0.7500\n",
      "Epoch 466/500: - 0.0390s/step - loss: 0.5664 - accuracy: 0.7500\n",
      "Epoch 467/500: - 0.0390s/step - loss: 0.5664 - accuracy: 0.7500\n",
      "Epoch 468/500: - 0.0390s/step - loss: 0.5664 - accuracy: 0.7500\n",
      "Epoch 469/500: - 0.0390s/step - loss: 0.5663 - accuracy: 0.7500\n",
      "Epoch 470/500: - 0.0390s/step - loss: 0.5663 - accuracy: 0.7500\n",
      "Epoch 471/500: - 0.0390s/step - loss: 0.5663 - accuracy: 0.7500\n",
      "Epoch 472/500: - 0.0390s/step - loss: 0.5662 - accuracy: 0.7500\n",
      "Epoch 473/500: - 0.0389s/step - loss: 0.5662 - accuracy: 0.7500\n",
      "Epoch 474/500: - 0.0389s/step - loss: 0.5662 - accuracy: 0.7500\n",
      "Epoch 475/500: - 0.0389s/step - loss: 0.5674 - accuracy: 0.7500\n",
      "Epoch 476/500: - 0.0389s/step - loss: 0.5674 - accuracy: 0.7500\n",
      "Epoch 477/500: - 0.0389s/step - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 478/500: - 0.0389s/step - loss: 0.5660 - accuracy: 0.7500\n",
      "Epoch 479/500: - 0.0389s/step - loss: 0.5660 - accuracy: 0.7500\n",
      "Epoch 480/500: - 0.0389s/step - loss: 0.5660 - accuracy: 0.7500\n",
      "Epoch 481/500: - 0.0389s/step - loss: 0.5660 - accuracy: 0.7500\n",
      "Epoch 482/500: - 0.0389s/step - loss: 0.5659 - accuracy: 0.7500\n",
      "Epoch 483/500: - 0.0389s/step - loss: 0.5659 - accuracy: 0.7500\n",
      "Epoch 484/500: - 0.0389s/step - loss: 0.5659 - accuracy: 0.7500\n",
      "Epoch 485/500: - 0.0389s/step - loss: 0.5658 - accuracy: 0.7500\n",
      "Epoch 486/500: - 0.0389s/step - loss: 0.5658 - accuracy: 0.7500\n",
      "Epoch 487/500: - 0.0389s/step - loss: 0.5658 - accuracy: 0.7500\n",
      "Epoch 488/500: - 0.0389s/step - loss: 0.5657 - accuracy: 0.7500\n",
      "Epoch 489/500: - 0.0389s/step - loss: 0.5657 - accuracy: 0.7500\n",
      "Epoch 490/500: - 0.0389s/step - loss: 0.5657 - accuracy: 0.7500\n",
      "Epoch 491/500: - 0.0389s/step - loss: 0.5656 - accuracy: 0.7500\n",
      "Epoch 492/500: - 0.0389s/step - loss: 0.5656 - accuracy: 0.7500\n",
      "Epoch 493/500: - 0.0389s/step - loss: 0.5656 - accuracy: 0.7500\n",
      "Epoch 494/500: - 0.0389s/step - loss: 0.5656 - accuracy: 0.7500\n",
      "Epoch 495/500: - 0.0389s/step - loss: 0.5655 - accuracy: 0.7500\n",
      "Epoch 496/500: - 0.0389s/step - loss: 0.5655 - accuracy: 0.7500\n",
      "Epoch 497/500: - 0.0389s/step - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 498/500: - 0.0388s/step - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 499/500: - 0.0388s/step - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 500/500: - 0.0388s/step - loss: 0.5660 - accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(images, network[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #               zip(train_ds, network)]\n",
    "        \n",
    "        network_new = []\n",
    "        #kernels_new = []\n",
    "        for net, hmc_kernel in zip(network, kernels):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            net_current = tf.concat(net_current, axis = 1)\n",
    "        \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.split(new_state, [2], axis = 1)   \n",
    "            network_new.append(net_new)\n",
    "\n",
    "        network = network_new\n",
    "        \n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(images, network[bs], labels))\n",
    "    \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    #print(preds)\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = StochasticMLP(hidden_layer_sizes = [30], n_outputs=1)\n",
    "network2 = [model2.call(images) for images, labels in train_ds]\n",
    "kernels2 = [model2.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor: shape=(4, 30), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1],\n",
      "       [0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0]], dtype=int32)>]]\n"
     ]
    }
   ],
   "source": [
    "print(network2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1000\n",
    "step_sizes2 = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    #print(i)\n",
    "    network_new2 = []\n",
    "    kernels_new2 = []\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network2, kernels2):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        net_current = net_current[0]\n",
    "        \n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2][4].numpy()\n",
    "        step_sizes2.append(new_step_size)\n",
    "        \n",
    "        new_state = rerange(samples[0][0])\n",
    "        #print(new_state)\n",
    "        net_new = tf.split(new_state, [30], axis = 1)   \n",
    "        network_new2.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model2.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new2.append(ker_new)\n",
    "            \n",
    "    network2 = network_new2\n",
    "    kernels2 = kernels_new2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes2)), step_sizes2)\n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 30), dtype=float32, numpy=\n",
      "array([[-6.        ,  6.        ,  6.        ,  3.452026  , -6.        ,\n",
      "         4.947679  ,  2.320207  , -4.254197  , -6.        , -5.6089454 ,\n",
      "         6.        ,  6.        ,  6.        ,  6.        ,  0.93498325,\n",
      "         6.        ,  6.        , -2.1147652 , -6.        , -3.9191213 ,\n",
      "         1.3871039 ,  0.60071373, -6.        ,  3.8207645 , -6.        ,\n",
      "        -6.        , -6.        , -6.        ,  6.        , -0.70796275],\n",
      "       [ 2.160128  , -6.        , -0.59218645, -6.        ,  6.        ,\n",
      "        -6.        ,  6.        , -2.4475965 ,  6.        , -2.8963027 ,\n",
      "        -4.4213486 ,  6.        ,  6.        ,  3.971283  , -6.        ,\n",
      "         6.        ,  1.1097124 , -1.1559033 ,  6.        ,  4.27173   ,\n",
      "         6.        , -6.        , -6.        ,  5.486821  ,  6.        ,\n",
      "         6.        , -3.1119752 ,  6.        , -1.7854977 , -6.        ],\n",
      "       [ 5.4353967 ,  6.        ,  6.        ,  5.6201305 ,  6.        ,\n",
      "        -6.        , -6.        ,  6.        ,  6.        ,  6.        ,\n",
      "         6.        , -6.        , -6.        ,  6.        , -6.        ,\n",
      "         6.        , -4.611589  ,  6.        ,  6.        ,  2.5367033 ,\n",
      "        -6.        , -6.        ,  6.        , -3.399971  ,  6.        ,\n",
      "         6.        , -5.5528245 ,  1.8684795 ,  6.        , -6.        ],\n",
      "       [ 6.        ,  6.        , -6.        ,  6.        ,  2.657956  ,\n",
      "         6.        , -6.        ,  6.        ,  6.        ,  5.812176  ,\n",
      "         6.        ,  6.        , -4.4131246 ,  6.        , -6.        ,\n",
      "        -6.        , -6.        ,  1.1146615 ,  6.        ,  2.2262647 ,\n",
      "         6.        ,  6.        ,  6.        ,  6.        ,  6.        ,\n",
      "         6.        , -2.3567955 ,  6.        , -0.73925894, -2.401968  ]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(network2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: - 0.0663s/step - loss: 20.5657 - accuracy: 0.5000\n",
      "Epoch 2/500: - 0.0597s/step - loss: 20.8966 - accuracy: 0.5000\n",
      "Epoch 3/500: - 0.0543s/step - loss: 21.7737 - accuracy: 0.5000\n",
      "Epoch 4/500: - 0.0513s/step - loss: 20.8552 - accuracy: 0.5000\n",
      "Epoch 5/500: - 0.0498s/step - loss: 20.9025 - accuracy: 0.5000\n",
      "Epoch 6/500: - 0.0505s/step - loss: 21.3311 - accuracy: 0.5000\n",
      "Epoch 7/500: - 0.0514s/step - loss: 21.0209 - accuracy: 0.5000\n",
      "Epoch 8/500: - 0.0501s/step - loss: 20.7111 - accuracy: 0.5000\n",
      "Epoch 9/500: - 0.0491s/step - loss: 20.7523 - accuracy: 0.5000\n",
      "Epoch 10/500: - 0.0494s/step - loss: 20.4784 - accuracy: 0.5000\n",
      "Epoch 11/500: - 0.0526s/step - loss: 20.1923 - accuracy: 0.5000\n",
      "Epoch 12/500: - 0.0540s/step - loss: 20.5886 - accuracy: 0.5000\n",
      "Epoch 13/500: - 0.0538s/step - loss: 20.3552 - accuracy: 0.5000\n",
      "Epoch 14/500: - 0.0532s/step - loss: 20.3039 - accuracy: 0.5000\n",
      "Epoch 15/500: - 0.0525s/step - loss: 19.9831 - accuracy: 0.5000\n",
      "Epoch 16/500: - 0.0529s/step - loss: 19.5503 - accuracy: 0.5000\n",
      "Epoch 17/500: - 0.0525s/step - loss: 19.5185 - accuracy: 0.7500\n",
      "Epoch 18/500: - 0.0525s/step - loss: 19.4590 - accuracy: 0.7500\n",
      "Epoch 19/500: - 0.0525s/step - loss: 18.9627 - accuracy: 0.5000\n",
      "Epoch 20/500: - 0.0534s/step - loss: 18.5687 - accuracy: 0.5000\n",
      "Epoch 21/500: - 0.0542s/step - loss: 18.2923 - accuracy: 0.5000\n",
      "Epoch 22/500: - 0.0555s/step - loss: 18.5652 - accuracy: 0.5000\n",
      "Epoch 23/500: - 0.0569s/step - loss: 18.2035 - accuracy: 0.5000\n",
      "Epoch 24/500: - 0.0567s/step - loss: 18.2198 - accuracy: 0.5000\n",
      "Epoch 25/500: - 0.0559s/step - loss: 17.9749 - accuracy: 0.5000\n",
      "Epoch 26/500: - 0.0556s/step - loss: 18.0142 - accuracy: 0.5000\n",
      "Epoch 27/500: - 0.0552s/step - loss: 18.1833 - accuracy: 0.5000\n",
      "Epoch 28/500: - 0.0547s/step - loss: 17.6668 - accuracy: 0.7500\n",
      "Epoch 29/500: - 0.0542s/step - loss: 17.4302 - accuracy: 0.5000\n",
      "Epoch 30/500: - 0.0537s/step - loss: 17.4037 - accuracy: 0.5000\n",
      "Epoch 31/500: - 0.0533s/step - loss: 17.2053 - accuracy: 0.5000\n",
      "Epoch 32/500: - 0.0532s/step - loss: 17.0178 - accuracy: 0.5000\n",
      "Epoch 33/500: - 0.0530s/step - loss: 16.8405 - accuracy: 0.5000\n",
      "Epoch 34/500: - 0.0532s/step - loss: 16.6724 - accuracy: 0.7500\n",
      "Epoch 35/500: - 0.0530s/step - loss: 16.5129 - accuracy: 0.7500\n",
      "Epoch 36/500: - 0.0536s/step - loss: 16.3381 - accuracy: 0.5000\n",
      "Epoch 37/500: - 0.0535s/step - loss: 16.1754 - accuracy: 0.5000\n",
      "Epoch 38/500: - 0.0532s/step - loss: 16.3603 - accuracy: 0.2500\n",
      "Epoch 39/500: - 0.0528s/step - loss: 16.3513 - accuracy: 0.5000\n",
      "Epoch 40/500: - 0.0525s/step - loss: 16.3096 - accuracy: 0.5000\n",
      "Epoch 41/500: - 0.0521s/step - loss: 16.1682 - accuracy: 0.5000\n",
      "Epoch 42/500: - 0.0522s/step - loss: 16.0334 - accuracy: 0.5000\n",
      "Epoch 43/500: - 0.0519s/step - loss: 15.9045 - accuracy: 0.5000\n",
      "Epoch 44/500: - 0.0516s/step - loss: 15.8903 - accuracy: 0.5000\n",
      "Epoch 45/500: - 0.0514s/step - loss: 15.7788 - accuracy: 0.5000\n",
      "Epoch 46/500: - 0.0511s/step - loss: 15.9522 - accuracy: 0.5000\n",
      "Epoch 47/500: - 0.0511s/step - loss: 15.5368 - accuracy: 0.5000\n",
      "Epoch 48/500: - 0.0509s/step - loss: 15.3825 - accuracy: 0.5000\n",
      "Epoch 49/500: - 0.0506s/step - loss: 15.7041 - accuracy: 0.5000\n",
      "Epoch 50/500: - 0.0504s/step - loss: 15.5495 - accuracy: 0.5000\n",
      "Epoch 51/500: - 0.0503s/step - loss: 15.1847 - accuracy: 0.5000\n",
      "Epoch 52/500: - 0.0504s/step - loss: 14.9127 - accuracy: 0.5000\n",
      "Epoch 53/500: - 0.0502s/step - loss: 14.7507 - accuracy: 0.5000\n",
      "Epoch 54/500: - 0.0499s/step - loss: 14.5969 - accuracy: 0.5000\n",
      "Epoch 55/500: - 0.0498s/step - loss: 14.4503 - accuracy: 0.5000\n",
      "Epoch 56/500: - 0.0497s/step - loss: 14.3100 - accuracy: 0.5000\n",
      "Epoch 57/500: - 0.0497s/step - loss: 14.3026 - accuracy: 0.5000\n",
      "Epoch 58/500: - 0.0495s/step - loss: 14.1789 - accuracy: 0.5000\n",
      "Epoch 59/500: - 0.0493s/step - loss: 14.0603 - accuracy: 0.5000\n",
      "Epoch 60/500: - 0.0491s/step - loss: 13.9463 - accuracy: 0.5000\n",
      "Epoch 61/500: - 0.0490s/step - loss: 13.8366 - accuracy: 0.5000\n",
      "Epoch 62/500: - 0.0490s/step - loss: 14.0662 - accuracy: 0.5000\n",
      "Epoch 63/500: - 0.0489s/step - loss: 13.9304 - accuracy: 0.5000\n",
      "Epoch 64/500: - 0.0487s/step - loss: 14.5162 - accuracy: 0.5000\n",
      "Epoch 65/500: - 0.0486s/step - loss: 14.4058 - accuracy: 0.5000\n",
      "Epoch 66/500: - 0.0484s/step - loss: 14.2995 - accuracy: 0.5000\n",
      "Epoch 67/500: - 0.0485s/step - loss: 14.2578 - accuracy: 0.5000\n",
      "Epoch 68/500: - 0.0484s/step - loss: 13.8171 - accuracy: 0.5000\n",
      "Epoch 69/500: - 0.0483s/step - loss: 14.2737 - accuracy: 0.5000\n",
      "Epoch 70/500: - 0.0481s/step - loss: 14.4095 - accuracy: 0.7500\n",
      "Epoch 71/500: - 0.0480s/step - loss: 14.2941 - accuracy: 0.7500\n",
      "Epoch 72/500: - 0.0481s/step - loss: 14.1831 - accuracy: 0.7500\n",
      "Epoch 73/500: - 0.0480s/step - loss: 14.0761 - accuracy: 0.7500\n",
      "Epoch 74/500: - 0.0479s/step - loss: 13.9729 - accuracy: 0.5000\n",
      "Epoch 75/500: - 0.0480s/step - loss: 13.8733 - accuracy: 0.5000\n",
      "Epoch 76/500: - 0.0483s/step - loss: 14.1032 - accuracy: 0.5000\n",
      "Epoch 77/500: - 0.0486s/step - loss: 13.5021 - accuracy: 0.7500\n",
      "Epoch 78/500: - 0.0487s/step - loss: 13.4221 - accuracy: 0.7500\n",
      "Epoch 79/500: - 0.0488s/step - loss: 13.2937 - accuracy: 0.5000\n",
      "Epoch 80/500: - 0.0490s/step - loss: 13.1720 - accuracy: 0.5000\n",
      "Epoch 81/500: - 0.0492s/step - loss: 13.0563 - accuracy: 0.7500\n",
      "Epoch 82/500: - 0.0494s/step - loss: 12.9458 - accuracy: 0.5000\n",
      "Epoch 83/500: - 0.0498s/step - loss: 12.8401 - accuracy: 0.5000\n",
      "Epoch 84/500: - 0.0500s/step - loss: 13.3142 - accuracy: 0.5000\n",
      "Epoch 85/500: - 0.0499s/step - loss: 13.2286 - accuracy: 0.5000\n",
      "Epoch 86/500: - 0.0498s/step - loss: 13.5672 - accuracy: 0.5000\n",
      "Epoch 87/500: - 0.0497s/step - loss: 13.4640 - accuracy: 0.7500\n",
      "Epoch 88/500: - 0.0496s/step - loss: 13.3649 - accuracy: 0.7500\n",
      "Epoch 89/500: - 0.0496s/step - loss: 13.2693 - accuracy: 0.7500\n",
      "Epoch 90/500: - 0.0495s/step - loss: 12.9385 - accuracy: 0.7500\n",
      "Epoch 91/500: - 0.0494s/step - loss: 13.0796 - accuracy: 0.5000\n",
      "Epoch 92/500: - 0.0492s/step - loss: 13.3682 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model2.update_weights(images, network2[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #               zip(train_ds, network)]\n",
    "        \n",
    "        network_new2 = []\n",
    "        for (images, labels), net, hmc_kernel in zip(train_ds, network2, kernels2):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            net_current = net_current[0]\n",
    "        \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.split(new_state, [30], axis = 1)   \n",
    "            network_new2.append(net_new)\n",
    "            \n",
    "        network2 = network_new2\n",
    "        \n",
    "        loss += -1 * tf.reduce_mean(model2.target_log_prob(images, network2[bs], labels))\n",
    "    \n",
    "    preds = [model2.get_predictions(images) for images, labels in train_ds]\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0513*445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.sigmoid(6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.sigmoid(-6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
