{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0, 0],\n",
    "           [0, 1],\n",
    "           [1, 0],\n",
    "           [1, 1]])\n",
    "y_train = np.array([[0],\n",
    "           [1],\n",
    "           [1],\n",
    "           [0]])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2_zero_one(x):\n",
    "    \n",
    "    t = tf.math.sigmoid(x)\n",
    "    #t = tf.cast(tf.math.greater(samples[0], 0.5), tf.int32)\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerange(x, r = 6.0):\n",
    "    \n",
    "    out_of_range = tf.cast(tf.math.greater(tf.math.abs(x), r), tf.float32)\n",
    "    sign = tf.math.sign(x)\n",
    "    \n",
    "    return x * (1 - out_of_range) + sign * r * out_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "class StochasticMLP(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes=[100], n_outputs=10):\n",
    "        super(StochasticMLP, self).__init__()\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.fc_layers = [Dense(layer_size) for layer_size in hidden_layer_sizes]\n",
    "        self.output_layer = Dense(n_outputs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \n",
    "        #x = Flatten()(x)\n",
    "        \n",
    "        network = []\n",
    "        \n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            \n",
    "            logits = layer(x)\n",
    "            x = tfp.distributions.Bernoulli(logits=logits).sample()\n",
    "            network.append(x)\n",
    "\n",
    "        final_logits = self.output_layer(x) # initial the weight of output layer\n",
    "            \n",
    "        return network\n",
    "    \n",
    "    def target_log_prob(self, x, h, y):\n",
    "        \n",
    "        h_current = convert2_zero_one([tf.cast(h_i, dtype=tf.float32) for h_i in h])\n",
    "        h_current = [h_current[0]]\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "    \n",
    "        nlog_prob = 0. # negative log probability\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "\n",
    "    def target_log_prob2(self, x, h, y):\n",
    "\n",
    "        h_current = convert2_zero_one(tf.split(h, self.hidden_layer_sizes, axis = 1))\n",
    "        h_current = [h_current[0]]\n",
    "        h_previous = [x] + h_current[:-1]\n",
    "        \n",
    "        nlog_prob = 0.\n",
    "        \n",
    "        for i, (cv, pv, layer) in enumerate(\n",
    "            zip(h_current, h_previous, self.fc_layers)):\n",
    "            \n",
    "            ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=cv, logits=layer(pv))\n",
    "            \n",
    "            nlog_prob += tf.reduce_sum(ce, axis = -1)\n",
    "        \n",
    "        fce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y, tf.float32), logits=self.output_layer(h_current[-1]))\n",
    "        nlog_prob += tf.reduce_sum(fce, axis = -1)\n",
    "            \n",
    "        return -1 * nlog_prob\n",
    "    \n",
    "    def generate_hmc_kernel(self, x, y, step_size = pow(1000, -1/4)):\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = step_size),\n",
    "            num_adaptation_steps=int(100 * 0.8))\n",
    "        \n",
    "        return adaptive_hmc\n",
    "    \n",
    "    # new proposing-state method with HamiltonianMonteCarlo\n",
    "    def propose_new_state_hamiltonian(self, x, h, y):\n",
    "    \n",
    "        h_current = h\n",
    "        h_current = [tf.cast(h_i, dtype=tf.float32) for h_i in h_current]\n",
    "        h_current = h_current[0]\n",
    "\n",
    "        # initialize the HMC transition kernel\n",
    "        \n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn = lambda v: self.target_log_prob2(x, v, y),\n",
    "            num_leapfrog_steps = 2,\n",
    "            step_size = pow(1000, -1/4)),\n",
    "            num_adaptation_steps=int(100*0.8))\n",
    "\n",
    "        # run the chain (with burn-in)\n",
    "        num_results = 1\n",
    "        num_burnin_steps = 100\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = h_current, # may need to be reshaped\n",
    "            kernel = adaptive_hmc,\n",
    "            trace_fn = None)\n",
    "\n",
    "        h_new = tf.split(samples[0], self.hidden_layer_sizes, axis = 1)\n",
    "\n",
    "        return(h_new)\n",
    "    \n",
    "    def update_weights(self, x, h, y, lr = 0.1):\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -1 * tf.reduce_mean(self.target_log_prob(x, h, y))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    def get_predictions(self, x):\n",
    "\n",
    "        logits = 0.0\n",
    "        for layer in self.fc_layers:\n",
    "            logits = layer(x)\n",
    "            x = tf.math.sigmoid(logits)\n",
    "        \n",
    "        logits = self.output_layer(x)\n",
    "        probs = tf.math.sigmoid(logits)\n",
    "        #print(probs)\n",
    "        labels = tf.cast(tf.math.greater(probs, 0.5), tf.int32)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticMLP(hidden_layer_sizes = [2], n_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weight\n",
    "w_0 = np.array([[1, -1], [1, -1]], dtype = \"float32\")\n",
    "b_0 = np.array([-0.5, 1], dtype = \"float32\")\n",
    "l_0 = [w_0, b_0]\n",
    "\n",
    "w_1 = np.array([[1], [1]], dtype = \"float32\")\n",
    "b_1 = np.array([-1], dtype = \"float32\")\n",
    "l_1 = [w_1, b_1]\n",
    "\n",
    "model.fc_layers[0].set_weights(l_0)\n",
    "model.output_layer.set_weights(l_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "  array([[0, 1],\n",
       "         [0, 1],\n",
       "         [1, 1],\n",
       "         [0, 0]], dtype=int32)>]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = [model.call(images) for images, labels in train_ds]\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [model.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "burnin = 1000\n",
    "step_sizes = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    network_new = []\n",
    "    kernels_new = []\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network, kernels):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        net_current = net_current[0]\n",
    "        \n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2].new_step_size.numpy()\n",
    "        step_sizes.append(new_step_size)\n",
    "        \n",
    "        new_state = rerange(samples[0][0])\n",
    "        net_new = tf.split(new_state, [2], axis = 1)   \n",
    "        network_new.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new.append(ker_new)\n",
    "            \n",
    "    network = network_new\n",
    "    kernels = kernels_new\n",
    "    \n",
    "    #print(network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes)), step_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
      "array([[-3.3686607, -6.       ],\n",
      "       [-6.       ,  6.       ],\n",
      "       [ 6.       ,  6.       ],\n",
      "       [-6.       , -4.750222 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: - 0.0592s/step - loss: 1.9642 - accuracy: 0.7500\n",
      "Epoch 2/500: - 0.0723s/step - loss: 2.3294 - accuracy: 0.7500\n",
      "Epoch 3/500: - 0.0627s/step - loss: 1.2504 - accuracy: 0.7500\n",
      "Epoch 4/500: - 0.0577s/step - loss: 1.2359 - accuracy: 0.7500\n",
      "Epoch 5/500: - 0.0552s/step - loss: 1.2233 - accuracy: 0.7500\n",
      "Epoch 6/500: - 0.0548s/step - loss: 1.2148 - accuracy: 0.7500\n",
      "Epoch 7/500: - 0.0525s/step - loss: 1.2034 - accuracy: 0.7500\n",
      "Epoch 8/500: - 0.0504s/step - loss: 0.8130 - accuracy: 0.7500\n",
      "Epoch 9/500: - 0.0490s/step - loss: 0.8114 - accuracy: 0.7500\n",
      "Epoch 10/500: - 0.0477s/step - loss: 0.8086 - accuracy: 0.7500\n",
      "Epoch 11/500: - 0.0482s/step - loss: 0.8071 - accuracy: 0.7500\n",
      "Epoch 12/500: - 0.0482s/step - loss: 0.8057 - accuracy: 0.7500\n",
      "Epoch 13/500: - 0.0478s/step - loss: 0.8042 - accuracy: 0.7500\n",
      "Epoch 14/500: - 0.0472s/step - loss: 0.8028 - accuracy: 0.7500\n",
      "Epoch 15/500: - 0.0466s/step - loss: 1.2814 - accuracy: 0.7500\n",
      "Epoch 16/500: - 0.0463s/step - loss: 1.2676 - accuracy: 0.7500\n",
      "Epoch 17/500: - 0.0467s/step - loss: 1.2582 - accuracy: 0.7500\n",
      "Epoch 18/500: - 0.0462s/step - loss: 1.2425 - accuracy: 1.0000\n",
      "Epoch 19/500: - 0.0460s/step - loss: 1.2310 - accuracy: 1.0000\n",
      "Epoch 20/500: - 0.0456s/step - loss: 1.2201 - accuracy: 1.0000\n",
      "Epoch 21/500: - 0.0454s/step - loss: 1.2098 - accuracy: 1.0000\n",
      "Epoch 22/500: - 0.0459s/step - loss: 1.2001 - accuracy: 1.0000\n",
      "Epoch 23/500: - 0.0455s/step - loss: 1.1909 - accuracy: 1.0000\n",
      "Epoch 24/500: - 0.0455s/step - loss: 1.1822 - accuracy: 1.0000\n",
      "Epoch 25/500: - 0.0454s/step - loss: 1.1739 - accuracy: 1.0000\n",
      "Epoch 26/500: - 0.0455s/step - loss: 1.1659 - accuracy: 1.0000\n",
      "Epoch 27/500: - 0.0467s/step - loss: 1.1606 - accuracy: 1.0000\n",
      "Epoch 28/500: - 0.0475s/step - loss: 0.8077 - accuracy: 1.0000\n",
      "Epoch 29/500: - 0.0480s/step - loss: 0.8056 - accuracy: 1.0000\n",
      "Epoch 30/500: - 0.0487s/step - loss: 0.8036 - accuracy: 1.0000\n",
      "Epoch 31/500: - 0.0488s/step - loss: 0.8016 - accuracy: 1.0000\n",
      "Epoch 32/500: - 0.0485s/step - loss: 0.7998 - accuracy: 1.0000\n",
      "Epoch 33/500: - 0.0483s/step - loss: 1.4049 - accuracy: 1.0000\n",
      "Epoch 34/500: - 0.0486s/step - loss: 1.1607 - accuracy: 1.0000\n",
      "Epoch 35/500: - 0.0483s/step - loss: 1.1528 - accuracy: 1.0000\n",
      "Epoch 36/500: - 0.0481s/step - loss: 1.5948 - accuracy: 1.0000\n",
      "Epoch 37/500: - 0.0479s/step - loss: 1.5789 - accuracy: 1.0000\n",
      "Epoch 38/500: - 0.0478s/step - loss: 1.5637 - accuracy: 1.0000\n",
      "Epoch 39/500: - 0.0479s/step - loss: 1.2203 - accuracy: 1.0000\n",
      "Epoch 40/500: - 0.0477s/step - loss: 1.2070 - accuracy: 1.0000\n",
      "Epoch 41/500: - 0.0474s/step - loss: 0.8053 - accuracy: 1.0000\n",
      "Epoch 42/500: - 0.0474s/step - loss: 0.8066 - accuracy: 1.0000\n",
      "Epoch 43/500: - 0.0473s/step - loss: 0.8011 - accuracy: 1.0000\n",
      "Epoch 44/500: - 0.0475s/step - loss: 0.7989 - accuracy: 1.0000\n",
      "Epoch 45/500: - 0.0473s/step - loss: 0.7969 - accuracy: 1.0000\n",
      "Epoch 46/500: - 0.0472s/step - loss: 1.2008 - accuracy: 1.0000\n",
      "Epoch 47/500: - 0.0471s/step - loss: 1.1888 - accuracy: 1.0000\n",
      "Epoch 48/500: - 0.0469s/step - loss: 1.6387 - accuracy: 1.0000\n",
      "Epoch 49/500: - 0.0470s/step - loss: 1.6279 - accuracy: 1.0000\n",
      "Epoch 50/500: - 0.0471s/step - loss: 1.6174 - accuracy: 1.0000\n",
      "Epoch 51/500: - 0.0475s/step - loss: 2.2413 - accuracy: 1.0000\n",
      "Epoch 52/500: - 0.0479s/step - loss: 2.0675 - accuracy: 1.0000\n",
      "Epoch 53/500: - 0.0481s/step - loss: 2.5378 - accuracy: 1.0000\n",
      "Epoch 54/500: - 0.0479s/step - loss: 2.5027 - accuracy: 1.0000\n",
      "Epoch 55/500: - 0.0478s/step - loss: 1.1528 - accuracy: 1.0000\n",
      "Epoch 56/500: - 0.0477s/step - loss: 1.1454 - accuracy: 1.0000\n",
      "Epoch 57/500: - 0.0477s/step - loss: 1.1385 - accuracy: 1.0000\n",
      "Epoch 58/500: - 0.0478s/step - loss: 0.8078 - accuracy: 1.0000\n",
      "Epoch 59/500: - 0.0476s/step - loss: 0.8056 - accuracy: 1.0000\n",
      "Epoch 60/500: - 0.0474s/step - loss: 0.8035 - accuracy: 1.0000\n",
      "Epoch 61/500: - 0.0473s/step - loss: 0.7988 - accuracy: 1.0000\n",
      "Epoch 62/500: - 0.0472s/step - loss: 0.7968 - accuracy: 1.0000\n",
      "Epoch 63/500: - 0.0473s/step - loss: 0.7949 - accuracy: 1.0000\n",
      "Epoch 64/500: - 0.0472s/step - loss: 0.7934 - accuracy: 1.0000\n",
      "Epoch 65/500: - 0.0471s/step - loss: 0.7913 - accuracy: 1.0000\n",
      "Epoch 66/500: - 0.0469s/step - loss: 0.7935 - accuracy: 1.0000\n",
      "Epoch 67/500: - 0.0469s/step - loss: 0.7880 - accuracy: 1.0000\n",
      "Epoch 68/500: - 0.0471s/step - loss: 0.7866 - accuracy: 1.0000\n",
      "Epoch 69/500: - 0.0470s/step - loss: 0.7851 - accuracy: 1.0000\n",
      "Epoch 70/500: - 0.0469s/step - loss: 0.7851 - accuracy: 1.0000\n",
      "Epoch 71/500: - 0.0467s/step - loss: 0.7836 - accuracy: 1.0000\n",
      "Epoch 72/500: - 0.0466s/step - loss: 0.7811 - accuracy: 1.0000\n",
      "Epoch 73/500: - 0.0470s/step - loss: 1.8990 - accuracy: 1.0000\n",
      "Epoch 74/500: - 0.0474s/step - loss: 1.8789 - accuracy: 1.0000\n",
      "Epoch 75/500: - 0.0477s/step - loss: 2.0809 - accuracy: 1.0000\n",
      "Epoch 76/500: - 0.0481s/step - loss: 1.3216 - accuracy: 1.0000\n",
      "Epoch 77/500: - 0.0480s/step - loss: 2.0856 - accuracy: 1.0000\n",
      "Epoch 78/500: - 0.0479s/step - loss: 1.2829 - accuracy: 0.7500\n",
      "Epoch 79/500: - 0.0478s/step - loss: 1.1002 - accuracy: 0.7500\n",
      "Epoch 80/500: - 0.0479s/step - loss: 1.0825 - accuracy: 0.7500\n",
      "Epoch 81/500: - 0.0478s/step - loss: 0.7859 - accuracy: 0.7500\n",
      "Epoch 82/500: - 0.0477s/step - loss: 0.7839 - accuracy: 0.7500\n",
      "Epoch 83/500: - 0.0476s/step - loss: 1.5550 - accuracy: 0.7500\n",
      "Epoch 84/500: - 0.0475s/step - loss: 1.5373 - accuracy: 0.7500\n",
      "Epoch 85/500: - 0.0475s/step - loss: 0.7784 - accuracy: 0.7500\n",
      "Epoch 86/500: - 0.0474s/step - loss: 0.7767 - accuracy: 0.7500\n",
      "Epoch 87/500: - 0.0474s/step - loss: 0.7757 - accuracy: 0.7500\n",
      "Epoch 88/500: - 0.0473s/step - loss: 0.7737 - accuracy: 0.7500\n",
      "Epoch 89/500: - 0.0474s/step - loss: 1.5289 - accuracy: 0.7500\n",
      "Epoch 90/500: - 0.0475s/step - loss: 1.4917 - accuracy: 0.7500\n",
      "Epoch 91/500: - 0.0473s/step - loss: 1.4576 - accuracy: 0.7500\n",
      "Epoch 92/500: - 0.0472s/step - loss: 1.4261 - accuracy: 1.0000\n",
      "Epoch 93/500: - 0.0471s/step - loss: 1.3809 - accuracy: 1.0000\n",
      "Epoch 94/500: - 0.0470s/step - loss: 0.9924 - accuracy: 1.0000\n",
      "Epoch 95/500: - 0.0471s/step - loss: 0.9969 - accuracy: 1.0000\n",
      "Epoch 96/500: - 0.0470s/step - loss: 0.9899 - accuracy: 1.0000\n",
      "Epoch 97/500: - 0.0470s/step - loss: 0.9760 - accuracy: 1.0000\n",
      "Epoch 98/500: - 0.0469s/step - loss: 0.9663 - accuracy: 1.0000\n",
      "Epoch 99/500: - 0.0468s/step - loss: 1.9005 - accuracy: 1.0000\n",
      "Epoch 100/500: - 0.0468s/step - loss: 1.8743 - accuracy: 0.7500\n",
      "Epoch 101/500: - 0.0468s/step - loss: 0.9507 - accuracy: 0.7500\n",
      "Epoch 102/500: - 0.0468s/step - loss: 0.9339 - accuracy: 0.7500\n",
      "Epoch 103/500: - 0.0469s/step - loss: 0.9271 - accuracy: 0.7500\n",
      "Epoch 104/500: - 0.0472s/step - loss: 1.3090 - accuracy: 0.7500\n",
      "Epoch 105/500: - 0.0473s/step - loss: 1.1987 - accuracy: 0.7500\n",
      "Epoch 106/500: - 0.0474s/step - loss: 1.1892 - accuracy: 0.7500\n",
      "Epoch 107/500: - 0.0473s/step - loss: 1.2035 - accuracy: 0.7500\n",
      "Epoch 108/500: - 0.0472s/step - loss: 1.1955 - accuracy: 1.0000\n",
      "Epoch 109/500: - 0.0472s/step - loss: 1.1883 - accuracy: 1.0000\n",
      "Epoch 110/500: - 0.0472s/step - loss: 1.1810 - accuracy: 1.0000\n",
      "Epoch 111/500: - 0.0472s/step - loss: 1.1743 - accuracy: 1.0000\n",
      "Epoch 112/500: - 0.0473s/step - loss: 1.1617 - accuracy: 1.0000\n",
      "Epoch 113/500: - 0.0472s/step - loss: 1.1540 - accuracy: 1.0000\n",
      "Epoch 114/500: - 0.0471s/step - loss: 1.1473 - accuracy: 1.0000\n",
      "Epoch 115/500: - 0.0471s/step - loss: 1.1400 - accuracy: 1.0000\n",
      "Epoch 116/500: - 0.0471s/step - loss: 0.8382 - accuracy: 1.0000\n",
      "Epoch 117/500: - 0.0470s/step - loss: 0.8374 - accuracy: 1.0000\n",
      "Epoch 118/500: - 0.0469s/step - loss: 0.8319 - accuracy: 1.0000\n",
      "Epoch 119/500: - 0.0468s/step - loss: 0.8211 - accuracy: 1.0000\n",
      "Epoch 120/500: - 0.0468s/step - loss: 0.8160 - accuracy: 1.0000\n",
      "Epoch 121/500: - 0.0473s/step - loss: 0.8165 - accuracy: 1.0000\n",
      "Epoch 122/500: - 0.0474s/step - loss: 1.1298 - accuracy: 1.0000\n",
      "Epoch 123/500: - 0.0473s/step - loss: 1.1228 - accuracy: 1.0000\n",
      "Epoch 124/500: - 0.0475s/step - loss: 1.7138 - accuracy: 1.0000\n",
      "Epoch 125/500: - 0.0477s/step - loss: 1.3886 - accuracy: 1.0000\n",
      "Epoch 126/500: - 0.0478s/step - loss: 1.5908 - accuracy: 1.0000\n",
      "Epoch 127/500: - 0.0480s/step - loss: 1.5840 - accuracy: 1.0000\n",
      "Epoch 128/500: - 0.0481s/step - loss: 1.3547 - accuracy: 1.0000\n",
      "Epoch 129/500: - 0.0488s/step - loss: 0.8356 - accuracy: 1.0000\n",
      "Epoch 130/500: - 0.0494s/step - loss: 1.1572 - accuracy: 1.0000\n",
      "Epoch 131/500: - 0.0497s/step - loss: 1.1516 - accuracy: 1.0000\n",
      "Epoch 132/500: - 0.0497s/step - loss: 1.1045 - accuracy: 1.0000\n",
      "Epoch 133/500: - 0.0497s/step - loss: 1.0989 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500: - 0.0497s/step - loss: 1.0932 - accuracy: 1.0000\n",
      "Epoch 135/500: - 0.0496s/step - loss: 1.0874 - accuracy: 1.0000\n",
      "Epoch 136/500: - 0.0496s/step - loss: 1.0826 - accuracy: 1.0000\n",
      "Epoch 137/500: - 0.0500s/step - loss: 1.0768 - accuracy: 1.0000\n",
      "Epoch 138/500: - 0.0502s/step - loss: 1.0742 - accuracy: 1.0000\n",
      "Epoch 139/500: - 0.0503s/step - loss: 1.0695 - accuracy: 1.0000\n",
      "Epoch 140/500: - 0.0502s/step - loss: 1.6803 - accuracy: 1.0000\n",
      "Epoch 141/500: - 0.0502s/step - loss: 1.6672 - accuracy: 1.0000\n",
      "Epoch 142/500: - 0.0501s/step - loss: 1.6521 - accuracy: 1.0000\n",
      "Epoch 143/500: - 0.0502s/step - loss: 1.0420 - accuracy: 1.0000\n",
      "Epoch 144/500: - 0.0503s/step - loss: 1.0380 - accuracy: 1.0000\n",
      "Epoch 145/500: - 0.0504s/step - loss: 0.8022 - accuracy: 1.0000\n",
      "Epoch 146/500: - 0.0505s/step - loss: 0.7999 - accuracy: 1.0000\n",
      "Epoch 147/500: - 0.0505s/step - loss: 0.7955 - accuracy: 1.0000\n",
      "Epoch 148/500: - 0.0505s/step - loss: 1.1127 - accuracy: 1.0000\n",
      "Epoch 149/500: - 0.0504s/step - loss: 1.0971 - accuracy: 1.0000\n",
      "Epoch 150/500: - 0.0503s/step - loss: 1.0802 - accuracy: 1.0000\n",
      "Epoch 151/500: - 0.0503s/step - loss: 1.6120 - accuracy: 1.0000\n",
      "Epoch 152/500: - 0.0503s/step - loss: 1.0610 - accuracy: 1.0000\n",
      "Epoch 153/500: - 0.0503s/step - loss: 1.8651 - accuracy: 1.0000\n",
      "Epoch 154/500: - 0.0503s/step - loss: 1.5973 - accuracy: 1.0000\n",
      "Epoch 155/500: - 0.0505s/step - loss: 1.5801 - accuracy: 1.0000\n",
      "Epoch 156/500: - 0.0505s/step - loss: 1.5834 - accuracy: 1.0000\n",
      "Epoch 157/500: - 0.0506s/step - loss: 1.0382 - accuracy: 1.0000\n",
      "Epoch 158/500: - 0.0506s/step - loss: 1.0255 - accuracy: 1.0000\n",
      "Epoch 159/500: - 0.0507s/step - loss: 1.0135 - accuracy: 1.0000\n",
      "Epoch 160/500: - 0.0508s/step - loss: 1.0000 - accuracy: 1.0000\n",
      "Epoch 161/500: - 0.0509s/step - loss: 0.9896 - accuracy: 1.0000\n",
      "Epoch 162/500: - 0.0510s/step - loss: 0.9797 - accuracy: 1.0000\n",
      "Epoch 163/500: - 0.0511s/step - loss: 0.8305 - accuracy: 1.0000\n",
      "Epoch 164/500: - 0.0511s/step - loss: 0.8248 - accuracy: 1.0000\n",
      "Epoch 165/500: - 0.0512s/step - loss: 0.8194 - accuracy: 1.0000\n",
      "Epoch 166/500: - 0.0512s/step - loss: 0.8143 - accuracy: 1.0000\n",
      "Epoch 167/500: - 0.0513s/step - loss: 0.8096 - accuracy: 1.0000\n",
      "Epoch 168/500: - 0.0514s/step - loss: 0.8136 - accuracy: 1.0000\n",
      "Epoch 169/500: - 0.0515s/step - loss: 1.4936 - accuracy: 1.0000\n",
      "Epoch 170/500: - 0.0516s/step - loss: 1.4605 - accuracy: 1.0000\n",
      "Epoch 171/500: - 0.0517s/step - loss: 1.4399 - accuracy: 1.0000\n",
      "Epoch 172/500: - 0.0518s/step - loss: 1.0124 - accuracy: 1.0000\n",
      "Epoch 173/500: - 0.0518s/step - loss: 1.0013 - accuracy: 1.0000\n",
      "Epoch 174/500: - 0.0519s/step - loss: 0.7883 - accuracy: 1.0000\n",
      "Epoch 175/500: - 0.0519s/step - loss: 1.0943 - accuracy: 1.0000\n",
      "Epoch 176/500: - 0.0520s/step - loss: 1.0879 - accuracy: 1.0000\n",
      "Epoch 177/500: - 0.0520s/step - loss: 1.0954 - accuracy: 1.0000\n",
      "Epoch 178/500: - 0.0521s/step - loss: 1.0876 - accuracy: 1.0000\n",
      "Epoch 179/500: - 0.0522s/step - loss: 1.0816 - accuracy: 1.0000\n",
      "Epoch 180/500: - 0.0521s/step - loss: 1.0758 - accuracy: 1.0000\n",
      "Epoch 181/500: - 0.0521s/step - loss: 1.0703 - accuracy: 1.0000\n",
      "Epoch 182/500: - 0.0520s/step - loss: 1.0650 - accuracy: 1.0000\n",
      "Epoch 183/500: - 0.0520s/step - loss: 1.0600 - accuracy: 1.0000\n",
      "Epoch 184/500: - 0.0520s/step - loss: 1.0551 - accuracy: 1.0000\n",
      "Epoch 185/500: - 0.0521s/step - loss: 0.8153 - accuracy: 1.0000\n",
      "Epoch 186/500: - 0.0522s/step - loss: 0.8122 - accuracy: 1.0000\n",
      "Epoch 187/500: - 0.0523s/step - loss: 1.0720 - accuracy: 1.0000\n",
      "Epoch 188/500: - 0.0523s/step - loss: 1.0493 - accuracy: 1.0000\n",
      "Epoch 189/500: - 0.0524s/step - loss: 1.0447 - accuracy: 1.0000\n",
      "Epoch 190/500: - 0.0524s/step - loss: 1.0629 - accuracy: 1.0000\n",
      "Epoch 191/500: - 0.0524s/step - loss: 1.0489 - accuracy: 1.0000\n",
      "Epoch 192/500: - 0.0525s/step - loss: 1.0361 - accuracy: 1.0000\n",
      "Epoch 193/500: - 0.0525s/step - loss: 1.0237 - accuracy: 1.0000\n",
      "Epoch 194/500: - 0.0524s/step - loss: 1.0119 - accuracy: 1.0000\n",
      "Epoch 195/500: - 0.0524s/step - loss: 1.0000 - accuracy: 1.0000\n",
      "Epoch 196/500: - 0.0524s/step - loss: 0.9894 - accuracy: 1.0000\n",
      "Epoch 197/500: - 0.0524s/step - loss: 0.9794 - accuracy: 1.0000\n",
      "Epoch 198/500: - 0.0525s/step - loss: 1.7900 - accuracy: 1.0000\n",
      "Epoch 199/500: - 0.0525s/step - loss: 1.2047 - accuracy: 1.0000\n",
      "Epoch 200/500: - 0.0525s/step - loss: 1.1901 - accuracy: 1.0000\n",
      "Epoch 201/500: - 0.0526s/step - loss: 1.1767 - accuracy: 1.0000\n",
      "Epoch 202/500: - 0.0526s/step - loss: 0.9356 - accuracy: 1.0000\n",
      "Epoch 203/500: - 0.0525s/step - loss: 0.9423 - accuracy: 1.0000\n",
      "Epoch 204/500: - 0.0525s/step - loss: 0.9351 - accuracy: 1.0000\n",
      "Epoch 205/500: - 0.0524s/step - loss: 0.9279 - accuracy: 1.0000\n",
      "Epoch 206/500: - 0.0525s/step - loss: 0.9214 - accuracy: 1.0000\n",
      "Epoch 207/500: - 0.0525s/step - loss: 0.8694 - accuracy: 1.0000\n",
      "Epoch 208/500: - 0.0526s/step - loss: 0.8605 - accuracy: 1.0000\n",
      "Epoch 209/500: - 0.0527s/step - loss: 0.8521 - accuracy: 1.0000\n",
      "Epoch 210/500: - 0.0527s/step - loss: 0.8443 - accuracy: 1.0000\n",
      "Epoch 211/500: - 0.0526s/step - loss: 0.8371 - accuracy: 1.0000\n",
      "Epoch 212/500: - 0.0526s/step - loss: 0.9159 - accuracy: 1.0000\n",
      "Epoch 213/500: - 0.0525s/step - loss: 0.8334 - accuracy: 1.0000\n",
      "Epoch 214/500: - 0.0525s/step - loss: 0.8266 - accuracy: 1.0000\n",
      "Epoch 215/500: - 0.0526s/step - loss: 0.8201 - accuracy: 0.7500\n",
      "Epoch 216/500: - 0.0526s/step - loss: 0.8140 - accuracy: 0.7500\n",
      "Epoch 217/500: - 0.0526s/step - loss: 0.8083 - accuracy: 0.7500\n",
      "Epoch 218/500: - 0.0526s/step - loss: 0.8029 - accuracy: 0.7500\n",
      "Epoch 219/500: - 0.0527s/step - loss: 0.7978 - accuracy: 0.7500\n",
      "Epoch 220/500: - 0.0527s/step - loss: 0.7929 - accuracy: 0.7500\n",
      "Epoch 221/500: - 0.0527s/step - loss: 0.7883 - accuracy: 0.7500\n",
      "Epoch 222/500: - 0.0526s/step - loss: 0.7839 - accuracy: 0.7500\n",
      "Epoch 223/500: - 0.0526s/step - loss: 0.7797 - accuracy: 0.7500\n",
      "Epoch 224/500: - 0.0527s/step - loss: 0.7759 - accuracy: 0.7500\n",
      "Epoch 225/500: - 0.0527s/step - loss: 0.7721 - accuracy: 0.7500\n",
      "Epoch 226/500: - 0.0528s/step - loss: 0.8033 - accuracy: 0.7500\n",
      "Epoch 227/500: - 0.0528s/step - loss: 0.8020 - accuracy: 0.7500\n",
      "Epoch 228/500: - 0.0528s/step - loss: 0.7648 - accuracy: 0.7500\n",
      "Epoch 229/500: - 0.0528s/step - loss: 0.7615 - accuracy: 0.7500\n",
      "Epoch 230/500: - 0.0527s/step - loss: 0.7590 - accuracy: 0.7500\n",
      "Epoch 231/500: - 0.0527s/step - loss: 0.7559 - accuracy: 0.7500\n",
      "Epoch 232/500: - 0.0527s/step - loss: 0.7523 - accuracy: 0.7500\n",
      "Epoch 233/500: - 0.0527s/step - loss: 0.7495 - accuracy: 0.7500\n",
      "Epoch 234/500: - 0.0528s/step - loss: 0.7468 - accuracy: 0.7500\n",
      "Epoch 235/500: - 0.0528s/step - loss: 1.0216 - accuracy: 0.7500\n",
      "Epoch 236/500: - 0.0528s/step - loss: 1.0119 - accuracy: 0.7500\n",
      "Epoch 237/500: - 0.0528s/step - loss: 1.6854 - accuracy: 0.7500\n",
      "Epoch 238/500: - 0.0528s/step - loss: 1.4390 - accuracy: 0.7500\n",
      "Epoch 239/500: - 0.0529s/step - loss: 1.4141 - accuracy: 0.7500\n",
      "Epoch 240/500: - 0.0530s/step - loss: 1.3905 - accuracy: 0.7500\n",
      "Epoch 241/500: - 0.0530s/step - loss: 1.3681 - accuracy: 0.7500\n",
      "Epoch 242/500: - 0.0531s/step - loss: 1.3467 - accuracy: 0.7500\n",
      "Epoch 243/500: - 0.0531s/step - loss: 0.7293 - accuracy: 0.7500\n",
      "Epoch 244/500: - 0.0531s/step - loss: 0.7276 - accuracy: 0.7500\n",
      "Epoch 245/500: - 0.0531s/step - loss: 0.7260 - accuracy: 0.7500\n",
      "Epoch 246/500: - 0.0530s/step - loss: 0.7243 - accuracy: 0.7500\n",
      "Epoch 247/500: - 0.0530s/step - loss: 0.7233 - accuracy: 0.7500\n",
      "Epoch 248/500: - 0.0529s/step - loss: 0.7218 - accuracy: 0.7500\n",
      "Epoch 249/500: - 0.0530s/step - loss: 0.7447 - accuracy: 0.7500\n",
      "Epoch 250/500: - 0.0530s/step - loss: 0.7436 - accuracy: 0.7500\n",
      "Epoch 251/500: - 0.0530s/step - loss: 0.7188 - accuracy: 0.7500\n",
      "Epoch 252/500: - 0.0530s/step - loss: 0.7167 - accuracy: 0.7500\n",
      "Epoch 253/500: - 0.0529s/step - loss: 0.7145 - accuracy: 0.7500\n",
      "Epoch 254/500: - 0.0529s/step - loss: 0.7131 - accuracy: 0.7500\n",
      "Epoch 255/500: - 0.0530s/step - loss: 0.7161 - accuracy: 0.7500\n",
      "Epoch 256/500: - 0.0530s/step - loss: 0.7105 - accuracy: 0.7500\n",
      "Epoch 257/500: - 0.0531s/step - loss: 0.7092 - accuracy: 0.7500\n",
      "Epoch 258/500: - 0.0532s/step - loss: 0.7079 - accuracy: 0.7500\n",
      "Epoch 259/500: - 0.0532s/step - loss: 0.7066 - accuracy: 0.7500\n",
      "Epoch 260/500: - 0.0533s/step - loss: 0.7054 - accuracy: 0.7500\n",
      "Epoch 261/500: - 0.0533s/step - loss: 0.7041 - accuracy: 0.7500\n",
      "Epoch 262/500: - 0.0533s/step - loss: 1.1398 - accuracy: 0.7500\n",
      "Epoch 263/500: - 0.0533s/step - loss: 0.7042 - accuracy: 1.0000\n",
      "Epoch 264/500: - 0.0533s/step - loss: 0.7032 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/500: - 0.0533s/step - loss: 0.7019 - accuracy: 1.0000\n",
      "Epoch 266/500: - 0.0533s/step - loss: 0.7023 - accuracy: 1.0000\n",
      "Epoch 267/500: - 0.0533s/step - loss: 0.7011 - accuracy: 1.0000\n",
      "Epoch 268/500: - 0.0533s/step - loss: 0.6979 - accuracy: 1.0000\n",
      "Epoch 269/500: - 0.0532s/step - loss: 0.6968 - accuracy: 1.0000\n",
      "Epoch 270/500: - 0.0532s/step - loss: 0.6956 - accuracy: 1.0000\n",
      "Epoch 271/500: - 0.0532s/step - loss: 1.2737 - accuracy: 1.0000\n",
      "Epoch 272/500: - 0.0533s/step - loss: 1.2558 - accuracy: 1.0000\n",
      "Epoch 273/500: - 0.0532s/step - loss: 1.2399 - accuracy: 1.0000\n",
      "Epoch 274/500: - 0.0532s/step - loss: 1.2247 - accuracy: 1.0000\n",
      "Epoch 275/500: - 0.0532s/step - loss: 0.6930 - accuracy: 1.0000\n",
      "Epoch 276/500: - 0.0533s/step - loss: 0.6920 - accuracy: 1.0000\n",
      "Epoch 277/500: - 0.0533s/step - loss: 0.6898 - accuracy: 1.0000\n",
      "Epoch 278/500: - 0.0534s/step - loss: 0.6888 - accuracy: 1.0000\n",
      "Epoch 279/500: - 0.0534s/step - loss: 0.6878 - accuracy: 1.0000\n",
      "Epoch 280/500: - 0.0534s/step - loss: 0.6868 - accuracy: 1.0000\n",
      "Epoch 281/500: - 0.0533s/step - loss: 0.6859 - accuracy: 1.0000\n",
      "Epoch 282/500: - 0.0533s/step - loss: 0.6849 - accuracy: 1.0000\n",
      "Epoch 283/500: - 0.0533s/step - loss: 0.6840 - accuracy: 1.0000\n",
      "Epoch 284/500: - 0.0533s/step - loss: 0.6832 - accuracy: 1.0000\n",
      "Epoch 285/500: - 0.0534s/step - loss: 0.6850 - accuracy: 1.0000\n",
      "Epoch 286/500: - 0.0534s/step - loss: 0.6811 - accuracy: 1.0000\n",
      "Epoch 287/500: - 0.0534s/step - loss: 0.6806 - accuracy: 1.0000\n",
      "Epoch 288/500: - 0.0534s/step - loss: 1.1606 - accuracy: 1.0000\n",
      "Epoch 289/500: - 0.0534s/step - loss: 1.1489 - accuracy: 1.0000\n",
      "Epoch 290/500: - 0.0534s/step - loss: 1.1379 - accuracy: 1.0000\n",
      "Epoch 291/500: - 0.0535s/step - loss: 1.1292 - accuracy: 1.0000\n",
      "Epoch 292/500: - 0.0535s/step - loss: 0.6885 - accuracy: 1.0000\n",
      "Epoch 293/500: - 0.0535s/step - loss: 0.6850 - accuracy: 1.0000\n",
      "Epoch 294/500: - 0.0534s/step - loss: 0.6811 - accuracy: 1.0000\n",
      "Epoch 295/500: - 0.0534s/step - loss: 0.6800 - accuracy: 1.0000\n",
      "Epoch 296/500: - 0.0534s/step - loss: 0.6789 - accuracy: 1.0000\n",
      "Epoch 297/500: - 0.0535s/step - loss: 0.6778 - accuracy: 1.0000\n",
      "Epoch 298/500: - 0.0535s/step - loss: 0.6768 - accuracy: 1.0000\n",
      "Epoch 299/500: - 0.0535s/step - loss: 0.6789 - accuracy: 1.0000\n",
      "Epoch 300/500: - 0.0535s/step - loss: 0.6748 - accuracy: 1.0000\n",
      "Epoch 301/500: - 0.0535s/step - loss: 0.6759 - accuracy: 1.0000\n",
      "Epoch 302/500: - 0.0535s/step - loss: 0.6727 - accuracy: 1.0000\n",
      "Epoch 303/500: - 0.0535s/step - loss: 0.6716 - accuracy: 1.0000\n",
      "Epoch 304/500: - 0.0535s/step - loss: 0.6707 - accuracy: 1.0000\n",
      "Epoch 305/500: - 0.0536s/step - loss: 0.6697 - accuracy: 1.0000\n",
      "Epoch 306/500: - 0.0535s/step - loss: 0.6701 - accuracy: 1.0000\n",
      "Epoch 307/500: - 0.0535s/step - loss: 0.6703 - accuracy: 1.0000\n",
      "Epoch 308/500: - 0.0535s/step - loss: 0.6693 - accuracy: 1.0000\n",
      "Epoch 309/500: - 0.0535s/step - loss: 0.6684 - accuracy: 1.0000\n",
      "Epoch 310/500: - 0.0536s/step - loss: 0.6649 - accuracy: 1.0000\n",
      "Epoch 311/500: - 0.0536s/step - loss: 0.6640 - accuracy: 1.0000\n",
      "Epoch 312/500: - 0.0537s/step - loss: 0.6660 - accuracy: 1.0000\n",
      "Epoch 313/500: - 0.0536s/step - loss: 0.6621 - accuracy: 1.0000\n",
      "Epoch 314/500: - 0.0536s/step - loss: 0.6612 - accuracy: 1.0000\n",
      "Epoch 315/500: - 0.0536s/step - loss: 0.6642 - accuracy: 1.0000\n",
      "Epoch 316/500: - 0.0535s/step - loss: 0.6633 - accuracy: 1.0000\n",
      "Epoch 317/500: - 0.0537s/step - loss: 0.6601 - accuracy: 1.0000\n",
      "Epoch 318/500: - 0.0537s/step - loss: 1.4604 - accuracy: 1.0000\n",
      "Epoch 319/500: - 0.0537s/step - loss: 1.4310 - accuracy: 1.0000\n",
      "Epoch 320/500: - 0.0537s/step - loss: 1.4254 - accuracy: 1.0000\n",
      "Epoch 321/500: - 0.0538s/step - loss: 1.6959 - accuracy: 1.0000\n",
      "Epoch 322/500: - 0.0538s/step - loss: 1.4188 - accuracy: 1.0000\n",
      "Epoch 323/500: - 0.0537s/step - loss: 0.6639 - accuracy: 1.0000\n",
      "Epoch 324/500: - 0.0537s/step - loss: 0.6630 - accuracy: 1.0000\n",
      "Epoch 325/500: - 0.0537s/step - loss: 0.6621 - accuracy: 1.0000\n",
      "Epoch 326/500: - 0.0537s/step - loss: 0.6568 - accuracy: 1.0000\n",
      "Epoch 327/500: - 0.0537s/step - loss: 0.6557 - accuracy: 1.0000\n",
      "Epoch 328/500: - 0.0537s/step - loss: 0.6548 - accuracy: 1.0000\n",
      "Epoch 329/500: - 0.0536s/step - loss: 0.6539 - accuracy: 1.0000\n",
      "Epoch 330/500: - 0.0537s/step - loss: 0.6530 - accuracy: 1.0000\n",
      "Epoch 331/500: - 0.0537s/step - loss: 0.6530 - accuracy: 1.0000\n",
      "Epoch 332/500: - 0.0538s/step - loss: 0.6512 - accuracy: 1.0000\n",
      "Epoch 333/500: - 0.0538s/step - loss: 0.6525 - accuracy: 1.0000\n",
      "Epoch 334/500: - 0.0538s/step - loss: 0.6495 - accuracy: 1.0000\n",
      "Epoch 335/500: - 0.0539s/step - loss: 0.6486 - accuracy: 1.0000\n",
      "Epoch 336/500: - 0.0539s/step - loss: 1.1510 - accuracy: 1.0000\n",
      "Epoch 337/500: - 0.0539s/step - loss: 0.6504 - accuracy: 1.0000\n",
      "Epoch 338/500: - 0.0539s/step - loss: 0.6491 - accuracy: 1.0000\n",
      "Epoch 339/500: - 0.0540s/step - loss: 1.1505 - accuracy: 1.0000\n",
      "Epoch 340/500: - 0.0539s/step - loss: 1.1391 - accuracy: 1.0000\n",
      "Epoch 341/500: - 0.0539s/step - loss: 0.6476 - accuracy: 1.0000\n",
      "Epoch 342/500: - 0.0540s/step - loss: 0.6467 - accuracy: 1.0000\n",
      "Epoch 343/500: - 0.0540s/step - loss: 0.6458 - accuracy: 1.0000\n",
      "Epoch 344/500: - 0.0539s/step - loss: 0.6449 - accuracy: 1.0000\n",
      "Epoch 345/500: - 0.0539s/step - loss: 0.6440 - accuracy: 1.0000\n",
      "Epoch 346/500: - 0.0539s/step - loss: 0.6431 - accuracy: 1.0000\n",
      "Epoch 347/500: - 0.0540s/step - loss: 0.6452 - accuracy: 1.0000\n",
      "Epoch 348/500: - 0.0540s/step - loss: 0.6444 - accuracy: 1.0000\n",
      "Epoch 349/500: - 0.0540s/step - loss: 1.4508 - accuracy: 1.0000\n",
      "Epoch 350/500: - 0.0540s/step - loss: 0.6422 - accuracy: 1.0000\n",
      "Epoch 351/500: - 0.0541s/step - loss: 0.6397 - accuracy: 1.0000\n",
      "Epoch 352/500: - 0.0540s/step - loss: 0.6388 - accuracy: 1.0000\n",
      "Epoch 353/500: - 0.0540s/step - loss: 0.6380 - accuracy: 1.0000\n",
      "Epoch 354/500: - 0.0540s/step - loss: 0.6372 - accuracy: 1.0000\n",
      "Epoch 355/500: - 0.0540s/step - loss: 0.6363 - accuracy: 1.0000\n",
      "Epoch 356/500: - 0.0540s/step - loss: 0.6355 - accuracy: 1.0000\n",
      "Epoch 357/500: - 0.0539s/step - loss: 0.6374 - accuracy: 1.0000\n",
      "Epoch 358/500: - 0.0539s/step - loss: 0.6366 - accuracy: 1.0000\n",
      "Epoch 359/500: - 0.0539s/step - loss: 0.6358 - accuracy: 1.0000\n",
      "Epoch 360/500: - 0.0540s/step - loss: 0.6323 - accuracy: 1.0000\n",
      "Epoch 361/500: - 0.0540s/step - loss: 0.6348 - accuracy: 1.0000\n",
      "Epoch 362/500: - 0.0540s/step - loss: 0.6340 - accuracy: 1.0000\n",
      "Epoch 363/500: - 0.0540s/step - loss: 0.6312 - accuracy: 1.0000\n",
      "Epoch 364/500: - 0.0540s/step - loss: 0.6295 - accuracy: 1.0000\n",
      "Epoch 365/500: - 0.0539s/step - loss: 0.6285 - accuracy: 1.0000\n",
      "Epoch 366/500: - 0.0539s/step - loss: 0.6277 - accuracy: 1.0000\n",
      "Epoch 367/500: - 0.0540s/step - loss: 0.6269 - accuracy: 1.0000\n",
      "Epoch 368/500: - 0.0539s/step - loss: 0.6261 - accuracy: 1.0000\n",
      "Epoch 369/500: - 0.0539s/step - loss: 0.6253 - accuracy: 1.0000\n",
      "Epoch 370/500: - 0.0539s/step - loss: 0.6245 - accuracy: 1.0000\n",
      "Epoch 371/500: - 0.0539s/step - loss: 0.6241 - accuracy: 1.0000\n",
      "Epoch 372/500: - 0.0539s/step - loss: 0.6230 - accuracy: 1.0000\n",
      "Epoch 373/500: - 0.0539s/step - loss: 0.6223 - accuracy: 1.0000\n",
      "Epoch 374/500: - 0.0539s/step - loss: 0.6215 - accuracy: 1.0000\n",
      "Epoch 375/500: - 0.0539s/step - loss: 0.6208 - accuracy: 1.0000\n",
      "Epoch 376/500: - 0.0540s/step - loss: 0.6200 - accuracy: 1.0000\n",
      "Epoch 377/500: - 0.0540s/step - loss: 0.6213 - accuracy: 1.0000\n",
      "Epoch 378/500: - 0.0540s/step - loss: 0.6206 - accuracy: 1.0000\n",
      "Epoch 379/500: - 0.0541s/step - loss: 0.6199 - accuracy: 1.0000\n",
      "Epoch 380/500: - 0.0541s/step - loss: 0.6171 - accuracy: 1.0000\n",
      "Epoch 381/500: - 0.0541s/step - loss: 0.6164 - accuracy: 1.0000\n",
      "Epoch 382/500: - 0.0542s/step - loss: 0.6156 - accuracy: 1.0000\n",
      "Epoch 383/500: - 0.0542s/step - loss: 0.6163 - accuracy: 1.0000\n",
      "Epoch 384/500: - 0.0543s/step - loss: 0.6198 - accuracy: 1.0000\n",
      "Epoch 385/500: - 0.0543s/step - loss: 0.6169 - accuracy: 1.0000\n",
      "Epoch 386/500: - 0.0543s/step - loss: 0.6149 - accuracy: 1.0000\n",
      "Epoch 387/500: - 0.0543s/step - loss: 0.6142 - accuracy: 1.0000\n",
      "Epoch 388/500: - 0.0543s/step - loss: 1.1671 - accuracy: 1.0000\n",
      "Epoch 389/500: - 0.0543s/step - loss: 0.6167 - accuracy: 1.0000\n",
      "Epoch 390/500: - 0.0542s/step - loss: 0.6184 - accuracy: 1.0000\n",
      "Epoch 391/500: - 0.0542s/step - loss: 0.6106 - accuracy: 1.0000\n",
      "Epoch 392/500: - 0.0542s/step - loss: 0.6099 - accuracy: 1.0000\n",
      "Epoch 393/500: - 0.0542s/step - loss: 0.6092 - accuracy: 1.0000\n",
      "Epoch 394/500: - 0.0542s/step - loss: 0.6081 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/500: - 0.0542s/step - loss: 1.2739 - accuracy: 1.0000\n",
      "Epoch 396/500: - 0.0543s/step - loss: 0.6072 - accuracy: 1.0000\n",
      "Epoch 397/500: - 0.0543s/step - loss: 0.6065 - accuracy: 1.0000\n",
      "Epoch 398/500: - 0.0543s/step - loss: 0.6058 - accuracy: 1.0000\n",
      "Epoch 399/500: - 0.0543s/step - loss: 0.6051 - accuracy: 1.0000\n",
      "Epoch 400/500: - 0.0543s/step - loss: 0.6044 - accuracy: 1.0000\n",
      "Epoch 401/500: - 0.0543s/step - loss: 0.6037 - accuracy: 1.0000\n",
      "Epoch 402/500: - 0.0544s/step - loss: 0.6031 - accuracy: 1.0000\n",
      "Epoch 403/500: - 0.0544s/step - loss: 0.6024 - accuracy: 1.0000\n",
      "Epoch 404/500: - 0.0544s/step - loss: 0.6017 - accuracy: 1.0000\n",
      "Epoch 405/500: - 0.0544s/step - loss: 0.6010 - accuracy: 1.0000\n",
      "Epoch 406/500: - 0.0544s/step - loss: 0.6016 - accuracy: 1.0000\n",
      "Epoch 407/500: - 0.0544s/step - loss: 0.6009 - accuracy: 1.0000\n",
      "Epoch 408/500: - 0.0544s/step - loss: 0.6026 - accuracy: 1.0000\n",
      "Epoch 409/500: - 0.0544s/step - loss: 0.6007 - accuracy: 1.0000\n",
      "Epoch 410/500: - 0.0544s/step - loss: 0.5996 - accuracy: 1.0000\n",
      "Epoch 411/500: - 0.0544s/step - loss: 0.5970 - accuracy: 1.0000\n",
      "Epoch 412/500: - 0.0544s/step - loss: 0.5963 - accuracy: 1.0000\n",
      "Epoch 413/500: - 0.0543s/step - loss: 0.5957 - accuracy: 1.0000\n",
      "Epoch 414/500: - 0.0543s/step - loss: 0.5953 - accuracy: 1.0000\n",
      "Epoch 415/500: - 0.0543s/step - loss: 0.6038 - accuracy: 1.0000\n",
      "Epoch 416/500: - 0.0543s/step - loss: 0.5952 - accuracy: 1.0000\n",
      "Epoch 417/500: - 0.0542s/step - loss: 0.5932 - accuracy: 1.0000\n",
      "Epoch 418/500: - 0.0542s/step - loss: 0.5931 - accuracy: 1.0000\n",
      "Epoch 419/500: - 0.0542s/step - loss: 0.5917 - accuracy: 1.0000\n",
      "Epoch 420/500: - 0.0542s/step - loss: 0.5914 - accuracy: 1.0000\n",
      "Epoch 421/500: - 0.0542s/step - loss: 0.5904 - accuracy: 1.0000\n",
      "Epoch 422/500: - 0.0542s/step - loss: 0.5898 - accuracy: 1.0000\n",
      "Epoch 423/500: - 0.0542s/step - loss: 0.5891 - accuracy: 1.0000\n",
      "Epoch 424/500: - 0.0542s/step - loss: 0.5913 - accuracy: 1.0000\n",
      "Epoch 425/500: - 0.0542s/step - loss: 0.5902 - accuracy: 1.0000\n",
      "Epoch 426/500: - 0.0542s/step - loss: 0.5872 - accuracy: 1.0000\n",
      "Epoch 427/500: - 0.0543s/step - loss: 0.5866 - accuracy: 1.0000\n",
      "Epoch 428/500: - 0.0543s/step - loss: 0.5859 - accuracy: 1.0000\n",
      "Epoch 429/500: - 0.0543s/step - loss: 0.5853 - accuracy: 1.0000\n",
      "Epoch 430/500: - 0.0543s/step - loss: 1.1871 - accuracy: 1.0000\n",
      "Epoch 431/500: - 0.0542s/step - loss: 2.3691 - accuracy: 1.0000\n",
      "Epoch 432/500: - 0.0542s/step - loss: 2.3403 - accuracy: 1.0000\n",
      "Epoch 433/500: - 0.0542s/step - loss: 2.3126 - accuracy: 1.0000\n",
      "Epoch 434/500: - 0.0542s/step - loss: 1.7694 - accuracy: 1.0000\n",
      "Epoch 435/500: - 0.0542s/step - loss: 1.7503 - accuracy: 1.0000\n",
      "Epoch 436/500: - 0.0542s/step - loss: 1.7318 - accuracy: 1.0000\n",
      "Epoch 437/500: - 0.0542s/step - loss: 1.7139 - accuracy: 1.0000\n",
      "Epoch 438/500: - 0.0542s/step - loss: 1.1861 - accuracy: 1.0000\n",
      "Epoch 439/500: - 0.0542s/step - loss: 1.1738 - accuracy: 1.0000\n",
      "Epoch 440/500: - 0.0542s/step - loss: 1.1587 - accuracy: 1.0000\n",
      "Epoch 441/500: - 0.0542s/step - loss: 1.1454 - accuracy: 1.0000\n",
      "Epoch 442/500: - 0.0542s/step - loss: 1.1316 - accuracy: 1.0000\n",
      "Epoch 443/500: - 0.0542s/step - loss: 1.1173 - accuracy: 1.0000\n",
      "Epoch 444/500: - 0.0542s/step - loss: 1.1047 - accuracy: 1.0000\n",
      "Epoch 445/500: - 0.0542s/step - loss: 1.0888 - accuracy: 1.0000\n",
      "Epoch 446/500: - 0.0542s/step - loss: 1.0306 - accuracy: 1.0000\n",
      "Epoch 447/500: - 0.0542s/step - loss: 1.0213 - accuracy: 1.0000\n",
      "Epoch 448/500: - 0.0542s/step - loss: 1.8296 - accuracy: 1.0000\n",
      "Epoch 449/500: - 0.0542s/step - loss: 1.0003 - accuracy: 1.0000\n",
      "Epoch 450/500: - 0.0542s/step - loss: 1.0352 - accuracy: 1.0000\n",
      "Epoch 451/500: - 0.0542s/step - loss: 0.6268 - accuracy: 1.0000\n",
      "Epoch 452/500: - 0.0542s/step - loss: 0.6175 - accuracy: 1.0000\n",
      "Epoch 453/500: - 0.0542s/step - loss: 0.6156 - accuracy: 1.0000\n",
      "Epoch 454/500: - 0.0542s/step - loss: 0.6139 - accuracy: 1.0000\n",
      "Epoch 455/500: - 0.0542s/step - loss: 1.0356 - accuracy: 1.0000\n",
      "Epoch 456/500: - 0.0542s/step - loss: 1.0280 - accuracy: 1.0000\n",
      "Epoch 457/500: - 0.0542s/step - loss: 1.0190 - accuracy: 1.0000\n",
      "Epoch 458/500: - 0.0541s/step - loss: 1.0103 - accuracy: 1.0000\n",
      "Epoch 459/500: - 0.0541s/step - loss: 1.0003 - accuracy: 1.0000\n",
      "Epoch 460/500: - 0.0541s/step - loss: 0.9923 - accuracy: 1.0000\n",
      "Epoch 461/500: - 0.0542s/step - loss: 0.9847 - accuracy: 1.0000\n",
      "Epoch 462/500: - 0.0542s/step - loss: 0.9773 - accuracy: 1.0000\n",
      "Epoch 463/500: - 0.0542s/step - loss: 0.9702 - accuracy: 1.0000\n",
      "Epoch 464/500: - 0.0542s/step - loss: 0.9634 - accuracy: 1.0000\n",
      "Epoch 465/500: - 0.0543s/step - loss: 0.9591 - accuracy: 1.0000\n",
      "Epoch 466/500: - 0.0543s/step - loss: 1.4374 - accuracy: 1.0000\n",
      "Epoch 467/500: - 0.0543s/step - loss: 1.3888 - accuracy: 1.0000\n",
      "Epoch 468/500: - 0.0543s/step - loss: 0.9324 - accuracy: 1.0000\n",
      "Epoch 469/500: - 0.0543s/step - loss: 0.9272 - accuracy: 1.0000\n",
      "Epoch 470/500: - 0.0544s/step - loss: 0.6684 - accuracy: 1.0000\n",
      "Epoch 471/500: - 0.0544s/step - loss: 0.6642 - accuracy: 1.0000\n",
      "Epoch 472/500: - 0.0544s/step - loss: 1.6035 - accuracy: 1.0000\n",
      "Epoch 473/500: - 0.0545s/step - loss: 1.5822 - accuracy: 1.0000\n",
      "Epoch 474/500: - 0.0545s/step - loss: 1.5645 - accuracy: 1.0000\n",
      "Epoch 475/500: - 0.0545s/step - loss: 1.5460 - accuracy: 1.0000\n",
      "Epoch 476/500: - 0.0546s/step - loss: 2.2167 - accuracy: 1.0000\n",
      "Epoch 477/500: - 0.0546s/step - loss: 2.2005 - accuracy: 1.0000\n",
      "Epoch 478/500: - 0.0546s/step - loss: 2.1843 - accuracy: 1.0000\n",
      "Epoch 479/500: - 0.0546s/step - loss: 2.1708 - accuracy: 1.0000\n",
      "Epoch 480/500: - 0.0546s/step - loss: 2.1051 - accuracy: 1.0000\n",
      "Epoch 481/500: - 0.0545s/step - loss: 0.6341 - accuracy: 1.0000\n",
      "Epoch 482/500: - 0.0545s/step - loss: 0.6315 - accuracy: 1.0000\n",
      "Epoch 483/500: - 0.0545s/step - loss: 0.7499 - accuracy: 1.0000\n",
      "Epoch 484/500: - 0.0545s/step - loss: 0.6790 - accuracy: 1.0000\n",
      "Epoch 485/500: - 0.0545s/step - loss: 0.6768 - accuracy: 1.0000\n",
      "Epoch 486/500: - 0.0545s/step - loss: 0.6722 - accuracy: 1.0000\n",
      "Epoch 487/500: - 0.0544s/step - loss: 0.6701 - accuracy: 1.0000\n",
      "Epoch 488/500: - 0.0544s/step - loss: 0.6681 - accuracy: 1.0000\n",
      "Epoch 489/500: - 0.0544s/step - loss: 0.6662 - accuracy: 1.0000\n",
      "Epoch 490/500: - 0.0544s/step - loss: 0.6643 - accuracy: 1.0000\n",
      "Epoch 491/500: - 0.0543s/step - loss: 0.6625 - accuracy: 1.0000\n",
      "Epoch 492/500: - 0.0543s/step - loss: 1.2259 - accuracy: 1.0000\n",
      "Epoch 493/500: - 0.0543s/step - loss: 1.1633 - accuracy: 1.0000\n",
      "Epoch 494/500: - 0.0543s/step - loss: 1.1534 - accuracy: 1.0000\n",
      "Epoch 495/500: - 0.0543s/step - loss: 1.1438 - accuracy: 1.0000\n",
      "Epoch 496/500: - 0.0542s/step - loss: 1.1346 - accuracy: 1.0000\n",
      "Epoch 497/500: - 0.0542s/step - loss: 1.1254 - accuracy: 1.0000\n",
      "Epoch 498/500: - 0.0542s/step - loss: 0.6212 - accuracy: 1.0000\n",
      "Epoch 499/500: - 0.0542s/step - loss: 0.6186 - accuracy: 1.0000\n",
      "Epoch 500/500: - 0.0542s/step - loss: 0.6162 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model.update_weights(images, network[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #               zip(train_ds, network)]\n",
    "        \n",
    "        network_new = []\n",
    "        #kernels_new = []\n",
    "        for net, hmc_kernel in zip(network, kernels):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            net_current = net_current[0]\n",
    "        \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.split(new_state, [2], axis = 1)   \n",
    "            network_new.append(net_new)\n",
    "\n",
    "        network = network_new\n",
    "        \n",
    "        loss += -1 * tf.reduce_mean(model.target_log_prob(images, network[bs], labels))\n",
    "    \n",
    "    preds = [model.get_predictions(images) for images, labels in train_ds]\n",
    "    #print(preds)\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[ 3.73042  , -2.611004 ],\n",
      "       [ 3.599267 , -2.4101195]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(2,) dtype=float32, numpy=array([-0.8907312,  3.8896167], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[2.8464344],\n",
      "       [2.8018098]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-4.36044], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = StochasticMLP(hidden_layer_sizes = [30], n_outputs=1)\n",
    "network2 = [model2.call(images) for images, labels in train_ds]\n",
    "kernels2 = [model2.generate_hmc_kernel(images, labels) for images, labels in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor: shape=(4, 30), dtype=int32, numpy=\n",
      "array([[0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1],\n",
      "       [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1],\n",
      "       [1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0]], dtype=int32)>]]\n"
     ]
    }
   ],
   "source": [
    "print(network2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1000\n",
    "step_sizes2 = []\n",
    "for i in range(burnin):\n",
    "    \n",
    "    #print(i)\n",
    "    network_new2 = []\n",
    "    kernels_new2 = []\n",
    "    \n",
    "    for (images, labels), net, hmc_kernel in zip(train_ds, network2, kernels2):\n",
    "        net_current = net\n",
    "        net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "        net_current = net_current[0]\n",
    "        \n",
    "        num_results = 1\n",
    "        num_burnin_steps = 0\n",
    "\n",
    "        samples = tfp.mcmc.sample_chain(\n",
    "            num_results = num_results,\n",
    "            num_burnin_steps = num_burnin_steps,\n",
    "            current_state = net_current, # may need to be reshaped\n",
    "            kernel = hmc_kernel,\n",
    "            #trace_fn = lambda _, pkr: pkr.inner_results.accepted_results.new_step_size,\n",
    "            trace_fn = None,\n",
    "            return_final_kernel_results = True)\n",
    "        \n",
    "        #print(samples[2].new_step_size.numpy())\n",
    "        new_step_size = samples[2][4].numpy()\n",
    "        step_sizes2.append(new_step_size)\n",
    "        \n",
    "        new_state = rerange(samples[0][0])\n",
    "        #print(new_state)\n",
    "        net_new = tf.split(new_state, [30], axis = 1)   \n",
    "        network_new2.append(net_new)\n",
    "        \n",
    "        # build new kernel\n",
    "        ker_new = model2.generate_hmc_kernel(images, labels, new_step_size)\n",
    "        kernels_new2.append(ker_new)\n",
    "            \n",
    "    network2 = network_new2\n",
    "    kernels2 = kernels_new2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(step_sizes2)), step_sizes2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 30), dtype=float32, numpy=\n",
      "array([[-6.        , -6.        ,  6.        , -6.        ,  6.        ,\n",
      "        -6.        , -6.        , -6.        ,  6.        ,  6.        ,\n",
      "         4.6416097 ,  6.        , -6.        , -6.        ,  6.        ,\n",
      "         6.        , -6.        , -6.        ,  6.        ,  6.        ,\n",
      "        -6.        , -4.2812653 ,  6.        , -6.        , -6.        ,\n",
      "        -6.        , -6.        , -6.        ,  6.        , -0.46524215],\n",
      "       [-6.        ,  6.        , -0.09040737, -6.        , -6.        ,\n",
      "        -6.        ,  6.        , -4.9125853 , -6.        ,  6.        ,\n",
      "        -6.        ,  6.        ,  4.967575  ,  6.        ,  6.        ,\n",
      "         5.7838607 , -6.        ,  3.7494664 ,  6.        ,  1.7914963 ,\n",
      "         6.        ,  6.        , -6.        , -6.        ,  6.        ,\n",
      "        -5.7645793 , -6.        ,  2.6464858 ,  6.        ,  6.        ],\n",
      "       [-1.3647175 , -6.        ,  6.        , -1.7725346 ,  2.0486927 ,\n",
      "        -0.46802628, -6.        , -6.        , -6.        , -6.        ,\n",
      "        -4.7153006 , -6.        , -6.        , -1.969408  ,  6.        ,\n",
      "         1.3747052 ,  2.4878242 ,  0.40974164,  1.3197553 ,  6.        ,\n",
      "         6.        , -6.        , -3.2281303 , -6.        ,  6.        ,\n",
      "        -6.        ,  6.        , -6.        ,  1.0389435 , -6.        ],\n",
      "       [ 0.49992603,  6.        , -1.4093479 ,  6.        , -0.7662866 ,\n",
      "         6.        , -5.8493123 , -0.4621637 , -6.        ,  6.        ,\n",
      "        -6.        , -0.31927252, -6.        , -5.6067123 , -6.        ,\n",
      "         2.6152797 ,  6.        ,  6.        ,  4.114139  , -6.        ,\n",
      "         6.        , -4.3112426 ,  6.        ,  6.        ,  3.2047415 ,\n",
      "        -6.        , -2.4854088 ,  6.        , -0.31840158, -4.555962  ]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(network2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: - 0.0596s/step - loss: 20.9670 - accuracy: 0.5000\n",
      "Epoch 2/500: - 0.0575s/step - loss: 20.5372 - accuracy: 0.5000\n",
      "Epoch 3/500: - 0.0545s/step - loss: 20.3230 - accuracy: 0.5000\n",
      "Epoch 4/500: - 0.0533s/step - loss: 20.1909 - accuracy: 0.5000\n",
      "Epoch 5/500: - 0.0547s/step - loss: 19.7377 - accuracy: 0.5000\n",
      "Epoch 6/500: - 0.0575s/step - loss: 19.4584 - accuracy: 0.5000\n",
      "Epoch 7/500: - 0.0553s/step - loss: 19.5633 - accuracy: 0.5000\n",
      "Epoch 8/500: - 0.0540s/step - loss: 19.8760 - accuracy: 0.5000\n",
      "Epoch 9/500: - 0.0527s/step - loss: 19.2386 - accuracy: 0.5000\n",
      "Epoch 10/500: - 0.0532s/step - loss: 18.9133 - accuracy: 0.5000\n",
      "Epoch 11/500: - 0.0524s/step - loss: 19.1469 - accuracy: 0.5000\n",
      "Epoch 12/500: - 0.0513s/step - loss: 19.4612 - accuracy: 0.5000\n",
      "Epoch 13/500: - 0.0503s/step - loss: 19.6146 - accuracy: 0.5000\n",
      "Epoch 14/500: - 0.0498s/step - loss: 19.6077 - accuracy: 0.5000\n",
      "Epoch 15/500: - 0.0496s/step - loss: 19.0453 - accuracy: 0.5000\n",
      "Epoch 16/500: - 0.0516s/step - loss: 18.8100 - accuracy: 0.5000\n",
      "Epoch 17/500: - 0.0521s/step - loss: 19.5794 - accuracy: 0.5000\n",
      "Epoch 18/500: - 0.0524s/step - loss: 19.8815 - accuracy: 0.5000\n",
      "Epoch 19/500: - 0.0528s/step - loss: 19.7076 - accuracy: 0.5000\n",
      "Epoch 20/500: - 0.0534s/step - loss: 19.1598 - accuracy: 0.5000\n",
      "Epoch 21/500: - 0.0537s/step - loss: 18.2080 - accuracy: 0.5000\n",
      "Epoch 22/500: - 0.0535s/step - loss: 18.0078 - accuracy: 0.5000\n",
      "Epoch 23/500: - 0.0532s/step - loss: 17.4598 - accuracy: 0.5000\n",
      "Epoch 24/500: - 0.0539s/step - loss: 17.5900 - accuracy: 0.5000\n",
      "Epoch 25/500: - 0.0540s/step - loss: 17.2894 - accuracy: 0.5000\n",
      "Epoch 26/500: - 0.0543s/step - loss: 17.6763 - accuracy: 0.5000\n",
      "Epoch 27/500: - 0.0547s/step - loss: 17.8305 - accuracy: 0.5000\n",
      "Epoch 28/500: - 0.0548s/step - loss: 17.5445 - accuracy: 0.5000\n",
      "Epoch 29/500: - 0.0545s/step - loss: 17.5073 - accuracy: 0.5000\n",
      "Epoch 30/500: - 0.0540s/step - loss: 17.2243 - accuracy: 0.5000\n",
      "Epoch 31/500: - 0.0534s/step - loss: 16.9460 - accuracy: 0.7500\n",
      "Epoch 32/500: - 0.0532s/step - loss: 16.6034 - accuracy: 0.5000\n",
      "Epoch 33/500: - 0.0530s/step - loss: 16.3203 - accuracy: 0.5000\n",
      "Epoch 34/500: - 0.0526s/step - loss: 16.7540 - accuracy: 0.7500\n",
      "Epoch 35/500: - 0.0523s/step - loss: 16.5270 - accuracy: 0.7500\n",
      "Epoch 36/500: - 0.0520s/step - loss: 16.3126 - accuracy: 0.5000\n",
      "Epoch 37/500: - 0.0518s/step - loss: 16.1151 - accuracy: 0.5000\n",
      "Epoch 38/500: - 0.0521s/step - loss: 15.6323 - accuracy: 0.7500\n",
      "Epoch 39/500: - 0.0525s/step - loss: 15.8691 - accuracy: 0.7500\n",
      "Epoch 40/500: - 0.0525s/step - loss: 15.6778 - accuracy: 0.5000\n",
      "Epoch 41/500: - 0.0524s/step - loss: 15.4983 - accuracy: 0.5000\n",
      "Epoch 42/500: - 0.0524s/step - loss: 15.3289 - accuracy: 0.5000\n",
      "Epoch 43/500: - 0.0523s/step - loss: 14.8865 - accuracy: 0.5000\n",
      "Epoch 44/500: - 0.0521s/step - loss: 15.1511 - accuracy: 0.5000\n",
      "Epoch 45/500: - 0.0520s/step - loss: 14.9427 - accuracy: 0.7500\n",
      "Epoch 46/500: - 0.0520s/step - loss: 14.7484 - accuracy: 0.5000\n",
      "Epoch 47/500: - 0.0518s/step - loss: 14.3538 - accuracy: 0.2500\n",
      "Epoch 48/500: - 0.0515s/step - loss: 14.1891 - accuracy: 0.5000\n",
      "Epoch 49/500: - 0.0513s/step - loss: 14.0337 - accuracy: 0.7500\n",
      "Epoch 50/500: - 0.0510s/step - loss: 13.8864 - accuracy: 0.7500\n",
      "Epoch 51/500: - 0.0508s/step - loss: 13.7463 - accuracy: 0.7500\n",
      "Epoch 52/500: - 0.0510s/step - loss: 14.5893 - accuracy: 0.7500\n",
      "Epoch 53/500: - 0.0513s/step - loss: 14.9636 - accuracy: 0.5000\n",
      "Epoch 54/500: - 0.0516s/step - loss: 14.7983 - accuracy: 0.2500\n",
      "Epoch 55/500: - 0.0516s/step - loss: 14.6412 - accuracy: 0.2500\n",
      "Epoch 56/500: - 0.0515s/step - loss: 14.2019 - accuracy: 0.2500\n",
      "Epoch 57/500: - 0.0514s/step - loss: 14.0575 - accuracy: 0.5000\n",
      "Epoch 58/500: - 0.0512s/step - loss: 13.7708 - accuracy: 0.5000\n",
      "Epoch 59/500: - 0.0510s/step - loss: 13.8503 - accuracy: 0.5000\n",
      "Epoch 60/500: - 0.0508s/step - loss: 13.7017 - accuracy: 0.5000\n",
      "Epoch 61/500: - 0.0506s/step - loss: 13.5634 - accuracy: 0.5000\n",
      "Epoch 62/500: - 0.0507s/step - loss: 13.4328 - accuracy: 0.5000\n",
      "Epoch 63/500: - 0.0508s/step - loss: 13.3081 - accuracy: 0.5000\n",
      "Epoch 64/500: - 0.0512s/step - loss: 13.2824 - accuracy: 0.5000\n",
      "Epoch 65/500: - 0.0514s/step - loss: 13.5811 - accuracy: 0.5000\n",
      "Epoch 66/500: - 0.0515s/step - loss: 13.4533 - accuracy: 0.5000\n",
      "Epoch 67/500: - 0.0513s/step - loss: 13.2007 - accuracy: 0.5000\n",
      "Epoch 68/500: - 0.0511s/step - loss: 13.0686 - accuracy: 0.5000\n",
      "Epoch 69/500: - 0.0510s/step - loss: 12.9423 - accuracy: 0.5000\n",
      "Epoch 70/500: - 0.0508s/step - loss: 12.8213 - accuracy: 0.5000\n",
      "Epoch 71/500: - 0.0507s/step - loss: 12.7054 - accuracy: 0.5000\n",
      "Epoch 72/500: - 0.0506s/step - loss: 12.5941 - accuracy: 0.5000\n",
      "Epoch 73/500: - 0.0505s/step - loss: 12.3942 - accuracy: 0.5000\n",
      "Epoch 74/500: - 0.0503s/step - loss: 12.2840 - accuracy: 0.5000\n",
      "Epoch 75/500: - 0.0502s/step - loss: 12.1805 - accuracy: 0.5000\n",
      "Epoch 76/500: - 0.0502s/step - loss: 12.0825 - accuracy: 0.5000\n",
      "Epoch 77/500: - 0.0505s/step - loss: 11.9891 - accuracy: 0.5000\n",
      "Epoch 78/500: - 0.0507s/step - loss: 11.8997 - accuracy: 0.5000\n",
      "Epoch 79/500: - 0.0505s/step - loss: 11.8139 - accuracy: 0.5000\n",
      "Epoch 80/500: - 0.0504s/step - loss: 11.7314 - accuracy: 0.5000\n",
      "Epoch 81/500: - 0.0503s/step - loss: 11.6518 - accuracy: 0.5000\n",
      "Epoch 82/500: - 0.0502s/step - loss: 11.5750 - accuracy: 0.7500\n",
      "Epoch 83/500: - 0.0502s/step - loss: 11.5008 - accuracy: 0.7500\n",
      "Epoch 84/500: - 0.0501s/step - loss: 12.0818 - accuracy: 0.7500\n",
      "Epoch 85/500: - 0.0500s/step - loss: 12.0064 - accuracy: 0.7500\n",
      "Epoch 86/500: - 0.0500s/step - loss: 11.9333 - accuracy: 0.7500\n",
      "Epoch 87/500: - 0.0501s/step - loss: 12.2920 - accuracy: 0.5000\n",
      "Epoch 88/500: - 0.0503s/step - loss: 12.1988 - accuracy: 0.5000\n",
      "Epoch 89/500: - 0.0504s/step - loss: 12.1092 - accuracy: 0.2500\n",
      "Epoch 90/500: - 0.0505s/step - loss: 12.0228 - accuracy: 0.5000\n",
      "Epoch 91/500: - 0.0504s/step - loss: 11.8728 - accuracy: 0.5000\n",
      "Epoch 92/500: - 0.0504s/step - loss: 11.7938 - accuracy: 0.5000\n",
      "Epoch 93/500: - 0.0503s/step - loss: 11.7174 - accuracy: 0.5000\n",
      "Epoch 94/500: - 0.0501s/step - loss: 11.6434 - accuracy: 0.5000\n",
      "Epoch 95/500: - 0.0500s/step - loss: 11.5715 - accuracy: 0.5000\n",
      "Epoch 96/500: - 0.0501s/step - loss: 11.0865 - accuracy: 0.5000\n",
      "Epoch 97/500: - 0.0501s/step - loss: 11.0106 - accuracy: 0.5000\n",
      "Epoch 98/500: - 0.0501s/step - loss: 10.9371 - accuracy: 0.5000\n",
      "Epoch 99/500: - 0.0500s/step - loss: 10.8658 - accuracy: 0.5000\n",
      "Epoch 100/500: - 0.0499s/step - loss: 10.7967 - accuracy: 0.5000\n",
      "Epoch 101/500: - 0.0501s/step - loss: 10.8373 - accuracy: 0.5000\n",
      "Epoch 102/500: - 0.0503s/step - loss: 10.7618 - accuracy: 0.5000\n",
      "Epoch 103/500: - 0.0502s/step - loss: 10.2053 - accuracy: 0.5000\n",
      "Epoch 104/500: - 0.0502s/step - loss: 10.1473 - accuracy: 0.5000\n",
      "Epoch 105/500: - 0.0502s/step - loss: 10.0908 - accuracy: 0.5000\n",
      "Epoch 106/500: - 0.0503s/step - loss: 10.0359 - accuracy: 0.5000\n",
      "Epoch 107/500: - 0.0502s/step - loss: 10.3372 - accuracy: 0.5000\n",
      "Epoch 108/500: - 0.0501s/step - loss: 10.1779 - accuracy: 0.5000\n",
      "Epoch 109/500: - 0.0500s/step - loss: 10.1192 - accuracy: 0.5000\n",
      "Epoch 110/500: - 0.0500s/step - loss: 10.0630 - accuracy: 0.5000\n",
      "Epoch 111/500: - 0.0502s/step - loss: 10.0087 - accuracy: 0.5000\n",
      "Epoch 112/500: - 0.0502s/step - loss: 9.9562 - accuracy: 0.5000\n",
      "Epoch 113/500: - 0.0502s/step - loss: 9.9052 - accuracy: 0.5000\n",
      "Epoch 114/500: - 0.0501s/step - loss: 9.8555 - accuracy: 0.5000\n",
      "Epoch 115/500: - 0.0501s/step - loss: 9.8070 - accuracy: 0.5000\n",
      "Epoch 116/500: - 0.0500s/step - loss: 9.7597 - accuracy: 0.5000\n",
      "Epoch 117/500: - 0.0500s/step - loss: 9.7134 - accuracy: 0.5000\n",
      "Epoch 118/500: - 0.0499s/step - loss: 9.6682 - accuracy: 0.5000\n",
      "Epoch 119/500: - 0.0499s/step - loss: 9.6240 - accuracy: 0.5000\n",
      "Epoch 120/500: - 0.0498s/step - loss: 9.5807 - accuracy: 0.5000\n",
      "Epoch 121/500: - 0.0497s/step - loss: 9.5383 - accuracy: 0.5000\n",
      "Epoch 122/500: - 0.0497s/step - loss: 9.4967 - accuracy: 0.5000\n",
      "Epoch 123/500: - 0.0496s/step - loss: 9.4560 - accuracy: 0.5000\n",
      "Epoch 124/500: - 0.0495s/step - loss: 9.4160 - accuracy: 0.5000\n",
      "Epoch 125/500: - 0.0495s/step - loss: 9.3768 - accuracy: 0.5000\n",
      "Epoch 126/500: - 0.0494s/step - loss: 9.3383 - accuracy: 0.5000\n",
      "Epoch 127/500: - 0.0494s/step - loss: 9.3006 - accuracy: 0.5000\n",
      "Epoch 128/500: - 0.0494s/step - loss: 9.2635 - accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500: - 0.0497s/step - loss: 9.2271 - accuracy: 0.5000\n",
      "Epoch 130/500: - 0.0498s/step - loss: 9.1913 - accuracy: 0.5000\n",
      "Epoch 131/500: - 0.0498s/step - loss: 9.1562 - accuracy: 0.5000\n",
      "Epoch 132/500: - 0.0499s/step - loss: 9.1216 - accuracy: 0.5000\n",
      "Epoch 133/500: - 0.0499s/step - loss: 9.0876 - accuracy: 0.5000\n",
      "Epoch 134/500: - 0.0499s/step - loss: 9.0541 - accuracy: 0.5000\n",
      "Epoch 135/500: - 0.0498s/step - loss: 9.0212 - accuracy: 0.5000\n",
      "Epoch 136/500: - 0.0497s/step - loss: 8.9889 - accuracy: 0.5000\n",
      "Epoch 137/500: - 0.0497s/step - loss: 8.9570 - accuracy: 0.5000\n",
      "Epoch 138/500: - 0.0498s/step - loss: 8.9256 - accuracy: 0.5000\n",
      "Epoch 139/500: - 0.0498s/step - loss: 8.8947 - accuracy: 0.5000\n",
      "Epoch 140/500: - 0.0498s/step - loss: 8.8643 - accuracy: 0.5000\n",
      "Epoch 141/500: - 0.0497s/step - loss: 8.8343 - accuracy: 0.5000\n",
      "Epoch 142/500: - 0.0496s/step - loss: 8.8047 - accuracy: 0.5000\n",
      "Epoch 143/500: - 0.0496s/step - loss: 8.7756 - accuracy: 0.5000\n",
      "Epoch 144/500: - 0.0497s/step - loss: 8.7469 - accuracy: 0.5000\n",
      "Epoch 145/500: - 0.0497s/step - loss: 8.7186 - accuracy: 0.5000\n",
      "Epoch 146/500: - 0.0498s/step - loss: 8.6907 - accuracy: 0.5000\n",
      "Epoch 147/500: - 0.0499s/step - loss: 8.6631 - accuracy: 0.5000\n",
      "Epoch 148/500: - 0.0500s/step - loss: 8.6360 - accuracy: 0.5000\n",
      "Epoch 149/500: - 0.0500s/step - loss: 8.6092 - accuracy: 0.5000\n",
      "Epoch 150/500: - 0.0499s/step - loss: 8.5827 - accuracy: 0.5000\n",
      "Epoch 151/500: - 0.0499s/step - loss: 8.5567 - accuracy: 0.5000\n",
      "Epoch 152/500: - 0.0500s/step - loss: 8.5309 - accuracy: 0.5000\n",
      "Epoch 153/500: - 0.0500s/step - loss: 8.5055 - accuracy: 0.5000\n",
      "Epoch 154/500: - 0.0500s/step - loss: 8.4804 - accuracy: 0.5000\n",
      "Epoch 155/500: - 0.0501s/step - loss: 8.4556 - accuracy: 0.5000\n",
      "Epoch 156/500: - 0.0501s/step - loss: 8.4311 - accuracy: 0.5000\n",
      "Epoch 157/500: - 0.0502s/step - loss: 8.4070 - accuracy: 0.5000\n",
      "Epoch 158/500: - 0.0502s/step - loss: 8.3831 - accuracy: 0.5000\n",
      "Epoch 159/500: - 0.0501s/step - loss: 8.3595 - accuracy: 0.5000\n",
      "Epoch 160/500: - 0.0501s/step - loss: 8.7931 - accuracy: 0.5000\n",
      "Epoch 161/500: - 0.0501s/step - loss: 8.7573 - accuracy: 0.5000\n",
      "Epoch 162/500: - 0.0501s/step - loss: 8.7224 - accuracy: 0.5000\n",
      "Epoch 163/500: - 0.0501s/step - loss: 8.6884 - accuracy: 0.5000\n",
      "Epoch 164/500: - 0.0503s/step - loss: 8.6553 - accuracy: 0.5000\n",
      "Epoch 165/500: - 0.0505s/step - loss: 8.6230 - accuracy: 0.5000\n",
      "Epoch 166/500: - 0.0506s/step - loss: 8.5914 - accuracy: 0.5000\n",
      "Epoch 167/500: - 0.0506s/step - loss: 8.5606 - accuracy: 0.5000\n",
      "Epoch 168/500: - 0.0506s/step - loss: 8.5305 - accuracy: 0.5000\n",
      "Epoch 169/500: - 0.0506s/step - loss: 8.5010 - accuracy: 0.5000\n",
      "Epoch 170/500: - 0.0506s/step - loss: 8.4722 - accuracy: 0.5000\n",
      "Epoch 171/500: - 0.0507s/step - loss: 8.4439 - accuracy: 0.5000\n",
      "Epoch 172/500: - 0.0507s/step - loss: 8.4162 - accuracy: 0.5000\n",
      "Epoch 173/500: - 0.0508s/step - loss: 8.3891 - accuracy: 0.5000\n",
      "Epoch 174/500: - 0.0508s/step - loss: 8.3625 - accuracy: 0.5000\n",
      "Epoch 175/500: - 0.0508s/step - loss: 8.3364 - accuracy: 0.5000\n",
      "Epoch 176/500: - 0.0508s/step - loss: 8.3108 - accuracy: 0.5000\n",
      "Epoch 177/500: - 0.0507s/step - loss: 8.2857 - accuracy: 0.5000\n",
      "Epoch 178/500: - 0.0507s/step - loss: 8.2610 - accuracy: 0.5000\n",
      "Epoch 179/500: - 0.0507s/step - loss: 8.2367 - accuracy: 0.5000\n",
      "Epoch 180/500: - 0.0508s/step - loss: 8.2128 - accuracy: 0.5000\n",
      "Epoch 181/500: - 0.0508s/step - loss: 8.1894 - accuracy: 0.5000\n",
      "Epoch 182/500: - 0.0507s/step - loss: 8.1663 - accuracy: 0.5000\n",
      "Epoch 183/500: - 0.0508s/step - loss: 8.1436 - accuracy: 0.5000\n",
      "Epoch 184/500: - 0.0510s/step - loss: 8.1213 - accuracy: 0.7500\n",
      "Epoch 185/500: - 0.0509s/step - loss: 8.0993 - accuracy: 0.7500\n",
      "Epoch 186/500: - 0.0509s/step - loss: 8.0776 - accuracy: 0.7500\n",
      "Epoch 187/500: - 0.0510s/step - loss: 8.0562 - accuracy: 0.7500\n",
      "Epoch 188/500: - 0.0511s/step - loss: 8.0352 - accuracy: 0.7500\n",
      "Epoch 189/500: - 0.0510s/step - loss: 8.0145 - accuracy: 0.7500\n",
      "Epoch 190/500: - 0.0510s/step - loss: 7.9941 - accuracy: 0.7500\n",
      "Epoch 191/500: - 0.0510s/step - loss: 7.9739 - accuracy: 0.7500\n",
      "Epoch 192/500: - 0.0511s/step - loss: 7.9540 - accuracy: 0.7500\n",
      "Epoch 193/500: - 0.0512s/step - loss: 7.9344 - accuracy: 0.7500\n",
      "Epoch 194/500: - 0.0512s/step - loss: 7.9151 - accuracy: 0.7500\n",
      "Epoch 195/500: - 0.0513s/step - loss: 7.8960 - accuracy: 0.7500\n",
      "Epoch 196/500: - 0.0513s/step - loss: 7.8771 - accuracy: 0.7500\n",
      "Epoch 197/500: - 0.0512s/step - loss: 7.8585 - accuracy: 0.7500\n",
      "Epoch 198/500: - 0.0512s/step - loss: 7.8402 - accuracy: 0.5000\n",
      "Epoch 199/500: - 0.0512s/step - loss: 7.8220 - accuracy: 0.5000\n",
      "Epoch 200/500: - 0.0513s/step - loss: 7.8041 - accuracy: 0.5000\n",
      "Epoch 201/500: - 0.0514s/step - loss: 7.7864 - accuracy: 0.5000\n",
      "Epoch 202/500: - 0.0515s/step - loss: 7.7689 - accuracy: 0.5000\n",
      "Epoch 203/500: - 0.0516s/step - loss: 7.7516 - accuracy: 0.5000\n",
      "Epoch 204/500: - 0.0515s/step - loss: 7.7345 - accuracy: 0.5000\n",
      "Epoch 205/500: - 0.0515s/step - loss: 7.7177 - accuracy: 0.5000\n",
      "Epoch 206/500: - 0.0514s/step - loss: 7.7010 - accuracy: 0.5000\n",
      "Epoch 207/500: - 0.0514s/step - loss: 7.6845 - accuracy: 0.5000\n",
      "Epoch 208/500: - 0.0515s/step - loss: 7.6681 - accuracy: 0.5000\n",
      "Epoch 209/500: - 0.0515s/step - loss: 7.6520 - accuracy: 0.5000\n",
      "Epoch 210/500: - 0.0515s/step - loss: 7.6360 - accuracy: 0.5000\n",
      "Epoch 211/500: - 0.0514s/step - loss: 7.6202 - accuracy: 0.5000\n",
      "Epoch 212/500: - 0.0514s/step - loss: 7.6046 - accuracy: 0.5000\n",
      "Epoch 213/500: - 0.0514s/step - loss: 7.5891 - accuracy: 0.5000\n",
      "Epoch 214/500: - 0.0513s/step - loss: 7.5738 - accuracy: 0.5000\n",
      "Epoch 215/500: - 0.0513s/step - loss: 7.5586 - accuracy: 0.5000\n",
      "Epoch 216/500: - 0.0512s/step - loss: 7.5437 - accuracy: 0.5000\n",
      "Epoch 217/500: - 0.0512s/step - loss: 7.5288 - accuracy: 0.5000\n",
      "Epoch 218/500: - 0.0513s/step - loss: 8.1051 - accuracy: 0.5000\n",
      "Epoch 219/500: - 0.0512s/step - loss: 8.0667 - accuracy: 0.5000\n",
      "Epoch 220/500: - 0.0512s/step - loss: 8.0312 - accuracy: 0.5000\n",
      "Epoch 221/500: - 0.0513s/step - loss: 7.9978 - accuracy: 0.5000\n",
      "Epoch 222/500: - 0.0514s/step - loss: 7.9663 - accuracy: 0.5000\n",
      "Epoch 223/500: - 0.0514s/step - loss: 7.9362 - accuracy: 0.5000\n",
      "Epoch 224/500: - 0.0513s/step - loss: 7.9073 - accuracy: 0.5000\n",
      "Epoch 225/500: - 0.0513s/step - loss: 7.8795 - accuracy: 0.5000\n",
      "Epoch 226/500: - 0.0513s/step - loss: 7.8525 - accuracy: 0.5000\n",
      "Epoch 227/500: - 0.0513s/step - loss: 7.8263 - accuracy: 0.5000\n",
      "Epoch 228/500: - 0.0512s/step - loss: 7.8007 - accuracy: 0.5000\n",
      "Epoch 229/500: - 0.0512s/step - loss: 7.7758 - accuracy: 0.5000\n",
      "Epoch 230/500: - 0.0512s/step - loss: 7.7515 - accuracy: 0.5000\n",
      "Epoch 231/500: - 0.0512s/step - loss: 7.7276 - accuracy: 0.5000\n",
      "Epoch 232/500: - 0.0513s/step - loss: 7.7042 - accuracy: 0.5000\n",
      "Epoch 233/500: - 0.0512s/step - loss: 7.6813 - accuracy: 0.5000\n",
      "Epoch 234/500: - 0.0512s/step - loss: 7.6588 - accuracy: 0.5000\n",
      "Epoch 235/500: - 0.0511s/step - loss: 7.6367 - accuracy: 0.5000\n",
      "Epoch 236/500: - 0.0511s/step - loss: 7.6150 - accuracy: 0.5000\n",
      "Epoch 237/500: - 0.0511s/step - loss: 7.5936 - accuracy: 0.5000\n",
      "Epoch 238/500: - 0.0511s/step - loss: 7.5726 - accuracy: 0.5000\n",
      "Epoch 239/500: - 0.0511s/step - loss: 7.5519 - accuracy: 0.5000\n",
      "Epoch 240/500: - 0.0513s/step - loss: 7.5316 - accuracy: 0.5000\n",
      "Epoch 241/500: - 0.0515s/step - loss: 7.5115 - accuracy: 0.5000\n",
      "Epoch 242/500: - 0.0516s/step - loss: 7.4918 - accuracy: 0.5000\n",
      "Epoch 243/500: - 0.0515s/step - loss: 7.4723 - accuracy: 0.5000\n",
      "Epoch 244/500: - 0.0515s/step - loss: 7.4531 - accuracy: 0.5000\n",
      "Epoch 245/500: - 0.0516s/step - loss: 7.4342 - accuracy: 0.5000\n",
      "Epoch 246/500: - 0.0515s/step - loss: 7.4155 - accuracy: 0.5000\n",
      "Epoch 247/500: - 0.0515s/step - loss: 7.3971 - accuracy: 0.5000\n",
      "Epoch 248/500: - 0.0516s/step - loss: 7.3790 - accuracy: 0.5000\n",
      "Epoch 249/500: - 0.0517s/step - loss: 7.3610 - accuracy: 0.5000\n",
      "Epoch 250/500: - 0.0517s/step - loss: 7.3433 - accuracy: 0.5000\n",
      "Epoch 251/500: - 0.0517s/step - loss: 7.3258 - accuracy: 0.5000\n",
      "Epoch 252/500: - 0.0517s/step - loss: 7.3086 - accuracy: 0.5000\n",
      "Epoch 253/500: - 0.0517s/step - loss: 7.2915 - accuracy: 0.5000\n",
      "Epoch 254/500: - 0.0517s/step - loss: 7.2747 - accuracy: 0.5000\n",
      "Epoch 255/500: - 0.0516s/step - loss: 7.2580 - accuracy: 0.5000\n",
      "Epoch 256/500: - 0.0516s/step - loss: 7.2416 - accuracy: 0.5000\n",
      "Epoch 257/500: - 0.0516s/step - loss: 7.2253 - accuracy: 0.5000\n",
      "Epoch 258/500: - 0.0516s/step - loss: 7.2092 - accuracy: 0.5000\n",
      "Epoch 259/500: - 0.0517s/step - loss: 7.1933 - accuracy: 0.5000\n",
      "Epoch 260/500: - 0.0517s/step - loss: 7.1776 - accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/500: - 0.0517s/step - loss: 7.1620 - accuracy: 0.5000\n",
      "Epoch 262/500: - 0.0517s/step - loss: 7.1466 - accuracy: 0.5000\n",
      "Epoch 263/500: - 0.0517s/step - loss: 7.1314 - accuracy: 0.5000\n",
      "Epoch 264/500: - 0.0517s/step - loss: 7.4262 - accuracy: 0.5000\n",
      "Epoch 265/500: - 0.0516s/step - loss: 7.3871 - accuracy: 0.5000\n",
      "Epoch 266/500: - 0.0516s/step - loss: 7.3515 - accuracy: 0.5000\n",
      "Epoch 267/500: - 0.0516s/step - loss: 7.3186 - accuracy: 0.5000\n",
      "Epoch 268/500: - 0.0516s/step - loss: 7.2878 - accuracy: 0.5000\n",
      "Epoch 269/500: - 0.0515s/step - loss: 7.2587 - accuracy: 0.5000\n",
      "Epoch 270/500: - 0.0515s/step - loss: 7.2310 - accuracy: 0.5000\n",
      "Epoch 271/500: - 0.0514s/step - loss: 7.2045 - accuracy: 0.5000\n",
      "Epoch 272/500: - 0.0514s/step - loss: 7.1789 - accuracy: 0.5000\n",
      "Epoch 273/500: - 0.0514s/step - loss: 7.1542 - accuracy: 0.5000\n",
      "Epoch 274/500: - 0.0514s/step - loss: 7.1301 - accuracy: 0.5000\n",
      "Epoch 275/500: - 0.0514s/step - loss: 7.1067 - accuracy: 0.5000\n",
      "Epoch 276/500: - 0.0515s/step - loss: 7.0838 - accuracy: 0.5000\n",
      "Epoch 277/500: - 0.0515s/step - loss: 7.0615 - accuracy: 0.5000\n",
      "Epoch 278/500: - 0.0516s/step - loss: 7.0396 - accuracy: 0.5000\n",
      "Epoch 279/500: - 0.0517s/step - loss: 7.0182 - accuracy: 0.5000\n",
      "Epoch 280/500: - 0.0517s/step - loss: 6.9972 - accuracy: 0.5000\n",
      "Epoch 281/500: - 0.0517s/step - loss: 6.9766 - accuracy: 0.5000\n",
      "Epoch 282/500: - 0.0517s/step - loss: 6.9563 - accuracy: 0.5000\n",
      "Epoch 283/500: - 0.0517s/step - loss: 6.9365 - accuracy: 0.5000\n",
      "Epoch 284/500: - 0.0517s/step - loss: 6.9169 - accuracy: 0.5000\n",
      "Epoch 285/500: - 0.0517s/step - loss: 6.8977 - accuracy: 0.5000\n",
      "Epoch 286/500: - 0.0518s/step - loss: 6.8788 - accuracy: 0.5000\n",
      "Epoch 287/500: - 0.0518s/step - loss: 6.8603 - accuracy: 0.5000\n",
      "Epoch 288/500: - 0.0518s/step - loss: 6.8420 - accuracy: 0.5000\n",
      "Epoch 289/500: - 0.0518s/step - loss: 6.8240 - accuracy: 0.5000\n",
      "Epoch 290/500: - 0.0518s/step - loss: 6.8062 - accuracy: 0.5000\n",
      "Epoch 291/500: - 0.0518s/step - loss: 6.7888 - accuracy: 0.5000\n",
      "Epoch 292/500: - 0.0518s/step - loss: 6.7715 - accuracy: 0.5000\n",
      "Epoch 293/500: - 0.0518s/step - loss: 6.7546 - accuracy: 0.5000\n",
      "Epoch 294/500: - 0.0518s/step - loss: 6.7379 - accuracy: 0.5000\n",
      "Epoch 295/500: - 0.0518s/step - loss: 6.7214 - accuracy: 0.5000\n",
      "Epoch 296/500: - 0.0519s/step - loss: 6.7051 - accuracy: 0.5000\n",
      "Epoch 297/500: - 0.0519s/step - loss: 6.6890 - accuracy: 0.5000\n",
      "Epoch 298/500: - 0.0518s/step - loss: 6.6732 - accuracy: 0.5000\n",
      "Epoch 299/500: - 0.0518s/step - loss: 6.6576 - accuracy: 0.5000\n",
      "Epoch 300/500: - 0.0518s/step - loss: 6.6422 - accuracy: 0.5000\n",
      "Epoch 301/500: - 0.0518s/step - loss: 6.6269 - accuracy: 0.5000\n",
      "Epoch 302/500: - 0.0518s/step - loss: 6.6119 - accuracy: 0.5000\n",
      "Epoch 303/500: - 0.0517s/step - loss: 6.5970 - accuracy: 0.5000\n",
      "Epoch 304/500: - 0.0517s/step - loss: 6.5823 - accuracy: 0.5000\n",
      "Epoch 305/500: - 0.0516s/step - loss: 6.5678 - accuracy: 0.5000\n",
      "Epoch 306/500: - 0.0517s/step - loss: 6.5535 - accuracy: 0.5000\n",
      "Epoch 307/500: - 0.0517s/step - loss: 6.5393 - accuracy: 0.5000\n",
      "Epoch 308/500: - 0.0517s/step - loss: 6.5253 - accuracy: 0.5000\n",
      "Epoch 309/500: - 0.0517s/step - loss: 6.5114 - accuracy: 0.5000\n",
      "Epoch 310/500: - 0.0517s/step - loss: 6.4977 - accuracy: 0.5000\n",
      "Epoch 311/500: - 0.0517s/step - loss: 6.4842 - accuracy: 0.5000\n",
      "Epoch 312/500: - 0.0517s/step - loss: 6.4708 - accuracy: 0.5000\n",
      "Epoch 313/500: - 0.0517s/step - loss: 6.4575 - accuracy: 0.5000\n",
      "Epoch 314/500: - 0.0516s/step - loss: 6.4444 - accuracy: 0.5000\n",
      "Epoch 315/500: - 0.0517s/step - loss: 6.4314 - accuracy: 0.5000\n",
      "Epoch 316/500: - 0.0517s/step - loss: 6.4185 - accuracy: 0.5000\n",
      "Epoch 317/500: - 0.0516s/step - loss: 6.4058 - accuracy: 0.5000\n",
      "Epoch 318/500: - 0.0516s/step - loss: 6.3932 - accuracy: 0.5000\n",
      "Epoch 319/500: - 0.0516s/step - loss: 6.3807 - accuracy: 0.5000\n",
      "Epoch 320/500: - 0.0517s/step - loss: 6.3684 - accuracy: 0.5000\n",
      "Epoch 321/500: - 0.0517s/step - loss: 6.3561 - accuracy: 0.5000\n",
      "Epoch 322/500: - 0.0516s/step - loss: 6.3440 - accuracy: 0.5000\n",
      "Epoch 323/500: - 0.0516s/step - loss: 6.3320 - accuracy: 0.5000\n",
      "Epoch 324/500: - 0.0516s/step - loss: 6.3201 - accuracy: 0.5000\n",
      "Epoch 325/500: - 0.0515s/step - loss: 6.3084 - accuracy: 0.5000\n",
      "Epoch 326/500: - 0.0515s/step - loss: 6.2967 - accuracy: 0.5000\n",
      "Epoch 327/500: - 0.0515s/step - loss: 6.2851 - accuracy: 0.5000\n",
      "Epoch 328/500: - 0.0515s/step - loss: 6.2737 - accuracy: 0.5000\n",
      "Epoch 329/500: - 0.0515s/step - loss: 6.2623 - accuracy: 0.5000\n",
      "Epoch 330/500: - 0.0515s/step - loss: 6.2511 - accuracy: 0.5000\n",
      "Epoch 331/500: - 0.0515s/step - loss: 6.2399 - accuracy: 0.5000\n",
      "Epoch 332/500: - 0.0514s/step - loss: 6.2288 - accuracy: 0.5000\n",
      "Epoch 333/500: - 0.0514s/step - loss: 6.2179 - accuracy: 0.5000\n",
      "Epoch 334/500: - 0.0514s/step - loss: 6.2070 - accuracy: 0.5000\n",
      "Epoch 335/500: - 0.0515s/step - loss: 6.1962 - accuracy: 0.5000\n",
      "Epoch 336/500: - 0.0515s/step - loss: 6.1855 - accuracy: 0.5000\n",
      "Epoch 337/500: - 0.0516s/step - loss: 6.1749 - accuracy: 0.5000\n",
      "Epoch 338/500: - 0.0516s/step - loss: 6.1644 - accuracy: 0.5000\n",
      "Epoch 339/500: - 0.0515s/step - loss: 6.1540 - accuracy: 0.5000\n",
      "Epoch 340/500: - 0.0515s/step - loss: 6.1437 - accuracy: 0.5000\n",
      "Epoch 341/500: - 0.0515s/step - loss: 6.1334 - accuracy: 0.5000\n",
      "Epoch 342/500: - 0.0514s/step - loss: 6.1232 - accuracy: 0.5000\n",
      "Epoch 343/500: - 0.0514s/step - loss: 6.1131 - accuracy: 0.5000\n",
      "Epoch 344/500: - 0.0514s/step - loss: 6.1031 - accuracy: 0.5000\n",
      "Epoch 345/500: - 0.0514s/step - loss: 6.0932 - accuracy: 0.5000\n",
      "Epoch 346/500: - 0.0514s/step - loss: 6.0833 - accuracy: 0.5000\n",
      "Epoch 347/500: - 0.0514s/step - loss: 6.0735 - accuracy: 0.5000\n",
      "Epoch 348/500: - 0.0515s/step - loss: 6.0638 - accuracy: 0.5000\n",
      "Epoch 349/500: - 0.0514s/step - loss: 6.0542 - accuracy: 0.5000\n",
      "Epoch 350/500: - 0.0514s/step - loss: 6.0446 - accuracy: 0.5000\n",
      "Epoch 351/500: - 0.0514s/step - loss: 6.0351 - accuracy: 0.5000\n",
      "Epoch 352/500: - 0.0514s/step - loss: 6.0257 - accuracy: 0.5000\n",
      "Epoch 353/500: - 0.0514s/step - loss: 6.0163 - accuracy: 0.5000\n",
      "Epoch 354/500: - 0.0514s/step - loss: 6.0071 - accuracy: 0.5000\n",
      "Epoch 355/500: - 0.0514s/step - loss: 5.9978 - accuracy: 0.5000\n",
      "Epoch 356/500: - 0.0514s/step - loss: 5.9887 - accuracy: 0.5000\n",
      "Epoch 357/500: - 0.0514s/step - loss: 5.9796 - accuracy: 0.5000\n",
      "Epoch 358/500: - 0.0514s/step - loss: 5.9706 - accuracy: 0.5000\n",
      "Epoch 359/500: - 0.0514s/step - loss: 5.9616 - accuracy: 0.5000\n",
      "Epoch 360/500: - 0.0513s/step - loss: 5.9528 - accuracy: 0.5000\n",
      "Epoch 361/500: - 0.0514s/step - loss: 5.9439 - accuracy: 0.5000\n",
      "Epoch 362/500: - 0.0514s/step - loss: 5.9352 - accuracy: 0.5000\n",
      "Epoch 363/500: - 0.0513s/step - loss: 5.9265 - accuracy: 0.5000\n",
      "Epoch 364/500: - 0.0513s/step - loss: 5.9178 - accuracy: 0.5000\n",
      "Epoch 365/500: - 0.0513s/step - loss: 5.9092 - accuracy: 0.5000\n",
      "Epoch 366/500: - 0.0513s/step - loss: 5.9007 - accuracy: 0.5000\n",
      "Epoch 367/500: - 0.0513s/step - loss: 5.8922 - accuracy: 0.5000\n",
      "Epoch 368/500: - 0.0513s/step - loss: 5.8838 - accuracy: 0.5000\n",
      "Epoch 369/500: - 0.0513s/step - loss: 5.8755 - accuracy: 0.5000\n",
      "Epoch 370/500: - 0.0514s/step - loss: 5.8672 - accuracy: 0.5000\n",
      "Epoch 371/500: - 0.0514s/step - loss: 5.8589 - accuracy: 0.5000\n",
      "Epoch 372/500: - 0.0514s/step - loss: 5.8507 - accuracy: 0.5000\n",
      "Epoch 373/500: - 0.0514s/step - loss: 5.8426 - accuracy: 0.5000\n",
      "Epoch 374/500: - 0.0514s/step - loss: 5.8345 - accuracy: 0.5000\n",
      "Epoch 375/500: - 0.0515s/step - loss: 5.8265 - accuracy: 0.5000\n",
      "Epoch 376/500: - 0.0515s/step - loss: 5.8185 - accuracy: 0.5000\n",
      "Epoch 377/500: - 0.0514s/step - loss: 5.8106 - accuracy: 0.5000\n",
      "Epoch 378/500: - 0.0514s/step - loss: 5.8027 - accuracy: 0.5000\n",
      "Epoch 379/500: - 0.0515s/step - loss: 5.7949 - accuracy: 0.5000\n",
      "Epoch 380/500: - 0.0515s/step - loss: 5.7872 - accuracy: 0.5000\n",
      "Epoch 381/500: - 0.0514s/step - loss: 5.7794 - accuracy: 0.5000\n",
      "Epoch 382/500: - 0.0514s/step - loss: 5.7718 - accuracy: 0.5000\n",
      "Epoch 383/500: - 0.0514s/step - loss: 5.7641 - accuracy: 0.5000\n",
      "Epoch 384/500: - 0.0514s/step - loss: 5.7566 - accuracy: 0.5000\n",
      "Epoch 385/500: - 0.0513s/step - loss: 5.7490 - accuracy: 0.5000\n",
      "Epoch 386/500: - 0.0513s/step - loss: 5.7416 - accuracy: 0.5000\n",
      "Epoch 387/500: - 0.0513s/step - loss: 5.7341 - accuracy: 0.7500\n",
      "Epoch 388/500: - 0.0513s/step - loss: 5.7267 - accuracy: 0.7500\n",
      "Epoch 389/500: - 0.0513s/step - loss: 5.7194 - accuracy: 0.7500\n",
      "Epoch 390/500: - 0.0514s/step - loss: 5.7121 - accuracy: 0.7500\n",
      "Epoch 391/500: - 0.0513s/step - loss: 5.7049 - accuracy: 0.7500\n",
      "Epoch 392/500: - 0.0513s/step - loss: 5.6976 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/500: - 0.0513s/step - loss: 5.6905 - accuracy: 0.7500\n",
      "Epoch 394/500: - 0.0513s/step - loss: 5.6834 - accuracy: 0.7500\n",
      "Epoch 395/500: - 0.0513s/step - loss: 5.6763 - accuracy: 0.7500\n",
      "Epoch 396/500: - 0.0514s/step - loss: 5.6693 - accuracy: 0.7500\n",
      "Epoch 397/500: - 0.0514s/step - loss: 5.6623 - accuracy: 0.7500\n",
      "Epoch 398/500: - 0.0514s/step - loss: 5.6553 - accuracy: 0.7500\n",
      "Epoch 399/500: - 0.0513s/step - loss: 5.6484 - accuracy: 0.7500\n",
      "Epoch 400/500: - 0.0513s/step - loss: 5.6415 - accuracy: 0.7500\n",
      "Epoch 401/500: - 0.0513s/step - loss: 5.6347 - accuracy: 0.7500\n",
      "Epoch 402/500: - 0.0513s/step - loss: 5.6279 - accuracy: 0.7500\n",
      "Epoch 403/500: - 0.0513s/step - loss: 5.6212 - accuracy: 0.7500\n",
      "Epoch 404/500: - 0.0514s/step - loss: 5.6145 - accuracy: 0.7500\n",
      "Epoch 405/500: - 0.0513s/step - loss: 5.6078 - accuracy: 0.7500\n",
      "Epoch 406/500: - 0.0513s/step - loss: 5.6011 - accuracy: 0.7500\n",
      "Epoch 407/500: - 0.0513s/step - loss: 5.5946 - accuracy: 0.7500\n",
      "Epoch 408/500: - 0.0514s/step - loss: 5.5880 - accuracy: 0.7500\n",
      "Epoch 409/500: - 0.0514s/step - loss: 5.5815 - accuracy: 0.7500\n",
      "Epoch 410/500: - 0.0513s/step - loss: 5.5750 - accuracy: 0.7500\n",
      "Epoch 411/500: - 0.0513s/step - loss: 5.5686 - accuracy: 0.7500\n",
      "Epoch 412/500: - 0.0514s/step - loss: 5.5621 - accuracy: 0.7500\n",
      "Epoch 413/500: - 0.0514s/step - loss: 5.5558 - accuracy: 0.7500\n",
      "Epoch 414/500: - 0.0514s/step - loss: 5.5494 - accuracy: 0.7500\n",
      "Epoch 415/500: - 0.0514s/step - loss: 5.5431 - accuracy: 0.7500\n",
      "Epoch 416/500: - 0.0514s/step - loss: 5.5369 - accuracy: 0.7500\n",
      "Epoch 417/500: - 0.0514s/step - loss: 5.5306 - accuracy: 0.7500\n",
      "Epoch 418/500: - 0.0514s/step - loss: 5.5244 - accuracy: 0.7500\n",
      "Epoch 419/500: - 0.0514s/step - loss: 5.5183 - accuracy: 0.7500\n",
      "Epoch 420/500: - 0.0514s/step - loss: 5.5121 - accuracy: 0.7500\n",
      "Epoch 421/500: - 0.0514s/step - loss: 5.5061 - accuracy: 0.7500\n",
      "Epoch 422/500: - 0.0514s/step - loss: 5.5000 - accuracy: 0.7500\n",
      "Epoch 423/500: - 0.0514s/step - loss: 5.4940 - accuracy: 0.7500\n",
      "Epoch 424/500: - 0.0514s/step - loss: 5.4880 - accuracy: 0.7500\n",
      "Epoch 425/500: - 0.0513s/step - loss: 5.4820 - accuracy: 0.7500\n",
      "Epoch 426/500: - 0.0513s/step - loss: 5.4761 - accuracy: 0.7500\n",
      "Epoch 427/500: - 0.0513s/step - loss: 5.4702 - accuracy: 0.7500\n",
      "Epoch 428/500: - 0.0513s/step - loss: 5.4643 - accuracy: 0.7500\n",
      "Epoch 429/500: - 0.0513s/step - loss: 5.4585 - accuracy: 0.7500\n",
      "Epoch 430/500: - 0.0513s/step - loss: 5.4527 - accuracy: 0.7500\n",
      "Epoch 431/500: - 0.0513s/step - loss: 5.4469 - accuracy: 0.7500\n",
      "Epoch 432/500: - 0.0513s/step - loss: 5.4412 - accuracy: 0.7500\n",
      "Epoch 433/500: - 0.0513s/step - loss: 5.4354 - accuracy: 0.7500\n",
      "Epoch 434/500: - 0.0513s/step - loss: 5.4298 - accuracy: 0.7500\n",
      "Epoch 435/500: - 0.0514s/step - loss: 5.4241 - accuracy: 0.7500\n",
      "Epoch 436/500: - 0.0514s/step - loss: 5.4185 - accuracy: 0.7500\n",
      "Epoch 437/500: - 0.0514s/step - loss: 5.4129 - accuracy: 0.7500\n",
      "Epoch 438/500: - 0.0514s/step - loss: 5.4073 - accuracy: 0.7500\n",
      "Epoch 439/500: - 0.0513s/step - loss: 5.4018 - accuracy: 0.7500\n",
      "Epoch 440/500: - 0.0513s/step - loss: 5.3963 - accuracy: 0.7500\n",
      "Epoch 441/500: - 0.0513s/step - loss: 5.3908 - accuracy: 0.7500\n",
      "Epoch 442/500: - 0.0513s/step - loss: 5.3854 - accuracy: 0.7500\n",
      "Epoch 443/500: - 0.0513s/step - loss: 5.3799 - accuracy: 0.7500\n",
      "Epoch 444/500: - 0.0513s/step - loss: 5.3746 - accuracy: 0.7500\n",
      "Epoch 445/500: - 0.0513s/step - loss: 5.3692 - accuracy: 1.0000\n",
      "Epoch 446/500: - 0.0513s/step - loss: 5.3639 - accuracy: 1.0000\n",
      "Epoch 447/500: - 0.0513s/step - loss: 5.3585 - accuracy: 1.0000\n",
      "Epoch 448/500: - 0.0514s/step - loss: 5.3533 - accuracy: 1.0000\n",
      "Epoch 449/500: - 0.0515s/step - loss: 5.3480 - accuracy: 1.0000\n",
      "Epoch 450/500: - 0.0516s/step - loss: 5.3428 - accuracy: 1.0000\n",
      "Epoch 451/500: - 0.0517s/step - loss: 5.3376 - accuracy: 1.0000\n",
      "Epoch 452/500: - 0.0518s/step - loss: 5.3324 - accuracy: 1.0000\n",
      "Epoch 453/500: - 0.0520s/step - loss: 5.3272 - accuracy: 1.0000\n",
      "Epoch 454/500: - 0.0521s/step - loss: 5.3221 - accuracy: 1.0000\n",
      "Epoch 455/500: - 0.0521s/step - loss: 5.3170 - accuracy: 1.0000\n",
      "Epoch 456/500: - 0.0524s/step - loss: 5.3119 - accuracy: 1.0000\n",
      "Epoch 457/500: - 0.0524s/step - loss: 5.3069 - accuracy: 1.0000\n",
      "Epoch 458/500: - 0.0524s/step - loss: 5.3019 - accuracy: 1.0000\n",
      "Epoch 459/500: - 0.0524s/step - loss: 5.2969 - accuracy: 1.0000\n",
      "Epoch 460/500: - 0.0525s/step - loss: 5.2919 - accuracy: 1.0000\n",
      "Epoch 461/500: - 0.0525s/step - loss: 5.2869 - accuracy: 1.0000\n",
      "Epoch 462/500: - 0.0525s/step - loss: 5.2820 - accuracy: 1.0000\n",
      "Epoch 463/500: - 0.0526s/step - loss: 5.2771 - accuracy: 1.0000\n",
      "Epoch 464/500: - 0.0527s/step - loss: 5.2722 - accuracy: 1.0000\n",
      "Epoch 465/500: - 0.0527s/step - loss: 5.2674 - accuracy: 1.0000\n",
      "Epoch 466/500: - 0.0527s/step - loss: 5.2625 - accuracy: 1.0000\n",
      "Epoch 467/500: - 0.0527s/step - loss: 5.2577 - accuracy: 1.0000\n",
      "Epoch 468/500: - 0.0528s/step - loss: 5.2529 - accuracy: 1.0000\n",
      "Epoch 469/500: - 0.0528s/step - loss: 5.2482 - accuracy: 1.0000\n",
      "Epoch 470/500: - 0.0529s/step - loss: 5.2434 - accuracy: 1.0000\n",
      "Epoch 471/500: - 0.0530s/step - loss: 5.2387 - accuracy: 1.0000\n",
      "Epoch 472/500: - 0.0530s/step - loss: 5.2340 - accuracy: 1.0000\n",
      "Epoch 473/500: - 0.0530s/step - loss: 5.2293 - accuracy: 1.0000\n",
      "Epoch 474/500: - 0.0530s/step - loss: 5.2247 - accuracy: 1.0000\n",
      "Epoch 475/500: - 0.0530s/step - loss: 5.2201 - accuracy: 1.0000\n",
      "Epoch 476/500: - 0.0530s/step - loss: 5.2155 - accuracy: 1.0000\n",
      "Epoch 477/500: - 0.0530s/step - loss: 5.2109 - accuracy: 1.0000\n",
      "Epoch 478/500: - 0.0529s/step - loss: 5.2063 - accuracy: 1.0000\n",
      "Epoch 479/500: - 0.0529s/step - loss: 5.2018 - accuracy: 1.0000\n",
      "Epoch 480/500: - 0.0530s/step - loss: 5.1973 - accuracy: 1.0000\n",
      "Epoch 481/500: - 0.0530s/step - loss: 5.1928 - accuracy: 1.0000\n",
      "Epoch 482/500: - 0.0530s/step - loss: 5.1883 - accuracy: 1.0000\n",
      "Epoch 483/500: - 0.0530s/step - loss: 5.1838 - accuracy: 1.0000\n",
      "Epoch 484/500: - 0.0530s/step - loss: 5.1794 - accuracy: 1.0000\n",
      "Epoch 485/500: - 0.0530s/step - loss: 5.1750 - accuracy: 1.0000\n",
      "Epoch 486/500: - 0.0530s/step - loss: 5.1706 - accuracy: 1.0000\n",
      "Epoch 487/500: - 0.0530s/step - loss: 5.1662 - accuracy: 1.0000\n",
      "Epoch 488/500: - 0.0530s/step - loss: 5.1618 - accuracy: 1.0000\n",
      "Epoch 489/500: - 0.0529s/step - loss: 5.1575 - accuracy: 1.0000\n",
      "Epoch 490/500: - 0.0529s/step - loss: 5.1532 - accuracy: 1.0000\n",
      "Epoch 491/500: - 0.0529s/step - loss: 5.1489 - accuracy: 1.0000\n",
      "Epoch 492/500: - 0.0529s/step - loss: 5.1446 - accuracy: 1.0000\n",
      "Epoch 493/500: - 0.0529s/step - loss: 5.1404 - accuracy: 1.0000\n",
      "Epoch 494/500: - 0.0529s/step - loss: 5.1361 - accuracy: 1.0000\n",
      "Epoch 495/500: - 0.0529s/step - loss: 5.1319 - accuracy: 1.0000\n",
      "Epoch 496/500: - 0.0529s/step - loss: 5.1277 - accuracy: 1.0000\n",
      "Epoch 497/500: - 0.0529s/step - loss: 5.1235 - accuracy: 1.0000\n",
      "Epoch 498/500: - 0.0529s/step - loss: 5.1194 - accuracy: 1.0000\n",
      "Epoch 499/500: - 0.0529s/step - loss: 5.1152 - accuracy: 1.0000\n",
      "Epoch 500/500: - 0.0529s/step - loss: 5.1111 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for bs, (images, labels) in enumerate(train_ds):\n",
    "        \n",
    "        # only one mini-batch\n",
    "        model2.update_weights(images, network2[bs], labels, 0.1)\n",
    "        #network_new = [model.propose_new_state_hamiltonian(images, net, labels) for (images, labels), net in \n",
    "        #               zip(train_ds, network)]\n",
    "        \n",
    "        network_new2 = []\n",
    "        for (images, labels), net, hmc_kernel in zip(train_ds, network2, kernels2):\n",
    "            net_current = net\n",
    "            net_current = [tf.cast(net_i, dtype=tf.float32) for net_i in net_current]\n",
    "            net_current = net_current[0]\n",
    "        \n",
    "            num_results = 1\n",
    "            num_burnin_steps = 0\n",
    "\n",
    "            samples = tfp.mcmc.sample_chain(\n",
    "                num_results = num_results,\n",
    "                num_burnin_steps = num_burnin_steps,\n",
    "                current_state = net_current, # may need to be reshaped\n",
    "                kernel = hmc_kernel,\n",
    "                trace_fn = None,\n",
    "                return_final_kernel_results = True)\n",
    "            \n",
    "            new_state = rerange(samples[0][0])\n",
    "            net_new = tf.split(new_state, [30], axis = 1)   \n",
    "            network_new2.append(net_new)\n",
    "            \n",
    "        network2 = network_new2\n",
    "        \n",
    "        loss += -1 * tf.reduce_mean(model2.target_log_prob(images, network2[bs], labels))\n",
    "    \n",
    "    preds = [model2.get_predictions(images) for images, labels in train_ds]\n",
    "    train_acc = accuracy_score(np.array(preds[0]), y_train)\n",
    "    print(\"Epoch %d/%d: - %.4fs/step - loss: %.4f - accuracy: %.4f\" \n",
    "          % (epoch + 1, epochs, (time.time() - start_time) / (epoch + 1), loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.8285"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0513*445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9975274>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.sigmoid(6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.002472623>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.sigmoid(-6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
